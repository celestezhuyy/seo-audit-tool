import streamlit as st
import time
import pandas as pd
import requests
import hashlib
import re
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
    from pptx.enum.shapes import MSO_SHAPE
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. å›½é™…åŒ–å­—å…¸ (i18n) ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "æ·±åº¦å®¡è®¡ç‰ˆ v3.1",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        "sitemap_status_title": "Sitemap çŠ¶æ€:",
        
        # PSI ç›¸å…³
        "psi_settings": "Google PSI API è®¾ç½® (å¯é€‰)",
        "psi_api_key_label": "è¾“å…¥ Google PageSpeed API Key",
        "psi_api_help": "ç•™ç©ºåˆ™ä»…è¿›è¡Œé™æ€ä»£ç æ£€æŸ¥ã€‚å¡«å…¥ Key å¯è·å–é¦–é¡µçš„çœŸå®ç”¨æˆ·ä½“éªŒæ•°æ® (LCP, CLS, INP)ã€‚",
        "psi_fetching": "æ­£åœ¨è°ƒç”¨ Google API è·å–çœŸå® CWV æ•°æ®...",
        "psi_success": "æˆåŠŸè·å–çœŸå®ç”¨æˆ·æ•°æ®ï¼",
        "psi_error": "API è°ƒç”¨å¤±è´¥æˆ–æ—  CrUX æ•°æ®",
        
        "input_header": "å¼€å§‹æ·±åº¦å®¡è®¡",
        "input_info": "è¯´æ˜: æ–°å¢ Google PageSpeed Insights (PSI) é›†æˆï¼Œæ”¯æŒè·å–çœŸå®ç”¨æˆ·ä½“éªŒæ•°æ®ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°",
        "adv_settings": "é«˜çº§è®¾ç½® (Advanced Settings)", 
        "manual_robots": "æ‰‹åŠ¨ Robots.txt åœ°å€ (å¯é€‰)", 
        "manual_sitemaps": "æ‰‹åŠ¨ Sitemap åœ°å€ (æ¯è¡Œä¸€ä¸ª, å¯é€‰)", 
        "start_btn": "å¼€å§‹æ·±åº¦çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨æ‰§è¡Œæ·±åº¦å®¡è®¡ (Max {} pages)...", 
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚",
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP çŠ¶æ€ç åˆ†å¸ƒ",
        "cwv_title": "æ ¸å¿ƒ Web æŒ‡æ ‡ (Core Web Vitals) - çœŸå®æ•°æ®",
        "cwv_source": "æ•°æ®æ¥æº: Google Chrome User Experience Report (CrUX)",
        
        "matrix_header": "çˆ¬å–æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼ŒåŒ…å« SERP é¢„è§ˆå’Œæ·±åº¦ä»£ç åˆ†æã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "åœ¨å·²çˆ¬å–æ ·æœ¬ä¸­å‘ç° **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "æè¿°:",
        "ppt_sugg": "ğŸ’¡ å»ºè®®:",
        "ppt_examples": "ğŸ” ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        
        "ppt_cover_title": "SEO æ·±åº¦æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro v3.1",
        "ppt_slide_desc_title": "é—®é¢˜æè¿° & å½±å“",
        "ppt_slide_count_title": "æ ·æœ¬ä¸­å—å½±å“é¡µé¢æ•°: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹ & è¯æ®",
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
        "serp_sim_title": "Google æœç´¢ç»“æœæ¨¡æ‹Ÿ:",
    },
    "en": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "Deep Audit Edition v3.1",
        "nav_label": "Navigation",
        "nav_options": ["Input URL", "Dashboard", "Data Matrix", "PPT Generator"],
        "lang_label": "Language / è¯­è¨€",
        "clear_data": "Clear Data & Reset",
        "cache_info": "Cached {} pages",
        "sitemap_status_title": "Sitemap Status:",
        
        # PSI Related
        "psi_settings": "Google PSI API Settings (Optional)",
        "psi_api_key_label": "Enter Google PageSpeed API Key",
        "psi_api_help": "Leave empty for static check only. Enter Key to fetch Real User Metrics (LCP, CLS, INP) for the home page.",
        "psi_fetching": "Fetching real CWV data from Google API...",
        "psi_success": "Real user data fetched!",
        "psi_error": "API Failed or No CrUX Data",
        
        "input_header": "Start Deep Audit",
        "input_info": "Note: Integrated Google PageSpeed Insights (PSI) for real-world user experience data.",
        "input_label": "Target URL",
        "input_placeholder": "https://example.com",
        "max_pages_label": "Max Pages to Crawl",
        "adv_settings": "Advanced Settings", 
        "manual_robots": "Manual Robots.txt URL (Optional)", 
        "manual_sitemaps": "Manual Sitemap URLs (One per line, Optional)", 
        "start_btn": "Start Deep Crawl",
        "error_url": "Invalid URL format",
        "spinner_crawl": "Running Deep Audit (Max {} pages)...", 
        "error_no_data": "No pages crawled.",
        "success_audit": "Audit Complete! Analyzed {} pages.",
        
        "dashboard_header": "Executive Summary",
        "warn_no_data": "No data available.",
        "kpi_health": "Health Score",
        "kpi_pages": "Analyzed Pages",
        "kpi_issues": "Total Issues",
        "kpi_critical": "Critical Issues",
        "chart_issues": "Issue Distribution",
        "chart_no_issues": "No significant issues found.",
        "chart_status": "HTTP Status Codes",
        "cwv_title": "Core Web Vitals - Real User Data",
        "cwv_source": "Source: Google Chrome User Experience Report (CrUX)",
        
        "matrix_header": "Crawled Data Matrix",
        "download_csv": "Download CSV Report",
        
        "ppt_header": "Pitch Deck Preview",
        "ppt_success_no_issues": "No critical issues found.",
        "ppt_download_header": "ğŸ“¥ Export Report",
        "ppt_info": "Note: PPT optimized for 16:9 with SERP simulation and deep code analysis.",
        "ppt_btn": "Generate & Download .pptx",
        "ppt_preview_header": "Web Preview",
        "ppt_slide_title": "Issue Type:",
        "ppt_severity": "Severity:",
        "ppt_impact": "Impact:",
        "ppt_impact_desc": "Affects **{}** pages in crawled sample.",
        "ppt_desc": "Description:",
        "ppt_sugg": "ğŸ’¡ Suggestion:",
        "ppt_examples": "ğŸ” Examples:",
        "ppt_prev": "â¬…ï¸ Previous",
        "ppt_next": "Next â¡ï¸",
        
        "ppt_cover_title": "SEO Technical Audit",
        "ppt_cover_sub": "Generated by AuditAI Pro v3.1",
        "ppt_slide_desc_title": "Description & Impact",
        "ppt_slide_count_title": "Affected Pages (in sample): {}",
        "ppt_slide_ex_title": "Example URLs & Evidence",
        "ppt_slide_sugg_title": "ğŸ’¡ Recommendation:",
        "serp_sim_title": "Google SERP Simulation:",
    }
}

# --- 3. çˆ¬è™«æ ¸å¿ƒå¼•æ“ (æ”¯æŒå¤šè¯­è¨€) ---

def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def get_browser_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }

# --- Google PSI API Integration ---
def fetch_psi_data(url, api_key):
    """Call Google PageSpeed Insights API"""
    if not api_key: return None
    
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            # Extract Crux Data (loadingExperience)
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available for this URL."}
            
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else:
            return {"error": f"API Error: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

def check_site_level_assets(start_url, lang="zh", manual_robots=None, manual_sitemaps=None):
    issues = []
    sitemap_has_hreflang = False
    
    parsed_url = urlparse(start_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    headers = get_browser_headers()
    
    # è¯­è¨€åŒ…
    txt = {
        "zh": {
            "no_robots": "ç¼ºå¤± Robots.txt",
            "robots_bad_rule": "Robots.txt è§„åˆ™é£é™©",
            "robots_no_sitemap": "Robots.txt æœªå£°æ˜ Sitemap",
            "no_sitemap": "Sitemap è®¿é—®å¤±è´¥",
            "sitemap_invalid": "Sitemap æ ¼å¼é”™è¯¯",
            "no_favicon": "ç«™ç‚¹ç¼ºå¤± Favicon",
        },
        "en": {
            "no_robots": "Missing Robots.txt",
            "robots_bad_rule": "Robots.txt Blocking Risk",
            "robots_no_sitemap": "Sitemap not in Robots.txt",
            "no_sitemap": "Sitemap Access Failed",
            "sitemap_invalid": "Invalid Sitemap Format",
            "no_favicon": "Site Missing Favicon",
        }
    }
    t = txt[lang]

    # --- 1. Robots.txt é€»è¾‘æ£€æŸ¥ ---
    robots_url = manual_robots if manual_robots else urljoin(base_url, "/robots.txt")
    try:
        r = requests.get(robots_url, headers=headers, timeout=10, allow_redirects=True, stream=True)
        if r.status_code != 200:
            issues.append({"severity": "Medium", "title": t["no_robots"], "desc": f"Status: {r.status_code}", "suggestion": "Ensure robots.txt exists.", "url": robots_url})
        else:
            # æ£€æŸ¥ Robots å†…å®¹
            content = r.text.lower()
            if "disallow: /" in content and "allow:" not in content:
                 issues.append({"severity": "Critical", "title": t["robots_bad_rule"], "desc": "Found 'Disallow: /' which blocks ALL crawling.", "suggestion": "Remove global disallow rule.", "url": robots_url})
            if "sitemap:" not in content:
                 issues.append({"severity": "Low", "title": t["robots_no_sitemap"], "desc": "Sitemap location not specified in robots.txt.", "suggestion": "Add 'Sitemap: [URL]' directive.", "url": robots_url})
        r.close()
    except: 
        issues.append({"severity": "Medium", "title": t["no_robots"], "desc": "Connection failed.", "suggestion": "Check server config.", "url": robots_url})

    # --- 2. Sitemap æ·±åº¦æ£€æŸ¥ ---
    sitemap_urls_to_check = manual_sitemaps if manual_sitemaps else [urljoin(base_url, "/sitemap.xml")]
    any_sitemap_valid = False
    
    for sitemap_url in sitemap_urls_to_check:
        sitemap_url = sitemap_url.strip()
        if not sitemap_url: continue
        try:
            r = requests.get(sitemap_url, headers=headers, timeout=15, allow_redirects=True)
            if r.status_code == 200:
                # å°è¯•è§£æ XML
                try:
                    root = ET.fromstring(r.content)
                    any_sitemap_valid = True
                    # æ£€æŸ¥ hreflang
                    if 'xhtml' in r.text or 'hreflang' in r.text:
                        sitemap_has_hreflang = True
                except ET.ParseError:
                    if not sitemap_url.endswith('.gz'): # å¿½ç•¥å‹ç¼©æ ¼å¼æŠ¥é”™
                        issues.append({"severity": "Medium", "title": t["sitemap_invalid"], "desc": "XML parsing failed.", "suggestion": "Check XML syntax.", "url": sitemap_url})
            else:
                if manual_sitemaps:
                    issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": f"Status: {r.status_code}", "suggestion": "Check URL.", "url": sitemap_url})
        except:
            if manual_sitemaps: issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": "Connection failed.", "suggestion": "Check URL.", "url": sitemap_url})

    if not any_sitemap_valid and not manual_sitemaps:
         issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": "Default sitemap not found.", "suggestion": "Ensure sitemap.xml exists.", "url": sitemap_urls_to_check[0]})

    return issues, sitemap_has_hreflang

def analyze_page(url, html_content, status_code, lang="zh", sitemap_has_hreflang=False):
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    
    # è¯­è¨€åŒ…
    txt = {
        "zh": {
            "hreflang_invalid": "Hreflang ä»£ç æ ¼å¼é”™è¯¯",
            "hreflang_invalid_desc": "è¯­è¨€ä»£ç ä¸ç¬¦åˆ ISO 639-1 æ ‡å‡† (å¦‚ 'en-US', 'zh-CN')ã€‚",
            "hreflang_no_default": "Hreflang ç¼ºå¤± x-default",
            "hreflang_no_default_desc": "æœªé…ç½® 'x-default' å›é€€ç‰ˆæœ¬ï¼Œå½±å“éåŒ¹é…åœ°åŒºç”¨æˆ·ä½“éªŒã€‚",
            "alt_bad_quality": "å›¾ç‰‡ Alt è´¨é‡å·®",
            "alt_bad_quality_desc": "Alt æ–‡æœ¬åŒ…å«æ— æ„ä¹‰è¯æ±‡ï¼ˆå¦‚æ–‡ä»¶åã€'image' ç­‰ï¼‰æˆ–è¿‡çŸ­ã€‚",
            "anchor_bad_quality": "é”šæ–‡æœ¬è´¨é‡å·® (Generic Anchor)",
            "anchor_bad_quality_desc": "ä½¿ç”¨äº†é€šç”¨è¯æ±‡ï¼ˆå¦‚ 'Click here'ï¼‰ï¼Œæ— æ³•ä¼ é€’é“¾æ¥ç›¸å…³æ€§ã€‚",
            "cls_risk": "å­˜åœ¨ CLS å¸ƒå±€åç§»é£é™© (CWV)",
            "cls_risk_desc": "æ£€æµ‹åˆ° img æ ‡ç­¾ç¼ºå¤± width æˆ– height å±æ€§ï¼Œä¼šå¯¼è‡´é¡µé¢åŠ è½½æ—¶æŠ–åŠ¨ã€‚",
            # ... (Existing keys)
            "missing_title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title)", "short_title": "æ ‡é¢˜è¿‡çŸ­", "long_title": "æ ‡é¢˜è¿‡é•¿",
            "missing_desc": "ç¼ºå¤±å…ƒæè¿°", "short_desc": "å…ƒæè¿°è¿‡çŸ­", "missing_h1": "ç¼ºå¤± H1 æ ‡ç­¾",
            "missing_viewport": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®", "missing_canonical": "ç¼ºå¤± Canonical æ ‡ç­¾",
            "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®", "missing_hreflang": "ç¼ºå¤± Hreflang",
            "soft_404": "ç–‘ä¼¼è½¯ 404 (Soft 404)", "missing_alt": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§",
            "duplicate": "å‘ç°é‡å¤å†…å®¹"
        },
        "en": {
            "hreflang_invalid": "Invalid Hreflang Code",
            "hreflang_invalid_desc": "Language code format is invalid (Use 'en-US', 'fr-FR').",
            "hreflang_no_default": "Missing x-default Hreflang",
            "hreflang_no_default_desc": "No 'x-default' fallback found for unmatched regions.",
            "alt_bad_quality": "Poor Quality Alt Text",
            "alt_bad_quality_desc": "Alt text uses filenames or generic words like 'image'.",
            "anchor_bad_quality": "Poor Anchor Text (Generic)",
            "anchor_bad_quality_desc": "Generic text (e.g., 'Click here') found. Use descriptive text.",
            "cls_risk": "CLS Layout Shift Risk (CWV)",
            "cls_risk_desc": "Images missing width/height attributes, causing layout jumps.",
            # ...
            "missing_title": "Missing Title Tag", "short_title": "Title Too Short", "long_title": "Title Too Long",
            "missing_desc": "Missing Meta Description", "short_desc": "Meta Description Too Short", "missing_h1": "Missing H1 Tag",
            "missing_viewport": "Missing Mobile Viewport", "missing_canonical": "Missing Canonical Tag",
            "missing_jsonld": "Missing Structured Data", "missing_hreflang": "Missing Hreflang",
            "soft_404": "Suspected Soft 404", "missing_alt": "Images Missing Alt Text",
            "duplicate": "Duplicate Content Detected"
        }
    }
    t = txt[lang]
    
    for script in soup(["script", "style"]): script.extract()
    text_content = soup.get_text().strip()
    content_hash = get_content_hash(text_content)

    # --- 1. Hreflang æ·±åº¦éªŒè¯ ---
    hreflangs = soup.find_all('link', hreflang=True)
    if hreflangs:
        has_x_default = False
        invalid_codes = []
        code_pattern = re.compile(r'^[a-z]{2}(-[a-zA-Z]{2})?$|x-default', re.IGNORECASE)
        
        for link in hreflangs:
            code = link.get('hreflang', '').strip()
            if code.lower() == 'x-default': has_x_default = True
            if not code_pattern.match(code): invalid_codes.append(code)
        
        if invalid_codes:
            issues.append({"severity": "High", "title": t["hreflang_invalid"], "desc": f"{t['hreflang_invalid_desc']} Found: {', '.join(invalid_codes[:3])}", "suggestion": "Use ISO 639-1 codes.", "url": url})
        if not has_x_default:
            issues.append({"severity": "Low", "title": t["hreflang_no_default"], "desc": t["hreflang_no_default_desc"], "suggestion": "Add hreflang='x-default'.", "url": url})
    elif not sitemap_has_hreflang:
        issues.append({"severity": "Low", "title": t["missing_hreflang"], "desc": "No hreflang in HTML or Sitemap.", "suggestion": "Add hreflang tags.", "url": url})

    # --- 2. å›¾ç‰‡ Alt è´¨é‡æ£€æŸ¥ & CWV (CLS) ---
    images = soup.find_all('img')
    missing_alt = 0
    bad_alt_count = 0
    cls_risk_count = 0
    bad_keywords = ["image", "photo", "picture", "img", "untitled", ".jpg", ".png"]
    
    for img in images:
        alt = img.get('alt', '').strip()
        if not alt: 
            missing_alt += 1
        else:
            if len(alt) < 3 or any(bk in alt.lower() for bk in bad_keywords):
                bad_alt_count += 1
        
        if not img.get('width') or not img.get('height'):
            cls_risk_count += 1

    if missing_alt > 0:
        issues.append({"severity": "Medium", "title": t["missing_alt"], "desc": f"{missing_alt} images missing alt.", "suggestion": "Add descriptive alt text.", "url": url})
    if bad_alt_count > 0:
        issues.append({"severity": "Low", "title": t["alt_bad_quality"], "desc": t["alt_bad_quality_desc"], "suggestion": "Avoid generic keywords.", "url": url, "evidence": f"{bad_alt_count} poor alts"})
    if cls_risk_count > 0:
        issues.append({"severity": "Medium", "title": t["cls_risk"], "desc": t["cls_risk_desc"], "suggestion": "Always specify width/height.", "url": url, "evidence": f"{cls_risk_count} images without dims"})

    # --- 3. é”šæ–‡æœ¬è´¨é‡æ£€æŸ¥ ---
    links = soup.find_all('a', href=True)
    bad_anchors = ["click here", "read more", "learn more", "more", "here", "link", "ç‚¹å‡»è¿™é‡Œ", "æ›´å¤š", "è¯¦æƒ…"]
    found_bad_anchors = []
    
    for link in links:
        anchor_text = link.get_text().strip().lower()
        if anchor_text in bad_anchors:
            found_bad_anchors.append(anchor_text)
