import streamlit as st
import time
import pandas as pd
import requests
import hashlib
import re
import urllib3
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
    from pptx.enum.shapes import MSO_SHAPE
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. æ’åºé€»è¾‘é…ç½® (Strict Logic Flow) ---
# é¡ºåºï¼šè®¿é—® -> ç´¢å¼• -> æŠ€æœ¯ -> å†…å®¹ -> ä½“éªŒ -> æ€§èƒ½(CWVå‹è½´)
CATEGORY_ORDER = ["access", "indexability", "technical", "content", "image_ux", "cwv_performance"]
SEVERITY_ORDER = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}

# é—®é¢˜ä¼˜å…ˆçº§åˆ—è¡¨ (ç”¨äºåŒç±»é—®é¢˜æ’åº)
ISSUE_PRIORITY_LIST = [
    # 1. Access
    "no_robots", "robots_bad_rule", "robots_no_sitemap", "no_sitemap", "sitemap_invalid",
    "http_5xx", "http_4xx", "soft_404", "http_3xx",
    
    # 2. Indexability
    "duplicate", "missing_canonical", "hreflang_invalid", "hreflang_no_default", "missing_hreflang",
    
    # 3. Technical
    "missing_viewport", "missing_jsonld", "js_links", "url_underscore", "url_uppercase",
    
    # 4. Content
    "missing_title", "short_title", "long_title",
    "missing_desc", "short_desc",
    "missing_h1",
    
    # 5. UX & Assets
    "no_favicon", "missing_alt", "alt_bad_quality", "anchor_bad_quality", "cls_risk",
    
    # 6. CWV Performance
    "lcp_issue", "inp_issue", "cls_issue"
]

def get_issue_priority(issue_id):
    try:
        return ISSUE_PRIORITY_LIST.index(issue_id)
    except ValueError:
        return 999 

# --- 3. å›½é™…åŒ–å­—å…¸ (i18n) ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "æ——èˆ°å®¡è®¡ç‰ˆ v4.9",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        "sitemap_status_title": "Sitemap çŠ¶æ€:",
        "sitemap_found_href": "âœ… å‘ç° Hreflang é…ç½®", 
        "sitemap_no_href": "âš ï¸ æœªå‘ç° Hreflang",     
        "sitemap_missing": "âŒ æœªæ‰¾åˆ° Sitemap",

        "psi_settings": "Google PSI API è®¾ç½® (å¯é€‰)",
        "psi_api_key_label": "è¾“å…¥ Google PageSpeed API Key",
        "psi_api_help": "å»ºè®®å¡«å…¥ä»¥è·å– LCP/CLS/INP çœŸå®æ•°æ®ã€‚ç•™ç©ºåˆ™åªè¿›è¡Œä»£ç å®¡è®¡ã€‚",
        "psi_get_key": "æ²¡æœ‰ API Key? [ç‚¹å‡»è¿™é‡Œå…è´¹ç”³è¯·](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "æ­£åœ¨è°ƒç”¨ Google API è·å–é¦–é¡µçœŸå® CWV æ•°æ®...",
        "psi_success": "æˆåŠŸè·å–çœŸå®ç”¨æˆ·æ•°æ®ï¼",
        "psi_error": "API è°ƒç”¨å¤±è´¥æˆ–æ—  CrUX æ•°æ®",
        
        "input_header": "å¼€å§‹æ·±åº¦å®¡è®¡",
        "input_info": "è¯´æ˜: v4.9 ä¸¥æ ¼éµå¾ªå®¡è®¡é€»è¾‘ (Access->Index->Tech->Content->UX->CWV)ï¼Œä¿®å¤äº†ä¸­è‹±æ–‡æ¡ˆæ··æ‚é—®é¢˜ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°",
        "adv_settings": "é«˜çº§è®¾ç½® (Advanced Settings)", 
        "manual_robots": "æ‰‹åŠ¨ Robots.txt åœ°å€ (å¯é€‰)", 
        "manual_sitemaps": "æ‰‹åŠ¨ Sitemap åœ°å€ (æ¯è¡Œä¸€ä¸ª, å¯é€‰)", 
        "start_btn": "å¼€å§‹æ·±åº¦çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨æ‰§è¡Œæ·±åº¦å®¡è®¡ (Max {} pages)...", 
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚åŸå› : {}", 
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP çŠ¶æ€ç åˆ†å¸ƒ",
        "cwv_title": "é¦–é¡µæ ¸å¿ƒ Web æŒ‡æ ‡ (Core Web Vitals) - çœŸå®æ•°æ®",
        "cwv_source": "æ•°æ®æ¥æº: Google Chrome User Experience Report (CrUX) - ä»…é¦–é¡µ",
        
        "matrix_header": "çˆ¬å–æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼ŒåŒ…å«é‡å¤å†…å®¹æº¯æºåŠå¯è§†åŒ–é¢„è§ˆã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_category": "åˆ†ç±»:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "åœ¨å·²çˆ¬å–æ ·æœ¬ä¸­å‘ç° **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "ğŸ”´ é—®é¢˜æè¿°:",
        "ppt_business_impact": "ğŸ“‰ å•†ä¸šä¸ SEO å½±å“:", 
        "ppt_sugg": "âœ… ä¿®å¤å»ºè®®:",
        "ppt_examples": "ğŸ” å—å½±å“é¡µé¢ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        
        # Categories Labels
        "cat_access": "1. å¯è®¿é—®æ€§ä¸ç´¢å¼• (Access & Indexing)",
        "cat_indexability": "2. ç´¢å¼•è§„èŒƒæ€§ (Indexability)",
        "cat_technical": "3. æŠ€æœ¯ä¸æ¶æ„ (Technical SEO)",
        "cat_content": "4. é¡µé¢å†…å®¹ (On-Page Content)",
        "cat_image_ux": "5. ç”¨æˆ·ä½“éªŒä¸èµ„æº (UX & Assets)",
        "cat_cwv_performance": "6. æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ (Core Web Vitals)",

        "ppt_cover_title": "SEO æ·±åº¦æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro v4.9",
        "ppt_slide_desc_title": "æ·±åº¦åˆ†æ",
        "ppt_slide_count_title": "æ ·æœ¬ä¸­å—å½±å“é¡µé¢æ•°: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹", 
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
        "serp_sim_title": "Google æœç´¢ç»“æœæ¨¡æ‹Ÿ (SERP):",
        "rich_sim_title": "å¯Œåª’ä½“ç»“æœæ¨¡æ‹Ÿ (Rich Results):",

        # Issues
        "lcp_issue": "LCP (æœ€å¤§å†…å®¹ç»˜åˆ¶) è¶…æ ‡", "lcp_desc": "LCP ä¸º {:.2f}s (ç›®æ ‡ <2.5s)ã€‚åŠ è½½è¿‡æ…¢ã€‚", "lcp_impact": "æ ¸å¿ƒæ’åå› ç´ ã€‚é«˜è·³å‡ºç‡ã€‚", "lcp_sugg": "ä¼˜åŒ–å›¾ç‰‡å¤§å° (WebP)ï¼Œä½¿ç”¨ CDNï¼Œæ¨è¿Ÿéå…³é”® JSï¼Œé¢„åŠ è½½ LCP å…ƒç´ ã€‚",
        "cls_issue": "CLS (ç´¯ç§¯å¸ƒå±€åç§») è¶…æ ‡", "cls_desc": "CLS ä¸º {:.3f} (ç›®æ ‡ <0.1)ã€‚è§†è§‰ä¸ç¨³å®šã€‚", "cls_impact": "æ ¸å¿ƒæ’åå› ç´ ã€‚ç”¨æˆ·ä½“éªŒå·®ã€‚", "cls_sugg": "æŒ‡å®šå›¾ç‰‡å®½é«˜ï¼Œé¿å…åŠ¨æ€å†…å®¹ã€‚",
        "inp_issue": "INP (äº¤äº’å»¶è¿Ÿ) è¶…æ ‡", "inp_desc": "INP ä¸º {}ms (ç›®æ ‡ <200ms)ã€‚å“åº”è¿Ÿé’ã€‚", "inp_impact": "æ ¸å¿ƒæ’åå› ç´ ã€‚å½±å“è½¬åŒ–ã€‚", "inp_sugg": "ä¼˜åŒ– JS æ‰§è¡Œï¼Œå‡å°‘ä¸»çº¿ç¨‹é˜»å¡ã€‚",
        
        "no_robots": "ç¼ºå¤± Robots.txt", "no_robots_desc": "æ— æ³•è®¿é—® robots.txtã€‚", "no_robots_impact": "çˆ¬å–ä¸å¯æ§ï¼Œæµªè´¹é¢„ç®—ã€‚", "no_robots_sugg": "åˆ›å»ºæ–‡ä»¶ã€‚",
        "robots_bad_rule": "Robots.txt å°ç¦é£é™©", "robots_desc": "æ£€æµ‹åˆ°å…¨ç«™å°ç¦è§„åˆ™ã€‚", "robots_impact": "å¯¼è‡´ä¸æ”¶å½•ã€‚", "robots_bad_rule_sugg": "ç§»é™¤ Disallow: / è§„åˆ™ã€‚",
        "robots_no_sitemap": "Robots æœªå£°æ˜ Sitemap", "robots_no_sitemap_desc": "æœªæŒ‡æ˜ Sitemap ä½ç½®ã€‚", "robots_no_sitemap_impact": "å‘ç°é¡µé¢å˜æ…¢ã€‚", "robots_no_sitemap_sugg": "æ·»åŠ  Sitemap æŒ‡ä»¤ã€‚",
        "no_sitemap": "Sitemap è®¿é—®å¤±è´¥", "no_sitemap_desc": "æ— æ³•è®¿é—® Sitemapã€‚", "no_sitemap_impact": "æ·±å±‚é¡µé¢éš¾å‘ç°ã€‚", "no_sitemap_sugg": "æ£€æŸ¥ Sitemap URLã€‚",
        "sitemap_invalid": "Sitemap æ ¼å¼é”™è¯¯", "sitemap_invalid_desc": "XML è§£æå¤±è´¥ã€‚", "sitemap_invalid_impact": "Sitemap å¤±æ•ˆã€‚", "sitemap_invalid_sugg": "ä¿®å¤ XML è¯­æ³•ã€‚",
        "no_favicon": "ç¼ºå¤± Favicon", "no_favicon_desc": "é¦–é¡µæ— å›¾æ ‡ã€‚", "no_favicon_impact": "é™ä½ SERP ç‚¹å‡»ç‡ã€‚", "no_favicon_sugg": "é…ç½® faviconã€‚",
        
        "duplicate": "å‘ç°æœªè§„èŒƒåŒ–çš„é‡å¤å†…å®¹", "duplicate_desc": "å†…å®¹é«˜åº¦é‡å¤ä¸” Canonical æœªç»Ÿä¸€ã€‚", "duplicate_impact": "æƒé‡åˆ†æ•£ï¼Œå…³é”®è¯ç«äº‰ã€‚", "duplicate_sugg": "ä½¿ç”¨ Canonical æŒ‡å‘åŸä»¶ã€‚",
        
        "3xx_title": "å†…éƒ¨é“¾æ¥é‡å®šå‘ (3xx)", "3xx_desc": "é“¾æ¥å‘ç”Ÿè·³è½¬ã€‚", "3xx_impact": "æµªè´¹çˆ¬å–é¢„ç®—ã€‚", "3xx_sugg": "æ›´æ–°é“¾æ¥ç›´æ¥æŒ‡å‘ç›®æ ‡ã€‚",
        "4xx_title": "æ­»é“¾ (4xx)", "4xx_desc": "é“¾æ¥è¿”å› 404ã€‚", "4xx_impact": "ç ´åä½“éªŒï¼Œæƒé‡æµå¤±ã€‚", "4xx_sugg": "ä¿®å¤æ­»é“¾ã€‚",
        "5xx_title": "æœåŠ¡å™¨é”™è¯¯ (5xx)", "5xx_desc": "æœåŠ¡å™¨æŠ¥é”™ã€‚", "5xx_impact": "ä¸¥é‡å½±å“æ”¶å½•ã€‚", "5xx_sugg": "æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—ã€‚",
        
        "hreflang_invalid": "Hreflang æ ¼å¼é”™è¯¯", "hreflang_invalid_desc": "ä»£ç ä¸ç¬¦åˆ ISO æ ‡å‡†ã€‚", "hreflang_invalid_impact": "å›½é™…åŒ–å¤±æ•ˆã€‚", "hreflang_invalid_sugg": "ä½¿ç”¨æ ‡å‡† ISO ä»£ç  (å¦‚ en-US)ã€‚",
        "hreflang_no_default": "Hreflang ç¼ºå¤± x-default", "hreflang_no_default_desc": "æ— é»˜è®¤å›é€€ã€‚", "hreflang_no_default_impact": "è¯­è¨€åŒ¹é…é”™è¯¯ã€‚", "hreflang_no_default_sugg": "æ·»åŠ  x-default æ ‡è®°ã€‚",
        
        "alt_bad_quality": "å›¾ç‰‡ Alt è´¨é‡å·®", "alt_bad_quality_desc": "ä½¿ç”¨æ— æ„ä¹‰è¯æ±‡ã€‚", "alt_bad_quality_impact": "é”™å¤±å›¾ç‰‡æµé‡ã€‚", "alt_bad_quality_sugg": "ä½¿ç”¨æè¿°æ€§å…³é”®è¯ã€‚",
        "anchor_bad_quality": "é”šæ–‡æœ¬è´¨é‡å·®", "anchor_bad_quality_desc": "ä½¿ç”¨é€šç”¨è¯æ±‡ã€‚", "anchor_bad_quality_impact": "æ— æ³•ä¼ é€’ç›¸å…³æ€§ã€‚", "anchor_bad_quality_sugg": "ä½¿ç”¨æè¿°æ€§é”šæ–‡æœ¬ã€‚",
        
        "cls_risk": "CLS å¸ƒå±€åç§»é£é™© (é™æ€)", "cls_risk_desc": "Img ç¼ºå¤±å®½é«˜å±æ€§ã€‚", "cls_risk_impact": "å¯¼è‡´é¡µé¢æŠ–åŠ¨ã€‚", "cls_risk_sugg": "ä¸ºå›¾ç‰‡æŒ‡å®š width å’Œ heightã€‚",
        
        "missing_title": "ç¼ºå¤± Title", "missing_title_desc": "æ— æ ‡é¢˜æ ‡ç­¾ã€‚", "missing_title_impact": "ä¸¥é‡å½±å“æ’åã€‚", "missing_title_sugg": "æ·»åŠ æ ‡é¢˜ã€‚",
        "short_title": "æ ‡é¢˜è¿‡çŸ­", "short_title_desc": "æ ‡é¢˜å¤ªçŸ­ã€‚", "short_title_impact": "è¦†ç›–è¯å°‘ã€‚", "short_title_sugg": "æ‰©å……æ ‡é¢˜ã€‚",
        "long_title": "æ ‡é¢˜è¿‡é•¿", "long_title_desc": "æ ‡é¢˜ >60 å­—ç¬¦ã€‚", "long_title_impact": "SERP æˆªæ–­ã€‚", "long_title_sugg": "ç²¾ç®€æ ‡é¢˜é•¿åº¦ã€‚",
        
        "missing_desc": "ç¼ºå¤± Meta Description", "missing_desc_desc": "æ— æè¿°ã€‚", "missing_desc_impact": "é™ä½ç‚¹å‡»ç‡ã€‚", "missing_desc_sugg": "æ·»åŠ æè¿°ã€‚",
        "short_desc": "Meta Description è¿‡çŸ­", "short_desc_desc": "å†…å®¹å¤ªå°‘ã€‚", "short_desc_impact": "å¸å¼•åŠ›ä¸è¶³ã€‚", "short_desc_sugg": "æ‰©å……æè¿°ã€‚",
        
        "missing_h1": "ç¼ºå¤± H1", "missing_h1_desc": "æ—  H1ã€‚", "missing_h1_impact": "ç»“æ„ä¸æ¸…ã€‚", "missing_h1_sugg": "æ·»åŠ  H1ã€‚",
        
        "missing_viewport": "ç¼ºå¤± Viewport", "missing_viewport_desc": "æ— ç§»åŠ¨ç«¯é…ç½®ã€‚", "missing_viewport_impact": "ç§»åŠ¨ç«¯æ’åä¸‹é™ã€‚", "missing_viewport_sugg": "æ·»åŠ  Viewport æ ‡ç­¾ã€‚",
        
        "missing_canonical": "ç¼ºå¤± Canonical", "missing_canonical_desc": "æ— è§„èŒƒæ ‡ç­¾ã€‚", "missing_canonical_impact": "é‡å¤å†…å®¹é£é™©ã€‚", "missing_canonical_sugg": "æ·»åŠ  Canonical æ ‡ç­¾ã€‚",
        
        "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®", "missing_jsonld_desc": "æœªæ£€æµ‹åˆ° Schema æ ‡è®°ã€‚", "missing_jsonld_impact": "æ— å¯Œåª’ä½“ç»“æœã€‚", 
        "missing_jsonld_sugg": "æ ¹æ®é¡µé¢ç±»å‹æ·»åŠ  JSON-LDï¼š\n- äº§å“é¡µ: Product\n- æ–‡ç« é¡µ: Article\n- é¦–é¡µ: Organization",
        
        "missing_hreflang": "ç¼ºå¤± Hreflang", "missing_hreflang_desc": "æ— è¯­è¨€æ ‡è®°ã€‚", "missing_hreflang_impact": "å›½é™…åŒ–å·®ã€‚", "missing_hreflang_sugg": "æ·»åŠ  Hreflang æ ‡ç­¾ã€‚",
        
        "soft_404": "ç–‘ä¼¼è½¯ 404", "soft_404_desc": "å‡ 200 é¡µé¢ã€‚", "soft_404_impact": "æµªè´¹é¢„ç®—ã€‚", "soft_404_sugg": "é…ç½® 404 çŠ¶æ€ç ã€‚",
        
        "missing_alt": "ç¼ºå¤± Alt", "missing_alt_desc": "å›¾ç‰‡æ—  Altã€‚", "missing_alt_impact": "ä¸åˆ©å›¾ç‰‡ SEOã€‚", "missing_alt_sugg": "æ·»åŠ  Alt å±æ€§ã€‚",
        
        "js_links": "JS ä¼ªé“¾æ¥", "js_links_desc": "href=javascriptã€‚", "js_links_impact": "çˆ¬è™«æ— æ³•æŠ“å–ã€‚", "js_links_sugg": "ä½¿ç”¨æ ‡å‡†é“¾æ¥ã€‚",
        
        "url_underscore": "URL å«ä¸‹åˆ’çº¿", "url_underscore_desc": "ä½¿ç”¨ _ã€‚", "url_underscore_impact": "åˆ†è¯å›°éš¾ã€‚", "url_underscore_sugg": "ä½¿ç”¨è¿å­—ç¬¦ -ã€‚",
        "url_uppercase": "URL å«å¤§å†™", "url_uppercase_desc": "æ··ç”¨å¤§å°å†™ã€‚", "url_uppercase_impact": "é‡å¤æ”¶å½•é£é™©ã€‚", "url_uppercase_sugg": "è½¬å°å†™ã€‚"
    },
    "en": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "Deep Audit Edition v4.9",
        "nav_label": "Navigation",
        "nav_options": ["Input URL", "Dashboard", "Data Matrix", "PPT Generator"],
        "lang_label": "Language / è¯­è¨€",
        "clear_data": "Clear Data & Reset",
        "cache_info": "Cached {} pages",
        "sitemap_status_title": "Sitemap Status:",
        "sitemap_found_href": "âœ… Hreflang Found", 
        "sitemap_no_href": "âš ï¸ No Hreflang",       
        "sitemap_missing": "âŒ Sitemap Missing",
        
        "psi_settings": "Google PSI API Settings (Optional)",
        "psi_api_key_label": "Enter Google PageSpeed API Key",
        "psi_api_help": "Leave empty for static check only.",
        "psi_get_key": "No API Key? [Get one for free here](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "Fetching real CWV data...",
        "psi_success": "Real user data fetched!",
        "psi_error": "API Failed or No CrUX Data",
        
        "input_header": "Start Deep Audit",
        "input_info": "Note: v4.9 features strict logical ordering (Access -> Tech -> Content -> UX -> CWV).",
        "input_label": "Target URL",
        "input_placeholder": "https://example.com",
        "max_pages_label": "Max Pages to Crawl",
        "adv_settings": "Advanced Settings", 
        "manual_robots": "Manual Robots.txt URL (Optional)", 
        "manual_sitemaps": "Manual Sitemap URLs (One per line, Optional)", 
        "start_btn": "Start Deep Crawl",
        "error_url": "Invalid URL format",
        "spinner_crawl": "Running Deep Audit (Max {} pages)...", 
        "error_no_data": "No pages crawled. Reason: {}", 
        "success_audit": "Audit Complete! Analyzed {} pages.",
        
        "dashboard_header": "Executive Summary",
        "warn_no_data": "No data available.",
        "kpi_health": "Health Score",
        "kpi_pages": "Analyzed Pages",
        "kpi_issues": "Total Issues",
        "kpi_critical": "Critical Issues",
        "chart_issues": "Issue Distribution",
        "chart_no_issues": "No significant issues found.",
        "chart_status": "HTTP Status Codes",
        "cwv_title": "Core Web Vitals - Real User Data (Home Only)",
        "cwv_source": "Source: Google Chrome User Experience Report (CrUX)",
        
        "matrix_header": "Crawled Data Matrix",
        "download_csv": "Download CSV Report",
        
        "ppt_header": "Pitch Deck Preview",
        "ppt_success_no_issues": "No critical issues found.",
        "ppt_download_header": "ğŸ“¥ Export Report",
        "ppt_info": "Note: PPT optimized for 16:9 with logical issue ordering.",
        "ppt_btn": "Generate & Download .pptx",
        "ppt_preview_header": "Web Preview",
        "ppt_slide_title": "Issue Type:",
        "ppt_category": "Category:",
        "ppt_severity": "Severity:",
        "ppt_impact": "Impact:",
        "ppt_impact_desc": "Affects **{}** pages in crawled sample.",
        "ppt_desc": "Description:",
        "ppt_business_impact": "ğŸ“‰ Business & SEO Impact:",
        "ppt_sugg": "ğŸ’¡ Suggestion:",
        "ppt_examples": "ğŸ” Examples:",
        "ppt_prev": "â¬…ï¸ Previous",
        "ppt_next": "Next â¡ï¸",
        
        # Categories
        "cat_access": "1. Access & Indexing",
        "cat_indexability": "2. Indexability",
        "cat_technical": "3. Technical SEO",
        "cat_content": "4. On-Page Content",
        "cat_image_ux": "5. UX & Assets",
        "cat_cwv_performance": "6. Core Web Vitals (Performance)",
        
        "ppt_cover_title": "SEO Technical Audit",
        "ppt_cover_sub": "Generated by AuditAI Pro v4.9",
        "ppt_slide_desc_title": "Description & Impact",
        "ppt_slide_count_title": "Affected Pages (in sample): {}",
        "ppt_slide_ex_title": "Affected Page Examples",
        "ppt_slide_sugg_title": "ğŸ’¡ Recommendation:",
        "serp_sim_title": "Google SERP Simulation:",
        "rich_sim_title": "Rich Results Simulation:",

        # Issues (Full sentences)
        "lcp_issue": "LCP Fails", "lcp_desc": "LCP is {:.2f}s (Target <2.5s). Main content loads too slowly.", "lcp_impact": "Core Ranking Factor. High bounce rates.", "lcp_sugg": "Optimize images, use CDN, defer JS.",
        "cls_issue": "CLS Fails", "cls_desc": "CLS is {:.3f} (Target <0.1). Layout shifts detected.", "cls_impact": "Ranking Factor. Bad user experience.", "cls_sugg": "Set width/height for media.",
        "inp_issue": "INP Fails", "inp_desc": "INP is {}ms (Target <200ms). Slow interactivity.", "inp_impact": "Ranking Factor. Low conversion rates.", "inp_sugg": "Optimize JS execution.",
        
        "no_robots": "Missing Robots.txt", "no_robots_desc": "Robots.txt not found.", "no_robots_impact": "Crawling cannot be controlled.", "no_robots_sugg": "Create robots.txt.",
        "robots_bad_rule": "Robots Blocking", "robots_desc": "Global disallow rule found.", "robots_impact": "Site will be de-indexed.", "robots_bad_rule_sugg": "Remove Disallow: /.",
        "robots_no_sitemap": "Sitemap Missing in Robots", "robots_no_sitemap_desc": "Sitemap link missing.", "robots_no_sitemap_impact": "Slow content discovery.", "robots_no_sitemap_sugg": "Add Sitemap directive.",
        "no_sitemap": "Sitemap Failed", "no_sitemap_desc": "Access denied.", "no_sitemap_impact": "Poor indexing coverage.", "no_sitemap_sugg": "Check Sitemap URL.",
        "sitemap_invalid": "Invalid Sitemap", "sitemap_invalid_desc": "XML syntax error.", "sitemap_invalid_impact": "Unreadable sitemap.", "sitemap_invalid_sugg": "Validate XML.",
        "no_favicon": "Missing Favicon", "no_favicon_desc": "No icon on homepage.", "no_favicon_impact": "Lower CTR in SERPs.", "no_favicon_sugg": "Add favicon tag.",
        
        "duplicate": "Duplicate Content", 
        "duplicate_desc": "Duplicate found without canonical.", 
        "duplicate_impact": "Keyword cannibalization & diluted equity.", 
        "duplicate_sugg": "Use canonical tags.",
        
        "3xx_title": "Redirect Chain", "3xx_desc": "Internal link redirects.", "3xx_impact": "Wasted budget & latency.", "3xx_sugg": "Update links directly.",
        "4xx_title": "Broken Link", "4xx_desc": "Link returns 4xx error.", "4xx_impact": "Bad UX & equity loss.", "4xx_sugg": "Fix broken links.",
        "5xx_title": "Server Error", "5xx_desc": "Link returns 5xx error.", "5xx_impact": "Google reduces crawl rate.", "5xx_sugg": "Check server logs.",
        
        "hreflang_invalid": "Invalid Hreflang", "hreflang_invalid_desc": "Invalid ISO code.", "hreflang_invalid_impact": "Targeting fails.", "hreflang_invalid_sugg": "Use standard codes.",
        "hreflang_no_default": "No x-default", "hreflang_no_default_desc": "Missing fallback.", "hreflang_no_default_impact": "Wrong lang shown.", "hreflang_no_default_sugg": "Add x-default.",
        
        "alt_bad_quality": "Bad Alt Text", "alt_bad_quality_desc": "Generic text used.", "alt_bad_quality_impact": "No image SEO value.", "alt_bad_quality_sugg": "Use descriptive text.",
        "anchor_bad_quality": "Bad Anchor", "anchor_bad_quality_desc": "Generic anchor text.", "anchor_bad_quality_impact": "No context passed.", "anchor_bad_quality_sugg": "Use keywords.",
        
        "cls_risk": "CLS Risk (Static)", "cls_risk_desc": "Missing dimensions.", "cls_risk_impact": "Layout shifts.", "cls_risk_sugg": "Add width/height.",
        
        "missing_title": "Missing Title", "missing_title_desc": "No title tag.", "missing_title_impact": "Ranking loss.", "missing_title_sugg": "Add title.",
        "short_title": "Title Short", "short_title_desc": "Too short.", "short_title_impact": "Missed keywords.", "short_title_sugg": "Expand title.",
        "long_title": "Title Long", "long_title_desc": "Too long.", "long_title_impact": "Truncated.", "long_title_sugg": "Shorten title.",
        
        "missing_desc": "Missing Desc", "missing_desc_desc": "No meta desc.", "missing_desc_impact": "Lower CTR.", "missing_desc_sugg": "Add description.",
        "short_desc": "Desc Short", "short_desc_desc": "Too thin.", "short_desc_impact": "Low appeal.", "short_desc_sugg": "Expand description.",
        
        "missing_h1": "Missing H1", "missing_h1_desc": "No H1.", "missing_h1_impact": "Poor structure.", "missing_h1_sugg": "Add H1.",
        
        "missing_viewport": "No Viewport", "missing_viewport_desc": "Not mobile friendly.", "missing_viewport_impact": "Ranking drop.", "missing_viewport_sugg": "Add viewport.",
        
        "missing_canonical": "No Canonical", "missing_canonical_desc": "Missing tag.", "missing_canonical_impact": "Duplicate risk.", "missing_canonical_sugg": "Add canonical.",
        
        "missing_jsonld": "No Schema", "missing_jsonld_desc": "No JSON-LD.", "missing_jsonld_impact": "No rich results.", 
        "missing_jsonld_sugg": "Add JSON-LD for page type.",
        
        "missing_hreflang": "No Hreflang", "missing_hreflang_desc": "Missing tags.", "missing_hreflang_impact": "Poor geo-targeting.", "missing_hreflang_sugg": "Add hreflang.",
        
        "soft_404": "Soft 404", "soft_404_desc": "Fake 200.", "soft_404_impact": "Wasted budget.", "soft_404_sugg": "Use 404.",
        
        "missing_alt": "Missing Alt", "missing_alt_desc": "No alt text.", "missing_alt_impact": "Bad image SEO.", "missing_alt_sugg": "Add alt attributes.",
        
        "js_links": "JS Links", "js_links_desc": "Uncrawlable.", "js_links_impact": "Broken graph.", "js_links_sugg": "Use standard links.",
        
        "url_underscore": "URL Underscores", "url_underscore_desc": "Has _.", "url_underscore_impact": "Bad parsing.", "url_underscore_sugg": "Use hyphens.",
        "url_uppercase": "URL Uppercase", "url_uppercase_desc": "Has Upper.", "url_uppercase_impact": "Duplicate risk.", "url_uppercase_sugg": "Lowercase."
    }
}

# --- 4. çˆ¬è™«æ ¸å¿ƒå¼•æ“ ---
def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except: return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def get_browser_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Connection': 'keep-alive',
    }

def fetch_psi_data(url, api_key):
    if not api_key: return None
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available for this URL."}
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else: return {"error": f"API Error: {response.status_code}"}
    except Exception as e: return {"error": str(e)}

# --- Analyze CWV Data for Issues ---
def check_cwv_issues(cwv_data, url, lang="zh"):
    issues = []
    if not cwv_data or "error" in cwv_data: return issues
    
    t = TRANSLATIONS[lang]
    # ä½¿ç”¨ç»Ÿä¸€çš„ category_key
    category_key = "cwv_performance"

    # LCP
    lcp = cwv_data.get("LCP", 0)
    if lcp > 2.5:
        severity = "Critical" if lcp > 4.0 else "High"
        issues.append({
            "id": "lcp_issue", "category": category_key, "severity": severity,
            "title": t["lcp_issue"], "desc": t["lcp_desc"].format(lcp), "impact": t["lcp_impact"], "suggestion": t["lcp_sugg"],
            "url": url, "examples": [url] 
        })
        
    # INP (Logic Order: LCP -> INP -> CLS)
    inp = cwv_data.get("INP", 0)
    if inp > 200:
        severity = "Critical" if inp > 500 else "High"
        issues.append({
            "id": "inp_issue", "category": category_key, "severity": severity,
            "title": t["inp_issue"], "desc": t["inp_desc"].format(inp), "impact": t["inp_impact"], "suggestion": t["inp_sugg"],
            "url": url, "examples": [url]
        })

    # CLS
    cls = cwv_data.get("CLS", 0)
    if cls > 0.1:
        severity = "Critical" if cls > 0.25 else "High"
        issues.append({
            "id": "cls_issue", "category": category_key, "severity": severity,
            "title": t["cls_issue"], "desc": t["cls_desc"].format(cls), "impact": t["cls_impact"], "suggestion": t["cls_sugg"],
            "url": url, "examples": [url]
        })
        
    return issues

def check_site_level_assets(start_url, lang="zh", manual_robots=None, manual_sitemaps=None):
    issues = []
    sitemap_has_hreflang = False
    parsed_url = urlparse(start_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    headers = get_browser_headers()
    t = TRANSLATIONS[lang]

    # Robots.txt
    robots_url = manual_robots if manual_robots else urljoin(base_url, "/robots.txt")
    try:
        r = requests.get(robots_url, headers=headers, timeout=10, allow_redirects=True, stream=True, verify=False)
        if r.status_code != 200:
            issues.append({"id": "no_robots", "category": "access", "severity": "Medium", "title": t["no_robots"], "desc": t["no_robots_desc"], "impact": t["no_robots_impact"], "suggestion": t["no_robots_sugg"], "url": robots_url, "examples": [robots_url]})
        else:
            content = r.text.lower()
            if "disallow: /" in content and "allow:" not in content:
                 issues.append({"id": "robots_bad_rule", "category": "access", "severity": "Critical", "title": t["robots_bad_rule"], "desc": t["robots_desc"], "impact": t["robots_impact"], "suggestion": t["robots_bad_rule_sugg"], "url": robots_url, "examples": [robots_url]})
            if "sitemap:" not in content:
                 issues.append({"id": "robots_no_sitemap", "category": "access", "severity": "Low", "title": t["robots_no_sitemap"], "desc": t["robots_no_sitemap_desc"], "impact": t["robots_no_sitemap_impact"], "suggestion": t["robots_no_sitemap_sugg"], "url": robots_url, "examples": [robots_url]})
        r.close()
    except: 
        issues.append({"id": "no_robots", "category": "access", "severity": "Medium", "title": t["no_robots"], "desc": "Connection failed.", "impact": t["no_robots_impact"], "suggestion": t["no_robots_sugg"], "url": robots_url, "examples": [robots_url]})

    # Sitemap
    sitemap_urls_to_check = manual_sitemaps if manual_sitemaps else [urljoin(base_url, "/sitemap.xml")]
    any_sitemap_valid = False
    for sitemap_url in sitemap_urls_to_check:
        sitemap_url = sitemap_url.strip()
        if not sitemap_url: continue
        try:
            r = requests.get(sitemap_url, headers=headers, timeout=15, allow_redirects=True, verify=False)
            if r.status_code == 200:
                try:
                    root = ET.fromstring(r.content)
                    any_sitemap_valid = True
                    if 'xhtml' in r.text or 'hreflang' in r.text:
                        sitemap_has_hreflang = True
                except ET.ParseError:
                    if not sitemap_url.endswith('.gz'): 
                        issues.append({"id": "sitemap_invalid", "category": "access", "severity": "Medium", "title": t["sitemap_invalid"], "desc": t["sitemap_invalid_desc"], "impact": t["sitemap_invalid_impact"], "suggestion": t["sitemap_invalid_sugg"], "url": sitemap_url, "examples": [sitemap_url]})
            else:
                if manual_sitemaps:
                    issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "title": t["no_sitemap"], "desc": f"{t['no_sitemap_desc']} (Status: {r.status_code})", "impact": t["no_sitemap_impact"], "suggestion": t["no_sitemap_sugg"], "url": sitemap_url, "examples": [sitemap_url]})
        except:
            if manual_sitemaps: issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "title": t["no_sitemap"], "desc": "Connection failed.", "impact": t["no_sitemap_impact"], "suggestion": t["no_sitemap_sugg"], "url": sitemap_url, "examples": [sitemap_url]})

    if not any_sitemap_valid and not manual_sitemaps:
         issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "title": t["no_sitemap"], "desc": t["no_sitemap_desc"], "impact": t["no_sitemap_impact"], "suggestion": t["no_sitemap_sugg"], "url": sitemap_urls_to_check[0], "examples": [sitemap_urls_to_check[0]]})

    # Favicon (Category: image_ux -> UX & Assets)
    has_favicon = False
    try:
        resp = requests.get(base_url, headers=headers, timeout=10, allow_redirects=True, verify=False)
        soup = BeautifulSoup(resp.content, 'html.parser')
        if soup.find('link', rel=lambda x: x and 'icon' in x.lower()): has_favicon = True
        else:
            r_ico = requests.get(urljoin(base_url, "/favicon.ico"), headers=headers, timeout=5, allow_redirects=True, stream=True, verify=False)
            if r_ico.status_code == 200 and int(r_ico.headers.get('content-length', 0)) > 0: has_favicon = True
            r_ico.close()
        
        if not has_favicon:
            issues.append({"id": "no_favicon", "category": "image_ux", "severity": "Low", "title": t["no_favicon"], "desc": t["no_favicon_desc"], "impact": t["no_favicon_impact"], "suggestion": t["no_favicon_sugg"], "url": base_url, "examples": [base_url]})
    except: pass

    return issues, sitemap_has_hreflang

def analyze_page(url, html_content, status_code, lang="zh", sitemap_has_hreflang=False):
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    t = TRANSLATIONS[lang]
    
    for script in soup(["script", "style"]): script.extract()
    text_content = soup.get_text().strip()
    content_hash = get_content_hash(text_content)

    # --- Technical ---
    hreflangs = soup.find_all('link', hreflang=True)
    if hreflangs:
        has_x_default = False
        invalid_codes = []
        code_pattern = re.compile(r'^[a-z]{2}(-[a-zA-Z]{2})?$|x-default', re.IGNORECASE)
        for link in hreflangs:
            code = link.get('hreflang', '').strip()
            if code.lower() == 'x-default': has_x_default = True
            if not code_pattern.match(code): invalid_codes.append(code)
        if invalid_codes:
            issues.append({"id": "hreflang_invalid", "category": "indexability", "severity": "High", "title": t["hreflang_invalid"], "desc": f"{t['hreflang_invalid_desc']} ({', '.join(invalid_codes[:3])})", "impact": t["hreflang_invalid_impact"], "suggestion": t["hreflang_invalid_sugg"], "url": url})
        if not has_x_default:
            issues.append({"id": "hreflang_no_default", "category": "indexability", "severity": "Low", "title": t["hreflang_no_default"], "desc": t["hreflang_no_default_desc"], "impact": t["hreflang_no_default_impact"], "suggestion": t["hreflang_no_default_sugg"], "url": url})
    elif not sitemap_has_hreflang:
        issues.append({"id": "missing_hreflang", "category": "indexability", "severity": "Low", "title": t["missing_hreflang"], "desc": t["missing_hreflang_desc"], "impact": t["missing_hreflang_impact"], "suggestion": t["missing_hreflang_sugg"], "url": url})

    if not soup.find('meta', attrs={'name': 'viewport'}):
        issues.append({"id": "missing_viewport", "category": "technical", "severity": "Critical", "title": t["missing_viewport"], "desc": t["missing_viewport_desc"], "impact": t["missing_viewport_impact"], "suggestion": t["missing_viewport_sugg"], "url": url})

    canonical_tag = soup.find('link', attrs={'rel': 'canonical'})
    canonical_url = canonical_tag['href'] if canonical_tag else None
    if not canonical_url:
        issues.append({"id": "missing_canonical", "category": "indexability", "severity": "Medium", "title": t["missing_canonical"], "desc": t["missing_canonical_desc"], "impact": t["missing_canonical_impact"], "suggestion": t["missing_canonical_sugg"], "url": url})

    if not soup.find('script', type='application/ld+json'):
         # Smart Suggestion Logic
         parsed_u = urlparse(url)
         path = parsed_u.path.lower()
         spec_rec = "BreadcrumbList"
         if path == "/" or path == "": spec_rec = "Organization / WebSite"
         elif any(k in path for k in ["product", "shop", "item"]): spec_rec = "Product"
         elif any(k in path for k in ["blog", "news", "article"]): spec_rec = "Article"
         
         custom_sugg = f"{t['missing_jsonld_sugg']}\n(Detected type: {spec_rec})"
         issues.append({"id": "missing_jsonld", "category": "technical", "severity": "Medium", "title": t["missing_jsonld"], "desc": t["missing_jsonld_desc"], "impact": t["missing_jsonld_impact"], "suggestion": custom_sugg, "url": url})

    parsed_url = urlparse(url)
    path = parsed_url.path
    if '_' in path:
         issues.append({"id": "url_underscore", "category": "technical", "severity": "Low", "title": t["url_underscore"], "desc": t["url_underscore_desc"], "impact": t["url_underscore_impact"], "suggestion": t["url_underscore_sugg"], "url": url})
    if any(c.isupper() for c in path):
         issues.append({"id": "url_uppercase", "category": "technical", "severity": "Medium", "title": t["url_uppercase"], "desc": t["url_uppercase_desc"], "impact": t["url_uppercase_impact"], "suggestion": t["url_uppercase_sugg"], "url": url})

    js_links = soup.find_all('a', href=lambda x: x and x.lower().startswith('javascript:'))
    if js_links:
        issues.append({"id": "js_links", "category": "technical", "severity": "High", "title": t["js_links"], "desc": t["js_links_desc"], "impact": t["js_links_impact"], "suggestion": t["js_links_sugg"], "url": url, "meta": f"Count: {len(js_links)}"})

    # --- UX & Assets ---
    images = soup.find_all('img')
    missing_alt = 0
    bad_alt_count = 0
    cls_risk_count = 0
    bad_keywords = ["image", "photo", "picture", "img", "untitled"]
    
    for img in images:
        alt = img.get('alt', '').strip()
        if not alt: missing_alt += 1
        else:
            if len(alt) < 3 or any(bk in alt.lower() for bk in bad_keywords): bad_alt_count += 1
        if not img.get('width') or not img.get('height'): cls_risk_count += 1

    if missing_alt > 0:
        issues.append({"id": "missing_alt", "category": "image_ux", "severity": "Medium", "title": t["missing_alt"], "desc": f"{missing_alt} {t['missing_alt_desc']}", "impact": t["missing_alt_impact"], "suggestion": t["missing_alt_sugg"], "url": url})
    if bad_alt_count > 0:
        issues.append({"id": "alt_bad_quality", "category": "image_ux", "severity": "Low", "title": t["alt_bad_quality"], "desc": t["alt_bad_quality_desc"], "impact": t["alt_bad_quality_impact"], "suggestion": t["alt_bad_quality_sugg"], "url": url, "evidence": f"{bad_alt_count} poor alts"})
    
    # å°† Static CLS Risk å½’ç±»ä¸º cwv_performance
    if cls_risk_count > 0:
        issues.append({"id": "cls_risk", "category": "cwv_performance", "severity": "Medium", "title": t["cls_risk"], "desc": t["cls_risk_desc"], "impact": t["cls_risk_impact"], "suggestion": t["cls_risk_sugg"], "url": url, "evidence": f"{cls_risk_count} images"})

    links = soup.find_all('a', href=True)
    bad_anchors = ["click here", "read more", "learn more", "more", "here"]
    found_bad = []
    for link in links:
        at = link.get_text().strip().lower()
        if at in bad_anchors: found_bad.append(at)
    if found_bad:
        issues.append({"id": "anchor_bad_quality", "category": "image_ux", "severity": "Low", "title": t["anchor_bad_quality"], "desc": f"{t['anchor_bad_quality_desc']} ({len(found_bad)})", "impact": t["anchor_bad_quality_impact"], "suggestion": t["anchor_bad_quality_sugg"], "url": url})

    # --- Content ---
    title_tag = soup.title
    title = title_tag.string.strip() if title_tag and title_tag.string else None
    if not title:
        issues.append({"id": "missing_title", "category": "content", "severity": "High", "title": t["missing_title"], "desc": t["missing_title_desc"], "impact": t["missing_title_impact"], "suggestion": t["missing_title_sugg"], "url": url})
    elif len(title) < 10:
         issues.append({"id": "short_title", "category": "content", "severity": "Medium", "title": t["short_title"], "desc": t["short_title_desc"], "impact": t["short_title_impact"], "suggestion": t["short_title_sugg"], "url": url, "evidence": title})
    elif len(title) > 60:
         issues.append({"id": "long_title", "category": "content", "severity": "Low", "title": t["long_title"], "desc": t["long_title_desc"], "impact": t["long_title_impact"], "suggestion": t["long_title_sugg"], "url": url, "evidence": title})

    meta_desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = meta_desc['content'].strip() if meta_desc and meta_desc.get('content') else None
    if not desc_content:
        issues.append({"id": "missing_desc", "category": "content", "severity": "High", "title": t["missing_desc"], "desc": t["missing_desc_desc"], "impact": t["missing_desc_impact"], "suggestion": t["missing_desc_sugg"], "url": url})
    elif len(desc_content) < 50:
        issues.append({"id": "short_desc", "category": "content", "severity": "Low", "title": t["short_desc"], "desc": t["short_desc_desc"], "impact": t["short_desc_impact"], "suggestion": t["short_desc_sugg"], "url": url, "evidence": desc_content})

    h1 = soup.find('h1')
    if not h1: issues.append({"id": "missing_h1", "category": "content", "severity": "High", "title": t["missing_h1"], "desc": t["missing_h1_desc"], "impact": t["missing_h1_impact"], "suggestion": t["missing_h1_sugg"], "url": url})

    if status_code == 200:
        error_kws = ["page not found", "404 error", "é¡µé¢æœªæ‰¾åˆ°"]
        is_s404 = False
        if title and any(k in title.lower() for k in error_kws): is_s404 = True
        elif soup.find('h1') and any(k in soup.find('h1').get_text().lower() for k in error_kws): is_s404 = True
        if is_s404:
            issues.append({"id": "soft_404", "category": "access", "severity": "Critical", "title": t["soft_404"], "desc": t["soft_404_desc"], "impact": t["soft_404_impact"], "suggestion": t["soft_404_sugg"], "url": url})

    return {
        "URL": url, "Status": status_code, "Title": title or "No Title",
        "H1": h1.get_text().strip() if h1 else "No H1", "Links_Count": len(soup.find_all('a')),
        "Issues_Count": len(issues), "Content_Hash": content_hash, "Canonical": canonical_url
    }, issues, []

def crawl_website(start_url, max_pages=100, lang="zh", manual_robots=None, manual_sitemaps=None, psi_key=None):
    visited = set()
    seen_hashes = {} 
    seen_urls = set()
    queue = [start_url]
    seen_urls.add(start_url)
    results_data = []
    all_issues = []
    first_error = None
    target_domain = None
    
    def clean_url(u): return u.split('?')[0].split('#')[0]
    t = TRANSLATIONS[lang] # Global

    progress_bar = st.progress(0, text="Initializing...")
    sitemap_has_hreflang = False
    
    # 1. Site Level Checks
    try:
        site_issues, sitemap_has_hreflang = check_site_level_assets(
            start_url, lang=lang, manual_robots=manual_robots, manual_sitemaps=manual_sitemaps
        )
        all_issues.extend(site_issues)
        st.session_state['sitemap_hreflang_found'] = sitemap_has_hreflang
    except Exception as e:
        pass

    # 2. CWV
    if psi_key:
        with st.spinner(TRANSLATIONS[lang]["psi_fetching"]):
            cwv_data = fetch_psi_data(start_url, psi_key)
            if cwv_data and "error" not in cwv_data: 
                st.session_state['cwv_data'] = cwv_data
                cwv_issues = check_cwv_issues(cwv_data, start_url, lang)
                all_issues.extend(cwv_issues)
            else: 
                st.session_state['cwv_data'] = None

    pages_crawled = 0
    headers = get_browser_headers()
    
    while queue and pages_crawled < max_pages:
        url = queue.pop(0)
        visited.add(url)
        pages_crawled += 1
        progress = int((pages_crawled / max_pages) * 100)
        progress_bar.progress(progress, text=f"Crawling ({pages_crawled}/{max_pages}): {url}")
        time.sleep(0.1)
        
        try:
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
            current_url = response.url 
            if target_domain is None: target_domain = urlparse(current_url).netloc
            final_status = response.status_code

            # 1. 3xx
            if response.history:
                all_issues.append({"id": "http_3xx", "category": "access", "severity": "Medium", "title": t["3xx_title"], "desc": f"{t['3xx_desc']} -> {current_url}", "impact": t["3xx_impact"], "suggestion": t["3xx_sugg"], "url": url, "examples": [url]})

            # 2. 4xx/5xx
            if final_status >= 400:
                is_5xx = final_status >= 500
                all_issues.append({"id": "http_5xx" if is_5xx else "http_4xx", "category": "access", "severity": "Critical" if is_5xx else "High", "title": t["5xx_title"] if is_5xx else t["4xx_title"], "desc": f"{t['5xx_desc'] if is_5xx else t['4xx_desc']} ({final_status})", "impact": t["5xx_impact"] if is_5xx else t["4xx_impact"], "suggestion": t["5xx_sugg"] if is_5xx else t["4xx_sugg"], "url": url, "examples": [url]})

            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                page_data, page_issues, _ = analyze_page(current_url, response.content, final_status, lang=lang, sitemap_has_hreflang=sitemap_has_hreflang)
                
                if final_status == 200:
                    current_hash = page_data['Content_Hash']
                    current_canonical = page_data['Canonical']
                    current_clean = clean_url(current_url)
                    
                    if current_hash in seen_hashes:
                        original_url = seen_hashes[current_hash]
                        original_clean = clean_url(original_url)
                        
                        if len(current_url) < len(original_url) and '?' not in current_url and '?' in original_url:
                            seen_hashes[current_hash] = current_url
                        elif current_clean == original_clean:
                            pass 
                        else:
                            is_handled = current_canonical and current_canonical != current_url
                            if not is_handled:
                                # é‡å¤å†…å®¹ç‰¹æ®Šæ ‡è®°
                                dup_issue = {
                                    "id": "duplicate", "category": "indexability", 
                                    "severity": "High", "title": t["duplicate"], 
                                    "desc": f"{t['duplicate_desc']}", 
                                    "impact": t["duplicate_impact"], 
                                    "suggestion": t["duplicate_sugg"], 
                                    "url": current_url, 
                                    "meta": f"Duplicate of: {original_url}" # Key for visualization
                                }
                                page_issues.append(dup_issue)
                    else:
                        seen_hashes[current_hash] = current_url

                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                soup = BeautifulSoup(response.content, 'html.parser')
                for a in soup.find_all('a', href=True):
                    link = urljoin(current_url, a['href'])
                    if urlparse(link).netloc == target_domain and link not in seen_urls:
                        if not any(link.lower().endswith(ext) for ext in ['.jpg', '.png', '.pdf', '.zip']):
                            seen_urls.add(link)
                            queue.append(link)
            else:
                if pages_crawled == 1: first_error = f"Content type: {content_type}"
        except Exception as e:
            if pages_crawled == 1: first_error = str(e)
            pass
    
    progress_bar.progress(100, text="Done!")
    time.sleep(0.5)
    progress_bar.empty()
    if not results_data and first_error: return None, None, first_error
    return results_data, all_issues, None

def create_styled_pptx(slides_data, lang="zh"):
    prs = Presentation()
    prs.slide_width = Inches(13.333)
    prs.slide_height = Inches(7.5)
    txt = TRANSLATIONS[lang] 
    
    def set_font(font_obj, size, bold=False, color=None):
        font_obj.size = Pt(size)
        font_obj.name = 'Microsoft YaHei' if lang == "zh" else 'Arial'
        font_obj.bold = bold
        if color: font_obj.color.rgb = color

    def draw_serp_preview(slide, issue_title, evidence, url):
        box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(2.5)) # Moved down
        box.fill.solid()
        box.fill.fore_color.rgb = RGBColor(255, 255, 255)
        box.line.color.rgb = RGBColor(220, 220, 220)
        tf = box.text_frame
        tf.margin_left = Inches(0.2)
        tf.margin_top = Inches(0.2)
        p = tf.add_paragraph()
        domain = urlparse(url).netloc
        p.text = f"{domain} â€º ..."
        set_font(p.font, 12, False, RGBColor(32, 33, 36))
        p = tf.add_paragraph()
        p.space_before = Pt(5)
        display_title = evidence if evidence else "Untitled Page"
        if len(display_title) > 60 and ("Long" in issue_title or "è¿‡é•¿" in issue_title): display_title = display_title[:55] + " ..."
        p.text = display_title
        set_font(p.font, 18, False, RGBColor(26, 13, 171)) 
        p = tf.add_paragraph()
        p.space_before = Pt(3)
        p.text = "Please provide a meta description..."
        set_font(p.font, 14, False, RGBColor(77, 81, 86))
        label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(3), Inches(0.3))
        p = label.text_frame.add_paragraph()
        p.text = txt["serp_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100))

    def draw_rich_snippet_preview(slide, url):
        box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(2.5)) # Moved down
        box.fill.solid()
        box.fill.fore_color.rgb = RGBColor(255, 255, 255)
        box.line.color.rgb = RGBColor(220, 220, 220)
        tf = box.text_frame
        tf.margin_left = Inches(0.2)
        tf.margin_top = Inches(0.2)
        p = tf.add_paragraph()
        p.text = f"{urlparse(url).netloc} â€º product"
        set_font(p.font, 12, False, RGBColor(32, 33, 36))
        p = tf.add_paragraph()
        p.space_before = Pt(5)
        p.text = "Product Name Example - Best Choice"
        set_font(p.font, 18, False, RGBColor(26, 13, 171)) 
        p = tf.add_paragraph()
        p.space_before = Pt(3)
        p.text = "â˜…â˜…â˜…â˜…â˜… Rating: 4.8 Â· $199.00 Â· In stock"
        set_font(p.font, 12, False, RGBColor(231, 113, 27))
        p = tf.add_paragraph()
        p.space_before = Pt(3)
        p.text = "This is a rich result enabled by Schema..."
        set_font(p.font, 14, False, RGBColor(77, 81, 86))
        label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
        p = label.text_frame.add_paragraph()
        p.text = txt["rich_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100))

    # Cover
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg = slide.shapes.add_shape(1, 0, 0, Inches(13.333), Inches(7.5))
    bg.fill.solid()
    bg.fill.fore_color.rgb = RGBColor(18, 52, 86)
    title = slide.shapes.add_textbox(Inches(1), Inches(2.5), Inches(11), Inches(2))
    p = title.text_frame.add_paragraph()
    p.text = txt["ppt_cover_title"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 54, True, RGBColor(255, 255, 255))
    sub = slide.shapes.add_textbox(Inches(1), Inches(4), Inches(11), Inches(1))
    p = sub.text_frame.add_paragraph()
    p.text = txt["ppt_cover_sub"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 24, False, RGBColor(200, 200, 200))

    # Slides
    for issue in slides_data:
        slide = prs.slides.add_slide(prs.slide_layouts[6])
        
        # Header
        h_shape = slide.shapes.add_shape(1, 0, 0, Inches(13.333), Inches(1.2))
        h_shape.fill.solid()
        h_shape.fill.fore_color.rgb = RGBColor(240, 242, 246)
        h_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(10), Inches(0.8))
        p = h_box.text_frame.add_paragraph()
        p.text = issue['title']
        set_font(p.font, 32, True, RGBColor(50, 50, 50))
        sev_color = RGBColor(220, 53, 69) if issue['severity'] == "Critical" else RGBColor(253, 126, 20)
        sev_box = slide.shapes.add_textbox(Inches(11), Inches(0.35), Inches(2), Inches(0.5))
        p = sev_box.text_frame.add_paragraph()
        p.text = f"{issue['severity']}"
        p.alignment = PP_ALIGN.CENTER
        set_font(p.font, 18, True, sev_color)
        
        # Category
        cat_key = f"cat_{issue['category']}"
        cat_label = txt.get(cat_key, issue['category'].upper())
        cat_box = slide.shapes.add_textbox(Inches(0.5), Inches(1.3), Inches(4), Inches(0.4))
        p = cat_box.text_frame.add_paragraph()
        p.text = cat_label
        set_font(p.font, 14, True, RGBColor(0, 102, 204))

        # Desc
        d_title = slide.shapes.add_textbox(Inches(0.5), Inches(1.8), Inches(6), Inches(0.5))
        p = d_title.text_frame.add_paragraph()
        p.text = txt["ppt_desc"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        d_box = slide.shapes.add_textbox(Inches(0.5), Inches(2.3), Inches(6), Inches(1.2))
        tf = d_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = issue['desc']
        set_font(p.font, 14, False, RGBColor(80, 80, 80))
        
        # Impact
        i_title = slide.shapes.add_textbox(Inches(0.5), Inches(3.6), Inches(6), Inches(0.5))
        p = i_title.text_frame.add_paragraph()
        p.text = txt["ppt_business_impact"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        i_box = slide.shapes.add_textbox(Inches(0.5), Inches(4.1), Inches(6), Inches(1.2))
        tf = i_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = issue['impact']
        set_font(p.font, 14, False, RGBColor(220, 53, 69))

        # Right Column Split
        ex_title = slide.shapes.add_textbox(Inches(7), Inches(1.5), Inches(5.8), Inches(0.5))
        p = ex_title.text_frame.add_paragraph()
        p.text = txt["ppt_slide_ex_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        ex_box = slide.shapes.add_textbox(Inches(7), Inches(2.0), Inches(5.8), Inches(1.5)) 
        tf = ex_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        
        for idx, url in enumerate(issue['examples'][:4]): 
            p = tf.add_paragraph()
            # Visualize Duplicate Logic
            if "duplicate" in issue['id']:
                 parts = url.split("Duplicate of:")
                 if len(parts) > 1:
                     p.text = f"{parts[0].strip()}\n   â†³ Original: {parts[1].strip()}"
                 else:
                     p.text = f"â€¢ {url}"
            else:
                 p.text = f"â€¢ {url}"
            
            set_font(p.font, 11, False, RGBColor(0, 102, 204))
            p.space_after = Pt(6)

        # Visualization
        is_serp = any(k in issue['title'] for k in ["Title", "æ ‡é¢˜", "Meta", "å…ƒæè¿°"])
        is_rich = any(k in issue['title'] for k in ["Schema", "Structured", "ç»“æ„åŒ–"])
        
        ev = issue.get('example_evidence', '')
        ex_url = issue['examples'][0] if issue['examples'] else "example.com"
        if "Duplicate" in issue['title'] and "Duplicate of:" in ex_url: 
             ex_url = ex_url.split("Duplicate of:")[0].strip()

        if is_serp:
            draw_serp_preview(slide, issue['title'], ev, ex_url)
        elif is_rich:
            draw_rich_snippet_preview(slide, ex_url)

        # Suggestion
        s_bg = slide.shapes.add_shape(1, Inches(0.5), Inches(5.8), Inches(12.333), Inches(1.5))
        s_bg.fill.solid()
        s_bg.fill.fore_color.rgb = RGBColor(230, 244, 234)
        s_bg.line.color.rgb = RGBColor(40, 167, 69)
        s_box = slide.shapes.add_textbox(Inches(0.7), Inches(5.9), Inches(11.9), Inches(1.3))
        tf = s_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = txt["ppt_slide_sugg_title"]
        set_font(p.font, 16, True, RGBColor(21, 87, 36))
        p = tf.add_paragraph()
        p.text = issue['suggestion']
        set_font(p.font, 14, False, RGBColor(21, 87, 36))
        p.space_before = Pt(5)

    out = BytesIO()
    prs.save(out)
    out.seek(0)
    return out

# --- 4. Init Session ---
if 'audit_data' not in st.session_state: st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state: st.session_state['audit_issues'] = []
if 'language' not in st.session_state: st.session_state['language'] = "zh" 
if 'sitemap_hreflang_found' not in st.session_state: st.session_state['sitemap_hreflang_found'] = False
if 'cwv_data' not in st.session_state: st.session_state['cwv_data'] = None

# --- 5. Sidebar ---
lang = st.session_state['language']
ui = TRANSLATIONS[lang]

with st.sidebar:
    st.title(ui["sidebar_title"])
    st.caption(ui["sidebar_caption"])
    
    st.divider()
    selected_lang = st.radio(ui["lang_label"], ["ä¸­æ–‡", "English"], index=0 if lang=="zh" else 1)
    new_lang = "zh" if selected_lang == "ä¸­æ–‡" else "en"
    if new_lang != lang:
        st.session_state['language'] = new_lang
        st.rerun()
    
    st.divider()
    menu_options = ui["nav_options"]
    menu_map = {ui["nav_options"][i]: ["input", "dashboard", "matrix", "ppt"][i] for i in range(4)}
    selected_menu = st.radio(ui["nav_label"], menu_options)
    menu_key = menu_map[selected_menu]
    
    st.divider()
    if st.session_state['audit_data'] is not None:
        st.success(ui["cache_info"].format(len(st.session_state['audit_data'])))
        st.markdown(f"**{ui['sitemap_status_title']}**")
        if st.session_state['sitemap_hreflang_found']: st.caption(ui["sitemap_found_href"])
        else: st.caption(ui["sitemap_no_href"])
        if st.button(ui["clear_data"]):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.session_state['sitemap_hreflang_found'] = False
            st.session_state['cwv_data'] = None
            st.rerun()

# --- 6. Main Logic ---
if menu_key == "input":
    st.header(ui["input_header"])
    st.info(ui["input_info"])
    
    col1, col2 = st.columns([3, 1])
    with col1:
        url_input = st.text_input(ui["input_label"], placeholder=ui["input_placeholder"])
    with col2:
        max_pages = st.number_input(ui.get("max_pages_label", "Max Pages"), min_value=1, max_value=1000, value=100)
    
    with st.expander(ui.get("adv_settings", "Advanced")):
        manual_robots = st.text_input(ui.get("manual_robots", "Manual Robots.txt"), placeholder="https://example.com/robots.txt")
        manual_sitemaps_text = st.text_area(ui.get("manual_sitemaps", "Manual Sitemaps"), placeholder="https://example.com/sitemap.xml")
        manual_sitemaps = [s.strip() for s in manual_sitemaps_text.split('\n') if s.strip()]
    
    with st.expander(ui.get("psi_settings", "Google PSI")):
        psi_key = st.text_input(ui.get("psi_api_key_label", "API Key"), type="password", help=ui.get("psi_api_help", ""))
        st.caption(ui["psi_get_key"])

    start_btn = st.button(ui["start_btn"], type="primary", use_container_width=True)
    
    if start_btn and url_input:
        if not is_valid_url(url_input):
            st.error(ui["error_url"])
        else:
            with st.spinner(ui["spinner_crawl"].format(max_pages)):
                data, issues, error_msg = crawl_website(url_input, max_pages, lang, manual_robots, manual_sitemaps, psi_key)
                if not data:
                    st.error(ui["error_no_data"].format(error_msg or "Unknown Error"))
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(ui["success_audit"].format(len(data)))
                    st.balloons()

elif menu_key == "dashboard":
    st.header(ui["dashboard_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        if st.session_state.get('cwv_data'):
            cwv = st.session_state['cwv_data']
            st.subheader(ui["cwv_title"])
            st.caption(ui["cwv_source"])
            c1, c2, c3, c4 = st.columns(4)
            def metric_color(val, good, poor):
                if val <= good: return "normal"
                if val >= poor: return "inverse"
                return "off"
            c1.metric("LCP (Loading)", f"{cwv.get('LCP',0):.2f}s", delta_color=metric_color(cwv.get('LCP',0), 2.5, 4.0))
            c2.metric("CLS (Visual)", f"{cwv.get('CLS',0):.3f}", delta_color=metric_color(cwv.get('CLS',0), 0.1, 0.25))
            c3.metric("INP (Interact)", f"{cwv.get('INP',0)}ms", delta_color=metric_color(cwv.get('INP',0), 200, 500))
            c4.metric("FCP", f"{cwv.get('FCP',0):.2f}s")
            st.divider()

        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        total = len(issues)
        score = max(0, 100 - int(total * 0.5))
        critical = len([i for i in issues if i['severity'] == 'Critical'])
        
        k1, k2, k3, k4 = st.columns(4)
        k1.metric(ui["kpi_health"], f"{score}/100")
        k2.metric(ui["kpi_pages"], str(len(df)))
        k3.metric(ui["kpi_issues"], str(total), delta_color="inverse")
        k4.metric(ui["kpi_critical"], str(critical), delta_color="inverse")
        
        st.divider()
        c1, c2 = st.columns(2)
        with c1:
            st.subheader(ui["chart_issues"])
            if issues: st.bar_chart(pd.DataFrame(issues)['title'].value_counts())
            else: st.info(ui["chart_no_issues"])
        with c2:
            st.subheader(ui["chart_status"])
            if not df.empty: st.bar_chart(df['Status'].value_counts())

elif menu_key == "matrix":
    st.header(ui["matrix_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(df, use_container_width=True, hide_index=True)
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(ui["download_csv"], csv, "audit_report.csv", "text/csv")

elif menu_key == "ppt":
    st.header(ui["ppt_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    elif not st.session_state['audit_issues']:
        st.success(ui["ppt_success_no_issues"])
    else:
        raw_issues = st.session_state['audit_issues']
        grouped = {}
        for i in raw_issues:
            t = i['title']
            if t not in grouped:
                grouped[t] = {
                    "id": i.get("id", "unknown"),
                    "title": t, "severity": i['severity'], "desc": i['desc'], 
                    "suggestion": i['suggestion'], "count": 0, "examples": [],
                    "example_evidence": i.get("evidence", ""),
                    "category": i.get("category", "content"),
                    "impact": i.get("impact", "")
                }
            grouped[t]['count'] += 1
            if len(grouped[t]['examples']) < 5: 
                # Fixed: Only try to split if "Duplicate of:" exists
                if "meta" in i and "Duplicate of:" in i['meta']: 
                     grouped[t]['examples'].append(f"{i['url']} Duplicate of: {i['meta'].split('Duplicate of: ')[1]}")
                else: 
                     grouped[t]['examples'].append(i['url'])
        
        # Sort by Category -> Severity -> Count
        sov_order = SEVERITY_ORDER
        cat_order = {k: v for v, k in enumerate(CATEGORY_ORDER)}
        
        slides = sorted(list(grouped.values()), key=lambda x: (
            cat_order.get(x['category'], 99), 
            get_issue_priority(x['id']), # NEW: Precise ordering
            sov_order.get(x['severity'], 3), 
            -x['count']
        ))

        st.write(f"### {ui['ppt_download_header']}")
        st.info(ui["ppt_info"])
        if st.button(ui["ppt_btn"]):
            with st.spinner("Generating..."):
                pptx = create_styled_pptx(slides, lang=lang)
                fname = f"seo_audit_report_{lang}.pptx"
                st.download_button(ui["ppt_btn"], pptx, fname, "application/vnd.openxmlformats-officedocument.presentationml.presentation")

        st.divider()
        st.subheader(ui["ppt_preview_header"])
        
        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(slides): st.session_state.slide_index = 0
        
        s = slides[st.session_state.slide_index]
        with st.container(border=True):
            # Display Category Label
            cat_key = f"cat_{s['category']}"
            cat_label = ui.get(cat_key, s['category'].upper())
            st.caption(f"ğŸ“‚ {cat_label}")
            
            st.markdown(f"### {ui['ppt_slide_title']} {s['title']}")
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if s['severity'] == "Critical" else "orange" if s['severity'] == "High" else "blue"
                st.markdown(f"**{ui['ppt_severity']}** :{color}[{s['severity']}]")
                st.markdown(f"**{ui['ppt_impact']}** {ui['ppt_impact_desc'].format(s['count'])}")
                
                st.markdown(f"**{ui['ppt_desc']}**")
                st.write(s['desc'])
                
                st.markdown(f"**{ui['ppt_business_impact']}**") 
                st.error(s['impact']) 
                
                st.info(f"{ui['ppt_sugg']} {s['suggestion']}")
            with c2:
                is_serp = any(k in s['title'] for k in ["Title", "æ ‡é¢˜", "Meta", "å…ƒæè¿°"])
                is_rich = any(k in s['title'] for k in ["Schema", "Structured", "ç»“æ„åŒ–"])
                
                if is_serp:
                    st.markdown(f"**{ui.get('serp_sim_title', 'SERP Preview')}**")
                    ev = s.get('example_evidence', '')
                    ex_url = s['examples'][0] if s['examples'] else "example.com"
                    if "Duplicate" in ex_url: ex_url = ex_url.split("Duplicate of:")[0].strip()
                    
                    display_title = ev if ev else "Untitled Page"
                    if len(display_title) > 60: display_title = display_title[:55] + " ..."
                    st.markdown(f"""
                    <div style="font-family: Arial, sans-serif; border: 1px solid #dfe1e5; border-radius: 8px; padding: 15px; background: white; box-shadow: 0 1px 6px rgba(32,33,36,0.28);">
                        <div style="font-size: 14px; color: #202124;">{urlparse(ex_url).netloc} <span style="color: #5f6368">â€º ...</span></div>
                        <div style="font-size: 20px; color: #1a0dab; margin-top: 5px;">{display_title}</div>
                        <div style="font-size: 14px; color: #4d5156; margin-top: 3px;">
                            Please provide a meta description...
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                elif is_rich:
                     st.markdown(f"**{ui.get('rich_sim_title', 'Rich Result Preview')}**")
                     st.markdown("""
                     <div style="font-family: Arial, sans-serif; border: 1px solid #dfe1e5; border-radius: 8px; padding: 15px; background: white; box-shadow: 0 1px 6px rgba(32,33,36,0.28);">
                        <div style="font-size: 14px; color: #202124;">example.com <span style="color: #5f6368">â€º product</span></div>
                        <div style="font-size: 20px; color: #1a0dab; margin-top: 5px;">Best Product - High Quality</div>
                        <div style="color: #e7711b; font-size: 14px;">â˜…â˜…â˜…â˜…â˜… <span style="color:#70757a">Rating: 4.8 Â· $199.00 Â· In stock</span></div>
                        <div style="font-size: 14px; color: #4d5156; margin-top: 3px;">This is a rich result enabled by Schema...</div>
                     </div>
                     """, unsafe_allow_html=True)
                else:
                    st.markdown(f"**{ui['ppt_examples']}**")
                    for ex in s['examples']: 
                        # Web View Format
                        if "Duplicate of:" in ex:
                            parts = ex.split("Duplicate of:")
                            st.markdown(f"- `{parts[0].strip()}`  \n  â†³ **Original:** `{parts[1].strip()}`")
                        else:
                            st.markdown(f"- `{ex}`")

        cp, ct, cn = st.columns([1, 2, 1])
        with cp:
            if st.button(ui["ppt_prev"]):
                st.session_state.slide_index = max(0, st.session_state.slide_index - 1)
                st.rerun()
        with ct:
            st.markdown(f"<div style='text-align: center'>Slide {st.session_state.slide_index + 1} / {len(slides)}</div>", unsafe_allow_html=True)
        with cn:
            if st.button(ui["ppt_next"]):
                st.session_state.slide_index = min(len(slides) - 1, st.session_state.slide_index + 1)
                st.rerun()
