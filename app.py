import streamlit as st
import time
import pandas as pd
import requests
import hashlib
import re
import urllib3
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO

# --- Level 0: é¡µé¢åŸºç¡€é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª st å‘½ä»¤) ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
    from pptx.enum.shapes import MSO_SHAPE
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- Level 1: åŸºç¡€å·¥å…·å‡½æ•° ---
def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def estimate_pixel_width(text, font_size=18):
    """ä¼°ç®—æ–‡æœ¬åœ¨ Google SERP ä¸­çš„åƒç´ å®½åº¦"""
    if not text: return 0
    width = 0
    for char in text:
        if ord(char) > 127: 
            width += font_size
        elif char.isupper():
            width += font_size * 0.7 
        else:
            width += font_size * 0.55 
    return width

def get_browser_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Connection': 'keep-alive',
    }

def fetch_psi_data(url, api_key):
    if not api_key: return None
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available"}
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else: return {"error": f"API Error: {response.status_code}"}
    except Exception as e: return {"error": str(e)}

# --- Level 2: æ’åºä¸é…ç½®å¸¸é‡ ---
CATEGORY_ORDER = ["access", "indexability", "technical", "content", "image_ux", "cwv_performance"]
SEVERITY_ORDER = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}

ISSUE_PRIORITY_LIST = [
    "no_robots", "robots_bad_rule", "robots_quality_issue", "baidu_robots_missing", "robots_no_sitemap", "no_sitemap", "sitemap_invalid",
    "http_5xx", "http_4xx", "soft_404", "http_3xx",
    "duplicate", "missing_canonical", "hreflang_invalid", "hreflang_no_default", "missing_hreflang",
    "missing_viewport", "missing_jsonld", "js_links", "url_underscore", "url_uppercase",
    "missing_baidu_stats", "missing_baidu_verify", "missing_applicable_device", "missing_no_transform",
    "missing_title", "short_title", "long_title", "missing_desc", "short_desc", "missing_h1", "missing_keywords", 
    "no_favicon", "missing_alt", "alt_bad_quality", "anchor_bad_quality", 
    "lcp_issue", "inp_issue", "cls_issue", "fcp_issue", "cls_risk"
]

def get_issue_priority(issue_id):
    try: return ISSUE_PRIORITY_LIST.index(issue_id)
    except ValueError: return 999 

# --- Level 3: å›½é™…åŒ–å­—å…¸ ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "æ——èˆ°å®¡è®¡ç‰ˆ v12.4",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        "sitemap_status_title": "Sitemap çŠ¶æ€:",
        "sitemap_found_href": "âœ… å‘ç° Hreflang é…ç½®", 
        "sitemap_no_href": "âš ï¸ æœªå‘ç° Hreflang",     
        "sitemap_missing": "âŒ æœªæ‰¾åˆ° Sitemap",

        "psi_settings": "Google PSI API è®¾ç½® (æ¨è)",
        "psi_api_key_label": "è¾“å…¥ Google PageSpeed API Key",
        "psi_api_help": "å»ºè®®å¡«å…¥ä»¥è·å– LCP/CLS/INP çœŸå®æ•°æ®ã€‚ç•™ç©ºåˆ™åªè¿›è¡Œä»£ç å®¡è®¡ã€‚",
        "psi_list_url_label": "äº§å“åˆ—è¡¨é¡µ URL (å¯é€‰)",
        "psi_detail_url_label": "äº§å“è¯¦æƒ…é¡µ URL (å¯é€‰)",
        "psi_get_key": "æ²¡æœ‰ API Key? [ç‚¹å‡»è¿™é‡Œå…è´¹ç”³è¯·](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "æ­£åœ¨è°ƒç”¨ Google API è·å– {} æ•°æ®...",
        "psi_success": "æˆåŠŸè·å–çœŸå®ç”¨æˆ·æ•°æ®ï¼",
        "psi_error": "API è°ƒç”¨å¤±è´¥æˆ–æ—  CrUX æ•°æ®",
        
        "input_header": "å¼€å§‹æ·±åº¦å®¡è®¡",
        "input_info": "è¯´æ˜: v12.4 ä¿®å¤äº†åº”ç”¨å¯åŠ¨ç™½å±é—®é¢˜ï¼Œå¹¶åŒ…å«ç™¾åº¦ SEO å¢å¼ºåŠŸèƒ½ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€ (é¦–é¡µ)",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°",
        "adv_settings": "é«˜çº§è®¾ç½® (Advanced Settings)", 
        "check_robots_label": "æ£€æŸ¥å¹¶éµå¾ª Robots.txt è§„åˆ™", 
        "crawl_sitemap_label": "è‡ªåŠ¨æŠ“å– Robots.txt ä¸­çš„ Sitemap", 
        "baidu_mode_label": "å¯ç”¨ç™¾åº¦ SEO å®¡è®¡æ¨¡å¼", 
        "allow_subdomains_label": "å…è®¸æŠ“å–å­åŸŸå (å¦‚ blog.site.com)",
        "allow_outside_folder_label": "å…è®¸æŠ“å–çˆ¶çº§ç›®å½• (å¦‚ä» /en/ å¼€å§‹æŠ“å– /fr/)",
        "manual_sitemaps": "æ‰‹åŠ¨ Sitemap åœ°å€ (æ¯è¡Œä¸€ä¸ª, è¡¥å……ç”¨)", 
        "start_btn": "å¼€å§‹æ·±åº¦çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨æ‰§è¡Œæ·±åº¦å®¡è®¡ (Max {} pages)...", 
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚åŸå› : {}", 
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP çŠ¶æ€ç åˆ†å¸ƒ",
        "cwv_title": "é¦–é¡µæ ¸å¿ƒ Web æŒ‡æ ‡ (Core Web Vitals) - çœŸå®æ•°æ®",
        "cwv_source": "æ•°æ®æ¥æº: Google Chrome User Experience Report (CrUX) - ä»…é¦–é¡µ",
        
        "matrix_header": "çˆ¬å–æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼ŒåŒ…å«å¢å¼ºç‰ˆå¯è§†åŒ–é¢„è§ˆã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_category": "åˆ†ç±»:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "åœ¨å·²çˆ¬å–æ ·æœ¬ä¸­å‘ç° **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "ğŸ”´ é—®é¢˜æè¿°:",
        "ppt_business_impact": "ğŸ“‰ Business & SEO Impact:", 
        "ppt_sugg": "âœ… ä¿®å¤å»ºè®®:",
        "ppt_examples": "ğŸ” å—å½±å“é¡µé¢ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        
        "cat_access": "å¯è®¿é—®æ€§ä¸ç´¢å¼• (Access & Indexing)",
        "cat_indexability": "ç´¢å¼•è§„èŒƒæ€§ (Indexability)",
        "cat_technical": "æŠ€æœ¯ä¸æ¶æ„ (Technical SEO)",
        "cat_content": "é¡µé¢å†…å®¹ (On-Page Content)",
        "cat_image_ux": "ç”¨æˆ·ä½“éªŒä¸èµ„æº (UX & Assets)",
        "cat_cwv_performance": "æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ (Core Web Vitals)",

        "ppt_cover_title": "SEO æ·±åº¦æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro v12.4",
        "ppt_slide_desc_title": "æ·±åº¦åˆ†æ",
        "ppt_slide_count_title": "æ ·æœ¬ä¸­å—å½±å“é¡µé¢æ•°: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹", 
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
        "serp_sim_title": "Google æœç´¢ç»“æœæ¨¡æ‹Ÿ (SERP):",
        "rich_sim_title": "å¯Œåª’ä½“ç»“æœæ¨¡æ‹Ÿ (Rich Results):",
        "code_sim_title": "ä»£ç ç‰‡æ®µç¤ºä¾‹ (Code Snippet):",
        "visual_sim_title": "è§†è§‰ä½“éªŒæ¨¡æ‹Ÿ:",
        "cwv_sim_title": "CWV æ€§èƒ½ä»ªè¡¨ç›˜ (Performance):",

        # Issues
        "lcp_issue": "LCP (æœ€å¤§å†…å®¹ç»˜åˆ¶) è¶…æ ‡", "lcp_issue_desc": "LCP æ—¶é—´ä¸º {:.2f}s (ç›®æ ‡ <2.5s)ã€‚é¡µé¢ä¸»è¦å†…å®¹åŠ è½½è¿‡äºç¼“æ…¢ã€‚", "lcp_issue_impact": "LCP æ˜¯ Google æ ¸å¿ƒæ’åå› ç´ ã€‚åŠ è½½ç¼“æ…¢ä¼šå¯¼è‡´ç”¨æˆ·è·³å‡ºç‡é£™å‡ï¼Œå¹¶ç›´æ¥é™ä½åœ¨ç§»åŠ¨ç«¯çš„æœç´¢æ’åã€‚", "lcp_issue_sugg": "å‹ç¼©å›¾ç‰‡ä½“ç§¯ï¼ˆä½¿ç”¨ WebPï¼‰ï¼Œä½¿ç”¨ CDN åˆ†å‘å†…å®¹ï¼Œæ¨è¿Ÿéå…³é”® JS æ‰§è¡Œï¼Œå¹¶é¢„åŠ è½½ LCP å…³é”®å…ƒç´ ã€‚",
        "cls_issue": "CLS (ç´¯ç§¯å¸ƒå±€åç§») è¶…æ ‡", "cls_issue_desc": "é¡µé¢åŠ è½½è¿‡ç¨‹ä¸­å…ƒç´ å‘ç”Ÿæ„å¤–ä½ç§» (Score > 0.1)ã€‚", "cls_issue_impact": "ä½œä¸ºæ ¸å¿ƒæ’åå› ç´ ï¼Œå¸ƒå±€ä¸ç¨³å®šä¼šå¯¼è‡´ç”¨æˆ·è¯¯è§¦å¹¿å‘Šæˆ–æŒ‰é’®ï¼Œä¸¥é‡æŸå®³å“ç‰Œä¿¡èª‰å’Œç”¨æˆ·ä½“éªŒã€‚", "cls_issue_sugg": "ä¸ºæ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘å…ƒç´ æŒ‡å®šæ˜ç¡®çš„å®½åº¦å’Œé«˜åº¦å±æ€§ï¼Œé¿å…åœ¨é¡¶éƒ¨åŠ¨æ€æ’å…¥å†…å®¹ã€‚",
        "inp_issue": "INP (äº¤äº’åˆ°ç»˜åˆ¶å»¶è¿Ÿ) è¶…æ ‡", "inp_issue_desc": "ç”¨æˆ·ç‚¹å‡»æŒ‰é’®åï¼Œé¡µé¢å“åº”å»¶è¿Ÿè¶…è¿‡ 200msã€‚", "inp_issue_impact": "Google æ–°å¼•å…¥çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚é«˜å»¶è¿Ÿä¼šè®©ç”¨æˆ·è§‰å¾—ç½‘ç«™â€œå¡é¡¿â€æˆ–æ— å“åº”ï¼Œä¸¥é‡å½±å“è½¬åŒ–ç‡ã€‚", "inp_issue_sugg": "å‡å°‘ä¸»çº¿ç¨‹é˜»å¡ï¼Œå°†é•¿ä»»åŠ¡ (Long Tasks) æ‹†åˆ†ä¸ºå°ä»»åŠ¡ï¼Œå¹¶ä¼˜åŒ–å¤æ‚çš„ JavaScript äº‹ä»¶å¤„ç†é€»è¾‘ã€‚",
        "fcp_issue": "FCP (é¦–æ¬¡å†…å®¹ç»˜åˆ¶) ç¼“æ…¢", "fcp_issue_desc": "FCP æ—¶é—´ä¸º {:.2f}s (ç›®æ ‡ <1.8s)ã€‚ç”¨æˆ·çœ‹åˆ°é¡µé¢ç¬¬ä¸€ä¸ªå†…å®¹çš„æ—¶é—´è¿‡é•¿ã€‚", "fcp_issue_impact": "FCP æ…¢ä¼šè®©ç”¨æˆ·æ„Ÿè§‰æœåŠ¡å™¨å“åº”è¿Ÿé’ï¼Œç›´æ¥å¢åŠ è·³å‡ºç‡ã€‚", "fcp_issue_sugg": "ä¼˜åŒ–æœåŠ¡å™¨å“åº”æ—¶é—´ (TTFB)ï¼Œæ¶ˆé™¤é˜»å¡æ¸²æŸ“çš„ CSS/JS èµ„æºã€‚",
        
        "no_robots": "ç¼ºå¤± Robots.txt", "no_robots_desc": "æ— æ³•è®¿é—®æ ¹ç›®å½•çš„ robots.txt æ–‡ä»¶ï¼Œæˆ–è€…æœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç ã€‚", "no_robots_impact": "çˆ¬è™«å¯èƒ½æŠ“å–æ— ç”¨çš„åå°é¡µé¢ï¼Œä¸ä»…æ¶ˆè€—æœåŠ¡å™¨èµ„æºï¼Œè¿˜ä¼šæµªè´¹å®è´µçš„çˆ¬å–é¢„ç®—ã€‚", "no_robots_sugg": "åœ¨ç½‘ç«™æ ¹ç›®å½•åˆ›å»ºæ ‡å‡†çš„ robots.txt æ–‡ä»¶ï¼Œå¹¶ç¡®ä¿å…¶å¯¹æœç´¢å¼•æ“çˆ¬è™«å…¬å¼€å¯è§ã€‚",
        "robots_bad_rule": "Robots.txt å°ç¦é£é™©", "robots_bad_rule_desc": "æ£€æµ‹åˆ°å…¨ç«™å°ç¦è§„åˆ™ (Disallow: /)ï¼Œä¸”æœªå‘ç°é’ˆå¯¹ Googlebot çš„ä¾‹å¤–è§„åˆ™ã€‚", "robots_bad_rule_impact": "è¿™å°†ç›´æ¥å¯¼è‡´æœç´¢å¼•æ“åœæ­¢æŠ“å–å¹¶ç´¢å¼•æ‚¨çš„ç½‘ç«™ï¼Œæ‰€æœ‰è‡ªç„¶æœç´¢æµé‡å°†å½’é›¶ã€‚", "robots_bad_rule_sugg": "ç«‹å³ç§»é™¤ 'Disallow: /' è§„åˆ™ï¼Œæˆ–è€…ä¸ºæœç´¢å¼•æ“çˆ¬è™«æ·»åŠ å…·ä½“çš„ 'Allow' è§„åˆ™ã€‚",
        "robots_quality_issue": "Robots.txt è§„åˆ™é…ç½®ä¸å½“", "robots_quality_issue_desc": "Robots.txt æ–‡ä»¶å­˜åœ¨æ½œåœ¨é—®é¢˜ï¼š{}ã€‚", "robots_quality_issue_impact": "å¯èƒ½å¯¼è‡´Googlebotè¡Œä¸ºå¼‚å¸¸ï¼ˆå¦‚è¯¯åˆ¤å±è”½æˆ–æ¸²æŸ“å¤±è´¥ï¼‰ã€‚", "robots_quality_issue_sugg": "æ£€æŸ¥ Robots.txtï¼Œç§»é™¤åºŸå¼ƒæŒ‡ä»¤ï¼ˆå¦‚ Noindexï¼‰ï¼Œå¹¶ç¡®ä¿å…è®¸è®¿é—® CSS/JS èµ„æºã€‚",
        "robots_no_sitemap": "Robots æœªå£°æ˜ Sitemap", "robots_no_sitemap_desc": "robots.txt æ–‡ä»¶ä¸­æœªæŒ‡æ˜ Sitemap XML æ–‡ä»¶çš„ä½ç½®ã€‚", "robots_no_sitemap_impact": "ä¼šé™ä½æœç´¢å¼•æ“å‘ç°æ–°é¡µé¢å’Œæ›´æ–°æ—§å†…å®¹çš„é€Ÿåº¦ï¼Œå°¤å…¶å¯¹äºå¤§å‹ç½‘ç«™å½±å“æ›´æ˜æ˜¾ã€‚", "robots_no_sitemap_sugg": "åœ¨ robots.txt æ–‡ä»¶åº•éƒ¨æ·»åŠ ä¸€è¡Œï¼šSitemap: https://yourdomain.com/sitemap.xml",
        "no_sitemap": "Sitemap è®¿é—®å¤±è´¥", "no_sitemap_desc": "æ— æ³•è®¿é—® Sitemap æ–‡ä»¶ï¼ŒæœåŠ¡å™¨è¿”å› 4xx æˆ– 5xx é”™è¯¯ã€‚", "no_sitemap_impact": "æœç´¢å¼•æ“éš¾ä»¥å‘ç°æ·±å±‚é“¾æ¥æˆ–å­¤å²›é¡µé¢ï¼Œå¯¼è‡´æ•´ä½“æ”¶å½•ç‡ä¸‹é™ã€‚", "no_sitemap_sugg": "æ£€æŸ¥ Sitemap æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä»¥åŠæœåŠ¡å™¨æƒé™è®¾ç½®æ˜¯å¦å…è®¸å¤–éƒ¨è®¿é—®ã€‚",
        "sitemap_invalid": "Sitemap æ ¼å¼é”™è¯¯", "sitemap_invalid_desc": "XML è§£æå¤±è´¥ï¼Œæ–‡ä»¶æ ¼å¼ä¸ç¬¦åˆæ ‡å‡†åè®®ã€‚", "sitemap_invalid_impact": "æœç´¢å¼•æ“æ— æ³•è¯»å–å…¶ä¸­çš„é“¾æ¥ï¼Œå¯¼è‡´ Sitemap å®Œå…¨å¤±æ•ˆã€‚", "sitemap_invalid_sugg": "ä½¿ç”¨ XML éªŒè¯å·¥å…·æ£€æŸ¥æ–‡ä»¶è¯­æ³•ï¼Œç¡®ä¿æ²¡æœ‰æœªé—­åˆçš„æ ‡ç­¾æˆ–éæ³•å­—ç¬¦ã€‚",
        "no_favicon": "ç¼ºå¤± Favicon", "no_favicon_desc": "æœªåœ¨é¦–é¡µæ£€æµ‹åˆ° Favicon å›¾æ ‡ã€‚", "no_favicon_impact": "é™ä½å“ç‰Œåœ¨æµè§ˆå™¨æ ‡ç­¾é¡µå’Œæœç´¢ç»“æœé¡µ (SERP) ä¸­çš„è¾¨è¯†åº¦ï¼Œè¿›è€Œå¯¼è‡´ç‚¹å‡»ç‡ (CTR) ä¸‹é™ã€‚", "no_favicon_sugg": "åˆ¶ä½œä¸€ä¸ª .ico æˆ– .png æ ¼å¼çš„å›¾æ ‡ï¼Œå¹¶åœ¨ <head> ä¸­é€šè¿‡ <link rel='icon'> å¼•ç”¨ã€‚",
        "duplicate": "å‘ç°æœªè§„èŒƒåŒ–çš„é‡å¤å†…å®¹", "duplicate_desc": "æ£€æµ‹åˆ°é«˜åº¦ç›¸ä¼¼çš„å†…å®¹é¡µé¢ï¼Œä¸”æœªæ­£ç¡®é…ç½® Canonical æ ‡ç­¾ã€‚", "duplicate_impact": "å¯¼è‡´å…³é”®è¯å†…éƒ¨ç«äº‰ (Cannibalization)ï¼Œåˆ†æ•£é¡µé¢æƒé‡ï¼Œä½¿æ‰€æœ‰ç›¸å…³é¡µé¢éƒ½éš¾ä»¥è·å¾—é«˜æ’åã€‚", "duplicate_sugg": "ä¿ç•™ä¸€ä¸ªé¦–é€‰ URLï¼Œå¹¶åœ¨å…¶ä»–å‰¯æœ¬é¡µé¢ä¸Šæ·»åŠ  rel='canonical' æŒ‡å‘è¯¥é¦–é€‰ URLã€‚",
        "http_3xx": "å†…éƒ¨é“¾æ¥é‡å®šå‘ (3xx)", "http_3xx_desc": "å†…éƒ¨é“¾æ¥å‘ç”Ÿè·³è½¬ (é“¾æ¡: {})ã€‚", "http_3xx_impact": "æµªè´¹çˆ¬è™«é¢„ç®—ï¼Œå¢åŠ é¡µé¢åŠ è½½å»¶è¿Ÿï¼Œä¸”æ¯æ¬¡è·³è½¬éƒ½ä¼šæŸè€—å°‘é‡é“¾æ¥ä¼ é€’çš„æƒé‡ (Link Equity)ã€‚", "http_3xx_sugg": "æ‰¹é‡æ›´æ–°å†…éƒ¨é“¾æ¥ï¼Œä½¿å…¶ç›´æ¥æŒ‡å‘æœ€ç»ˆçš„ç›®æ ‡ URLï¼Œé¿å…ä¸­é—´è·³è½¬ã€‚",
        "http_4xx": "æ­»é“¾/å®¢æˆ·ç«¯é”™è¯¯ (4xx)", "http_4xx_desc": "å†…éƒ¨é“¾æ¥è¿”å› 404 (æœªæ‰¾åˆ°) æˆ– 403 (ç¦æ­¢è®¿é—®) é”™è¯¯ã€‚", "http_4xx_impact": "ä¸¥é‡ç ´åç”¨æˆ·ä½“éªŒï¼Œä¸­æ–­æƒé‡ä¼ é€’è·¯å¾„ï¼Œå¹¶å¯èƒ½å¯¼è‡´å·²ç´¢å¼•çš„é¡µé¢è¢« Google ç§»é™¤ã€‚", "http_4xx_sugg": "ç§»é™¤æ­»é“¾ï¼Œæˆ–è€…å°†å…¶é‡å®šå‘åˆ°æœ€ç›¸å…³çš„æœ‰æ•ˆé¡µé¢ã€‚",
        "http_5xx": "æœåŠ¡å™¨é”™è¯¯ (5xx)", "http_5xx_desc": "æœåŠ¡å™¨å“åº” 500/502/503 ç­‰å†…éƒ¨é”™è¯¯ã€‚", "http_5xx_impact": "è¡¨æ˜æœåŠ¡å™¨æå…¶ä¸ç¨³å®šï¼ŒGooglebot ä¼šå› æ­¤é™ä½å¯¹è¯¥ç«™ç‚¹çš„çˆ¬å–é¢‘ç‡ä»¥å‡è½»è´Ÿè½½ã€‚", "http_5xx_sugg": "æ£€æŸ¥æœåŠ¡å™¨é”™è¯¯æ—¥å¿—ï¼Œä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æˆ–å‡çº§æœåŠ¡å™¨é…ç½®ã€‚",
        "hreflang_invalid": "Hreflang æ ¼å¼é”™è¯¯", "hreflang_invalid_desc": "è¯­è¨€ä»£ç ä¸ç¬¦åˆ ISO 639-1 æ ‡å‡† (å¦‚ä½¿ç”¨äº† {} ç­‰é”™è¯¯æ ¼å¼)ã€‚", "hreflang_invalid_impact": "Google æ— æ³•è¯†åˆ«ç›®æ ‡è¯­è¨€ï¼Œå¯¼è‡´å›½é™…åŒ–å®šä½å¤±æ•ˆã€‚", "hreflang_invalid_sugg": "ä½¿ç”¨æ ‡å‡†çš„ ISO è¯­è¨€ä»£ç  (ä¾‹å¦‚ 'en-US' è€Œä¸æ˜¯ 'en_US')ã€‚",
        "hreflang_no_default": "Hreflang ç¼ºå¤± x-default", "hreflang_no_default_desc": "æœªé…ç½® 'x-default' å›é€€ç‰ˆæœ¬ã€‚", "hreflang_no_default_impact": "å½“ç”¨æˆ·æ¥è‡ªæœªæŒ‡å®šçš„è¯­è¨€/åœ°åŒºæ—¶ï¼Œå¯èƒ½æ— æ³•è‡ªåŠ¨åŒ¹é…åˆ°æœ€åˆé€‚çš„é€šç”¨ç‰ˆæœ¬ï¼ˆé€šå¸¸æ˜¯è‹±è¯­ï¼‰ã€‚", "hreflang_no_default_sugg": "æ·»åŠ  hreflang='x-default' æ ‡ç­¾ï¼ŒæŒ‡å®šé»˜è®¤çš„è¯­è¨€ç‰ˆæœ¬ã€‚",
        "alt_bad_quality": "å›¾ç‰‡ Alt è´¨é‡å·®", "alt_bad_quality_desc": "Alt æ–‡æœ¬ä½¿ç”¨äº†æ— æ„ä¹‰è¯æ±‡ï¼ˆå¦‚ image1.jpg, photoï¼‰æˆ–è¿‡çŸ­ã€‚", "alt_bad_quality_impact": "æœç´¢å¼•æ“æ— æ³•ç†è§£å›¾ç‰‡å†…å®¹ï¼Œé”™å¤±å›¾ç‰‡æœç´¢æµé‡ï¼Œä¸”å¯¹è§†éšœç”¨æˆ·æä¸å‹å¥½ã€‚", "alt_bad_quality_sugg": "ä½¿ç”¨æè¿°æ€§æ–‡æœ¬å‡†ç¡®æè¿°å›¾ç‰‡å†…å®¹ï¼ŒåŒ…å«ç›¸å…³çš„å…³é”®è¯ã€‚",
        "anchor_bad_quality": "é”šæ–‡æœ¬è´¨é‡å·®", "anchor_bad_quality_desc": "ä½¿ç”¨äº†â€œç‚¹å‡»è¿™é‡Œâ€ã€â€œæ›´å¤šâ€ç­‰é€šç”¨è¯æ±‡ä½œä¸ºé“¾æ¥æ–‡æœ¬ã€‚", "anchor_bad_quality_impact": "æ— æ³•å‘æœç´¢å¼•æ“ä¼ é€’ç›®æ ‡é¡µé¢çš„å…³é”®è¯ç›¸å…³æ€§ï¼Œé™ä½äº†ç›®æ ‡é¡µé¢çš„æ’åæ½œåŠ›ã€‚", "anchor_bad_quality_sugg": "ä½¿ç”¨æè¿°æ€§ keywords in the anchor text.",
        "cls_risk": "CLS å¸ƒå±€åç§»é£é™© (é™æ€æ£€æµ‹)", "cls_risk_desc": "æ£€æµ‹åˆ° <img> æ ‡ç­¾ç¼ºå¤± width æˆ– height å±æ€§ã€‚", "cls_risk_impact": "å›¾ç‰‡åŠ è½½æ—¶ä¼šæ’‘å¼€é¡µé¢ï¼Œå¯¼è‡´å¸ƒå±€å‘ç”Ÿæ„å¤–æŠ–åŠ¨ï¼Œç›´æ¥æ¶åŒ– CLS æŒ‡æ ‡ã€‚", "cls_risk_sugg": "åœ¨ HTML ä¸­æ˜¾å¼æŒ‡å®šå›¾ç‰‡å’Œè§†é¢‘çš„å®½åº¦å’Œé«˜åº¦å±æ€§ã€‚",
        "missing_title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title)", "missing_title_desc": "é¡µé¢ä»£ç ä¸­æœªæ‰¾åˆ° <title> æ ‡ç­¾ã€‚", "missing_title_impact": "Title æ˜¯æœ€é‡è¦çš„ SEO æ ‡ç­¾ã€‚ç¼ºå¤±å°†å¯¼è‡´æœç´¢å¼•æ“æ— æ³•åˆ¤æ–­é¡µé¢ä¸»é¢˜ï¼Œå…³é”®è¯æ’åæå·®ã€‚", "missing_title_sugg": "ä¸ºæ¯ä¸ªé¡µé¢æ·»åŠ ç‹¬ç‰¹ã€åŒ…å«æ ¸å¿ƒå…³é”®è¯çš„æ ‡é¢˜ã€‚",
        "short_title": "æ ‡é¢˜è¿‡çŸ­", "short_title_desc": "æ ‡é¢˜é•¿åº¦ä¸è¶³ (çº¦ {} px)ï¼Œéš¾ä»¥å®Œæ•´è¡¨è¾¾é¡µé¢æ„å›¾ã€‚", "short_title_impact": "æµªè´¹äº†å®è´µçš„æ ‡é¢˜ç©ºé—´ï¼Œé”™å¤±äº†è¦†ç›–é•¿å°¾å…³é”®è¯æ’åçš„æœºä¼šã€‚", "short_title_sugg": "ä¸°å¯Œæ ‡é¢˜å†…å®¹ï¼ŒåŠ å…¥å“ç‰Œè¯æˆ–ä¿®é¥°è¯ï¼Œå»ºè®®é•¿åº¦åœ¨ 285-575 px ä¹‹é—´ã€‚",
        "long_title": "æ ‡é¢˜è¿‡é•¿", "long_title_desc": "æ ‡é¢˜è¶…è¿‡å»ºè®®æ˜¾ç¤ºå®½åº¦ (çº¦ {} px)ã€‚", "long_title_impact": "æ ‡é¢˜å°†åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­ï¼Œé™ä½å¯è¯»æ€§å’Œç‚¹å‡»ç‡ã€‚", "long_title_sugg": "ç²¾ç®€æ ‡é¢˜é•¿åº¦ï¼Œå°†æ ¸å¿ƒä¿¡æ¯å‰ç½®ï¼Œæ§åˆ¶åœ¨ 600 px ä»¥å†…ã€‚",
        "missing_desc": "ç¼ºå¤±å…ƒæè¿°", "missing_desc_desc": "é¡µé¢æœªåŒ…å« <meta name='description'> æ ‡ç­¾ã€‚", "missing_desc_impact": "è™½ç„¶ä¸ç›´æ¥å½±å“æ’åï¼Œä½† Google ä¼šéšæœºæŠ“å–æ­£æ–‡ä½œä¸ºæ‘˜è¦ï¼Œé€šå¸¸ä¸å¯æ§ä¸”ç‚¹å‡»ç‡ä½ã€‚", "missing_desc_sugg": "æ·»åŠ å¸å¼•äººçš„å…ƒæè¿°ï¼Œæ¦‚æ‹¬é¡µé¢å†…å®¹å¹¶åŒ…å«å·å¬æ€§ç”¨è¯­ã€‚",
        "short_desc": "å…ƒæè¿°è¿‡çŸ­", "short_desc_desc": "å†…å®¹è¿‡å°‘ (çº¦ {} px)ï¼Œå¸å¼•åŠ›ä¸è¶³ã€‚", "short_desc_impact": "æ— æ³•å……åˆ†å±•ç¤ºé¡µé¢å–ç‚¹ï¼Œåœ¨æœç´¢ç»“æœä¸­ç¼ºä¹ç«äº‰åŠ›ã€‚", "short_desc_sugg": "æ‰©å……æè¿°è‡³ 400-920 pxï¼Œæä¾›æ›´å¤šæœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚",
        "missing_h1": "ç¼ºå¤± H1 æ ‡ç­¾", "missing_h1_desc": "é¡µé¢ç¼ºä¹ <h1> ä¸»æ ‡é¢˜ã€‚", "missing_h1_impact": "æœç´¢å¼•æ“éš¾ä»¥ç†è§£å†…å®¹çš„å±‚çº§ç»“æ„å’Œæ ¸å¿ƒä¸»é¢˜ï¼Œé™ä½äº†å…³é”®è¯çš„ç›¸å…³æ€§æƒé‡ã€‚", "missing_h1_sugg": "ç¡®ä¿æ¯ä¸ªé¡µé¢æœ‰ä¸”ä»…æœ‰ä¸€ä¸ª H1 æ ‡ç­¾ï¼Œæ¦‚æ‹¬å½“å‰é¡µé¢çš„ä¸»é¢˜ã€‚",
        "missing_viewport": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®", "missing_viewport_desc": "æœªé…ç½® <meta name='viewport'> æ ‡ç­¾ã€‚", "missing_viewport_impact": "åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ˜¾ç¤ºå¼‚å¸¸ï¼ˆå­—ä½“æå°ï¼‰ã€‚Google ç§»åŠ¨ä¼˜å…ˆç´¢å¼•ä¼šä¸¥é‡æƒ©ç½šæ­¤ç±»é¡µé¢ã€‚", "missing_viewport_sugg": "åœ¨ <head> ä¸­æ·»åŠ æ ‡å‡†çš„ viewport meta æ ‡ç­¾ã€‚",
        "missing_canonical": "ç¼ºå¤± Canonical æ ‡ç­¾", "missing_canonical_desc": "æœªæŒ‡å®šè§„èŒƒé“¾æ¥ã€‚", "missing_canonical_impact": "æ— æ³•åº”å¯¹ URL å‚æ•°ï¼ˆå¦‚ ?id=1ï¼‰å¯¼è‡´çš„é‡å¤å†…å®¹é—®é¢˜ï¼Œå®¹æ˜“é€ æˆæƒé‡ç¨€é‡Šã€‚", "missing_canonical_sugg": "åœ¨æ‰€æœ‰é¡µé¢æ·»åŠ è‡ªå¼•ç”¨ï¼ˆSelf-referencingï¼‰æˆ–æŒ‡å‘åŸä»¶çš„ Canonical æ ‡ç­¾ã€‚",
        "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®", "missing_jsonld_desc": "æœªæ£€æµ‹åˆ° Schema.org æ ‡è®°ã€‚", "missing_jsonld_impact": "é”™å¤±å¯Œåª’ä½“æœç´¢ç»“æœï¼ˆRich Resultsï¼‰ï¼Œåœ¨ SERP ä¸­ä¸å¦‚ç«äº‰å¯¹æ‰‹æ˜¾çœ¼ã€‚", "missing_jsonld_sugg": "å»ºè®®é…ç½®ç»“æ„åŒ–æ•°æ®ã€‚åŸºäºé¡µé¢å†…å®¹ï¼Œæ¨èæ·»åŠ ï¼š{}ã€‚",
        "missing_hreflang": "ç¼ºå¤± Hreflang", "missing_hreflang_desc": "æœªå‘ç°è¯­è¨€åŒºåŸŸæ ‡è®°ï¼ˆHTMLæˆ–Sitemapä¸­å‡æ— ï¼‰ã€‚", "missing_hreflang_impact": "å¤šè¯­è¨€ç«™ç‚¹æ— æ³•æ­£ç¡®å®šä½ç›®æ ‡å—ä¼—ï¼Œå¯¼è‡´æµé‡ä¸ç²¾å‡†ã€‚", "missing_hreflang_sugg": "åœ¨ HTML å¤´éƒ¨æˆ– Sitemap ä¸­é…ç½® hreflang æ ‡ç­¾ã€‚",
        "soft_404": "ç–‘ä¼¼è½¯ 404 (Soft 404)", "soft_404_desc": "é¡µé¢è¿”å› 200 çŠ¶æ€ç ä½†å†…å®¹æ˜¾ç¤ºâ€œæœªæ‰¾åˆ°â€ã€‚", "soft_404_impact": "ä¸¥é‡æµªè´¹çˆ¬è™«é¢„ç®—ï¼Œå¯¼è‡´æ— æ•ˆé¡µé¢æŒ¤å æœ‰æ•ˆé¡µé¢çš„ç´¢å¼•åé¢ã€‚", "soft_404_sugg": "é…ç½®æœåŠ¡å™¨å¯¹ä¸å­˜åœ¨çš„é¡µé¢è¿”å› 404 HTTP çŠ¶æ€ç ã€‚",
        "missing_alt": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§", "missing_alt_desc": "å›¾ç‰‡æ ‡ç­¾ç¼ºå°‘ alt å±æ€§ã€‚", "missing_alt_impact": "æœç´¢å¼•æ“æ— æ³•ç†è§£å›¾ç‰‡å†…å®¹ï¼Œé”™å¤±å›¾ç‰‡æœç´¢æµé‡ã€‚", "missing_alt_sugg": "ä¸ºæ‰€æœ‰æœ‰æ„ä¹‰çš„å›¾ç‰‡æ·»åŠ æè¿°æ€§çš„ alt å±æ€§ã€‚",
        "js_links": "å‘ç° JS ä¼ªé“¾æ¥", "js_links_desc": "ä½¿ç”¨äº† href='javascript:...' å½¢å¼çš„é“¾æ¥ã€‚", "js_links_impact": "çˆ¬è™«æ— æ³•è·Ÿè¸ªæ­¤ç±»é“¾æ¥ï¼Œå¯¼è‡´å†…éƒ¨é“¾æ¥æ–­è£‚ï¼Œæ·±å±‚é¡µé¢å˜æˆâ€œå­¤å²›â€ã€‚", "js_links_sugg": "ä½¿ç”¨æ ‡å‡†çš„ <a href> æ ‡ç­¾ï¼Œä»…åœ¨ onclick äº‹ä»¶ä¸­å¤„ç† JS é€»è¾‘ã€‚",
        "url_underscore": "URL åŒ…å«ä¸‹åˆ’çº¿", "url_underscore_desc": "URL è·¯å¾„ä¸­ä½¿ç”¨ä¸‹åˆ’çº¿ (_) åˆ†éš”å•è¯ã€‚", "url_underscore_impact": "Google å»ºè®®ä½¿ç”¨è¿å­—ç¬¦ã€‚ä¸‹åˆ’çº¿å¯èƒ½å¯¼è‡´å…³é”®è¯æ— æ³•è¢«æ­£ç¡®åˆ‡åˆ†ï¼ˆè¢«è§†ä¸ºä¸€ä¸ªé•¿å•è¯ï¼‰ã€‚", "url_underscore_sugg": "åœ¨ URL ç»“æ„ä¸­ä½¿ç”¨è¿å­—ç¬¦ (-) ä»£æ›¿ä¸‹åˆ’çº¿ã€‚",
        "url_uppercase": "URL åŒ…å«å¤§å†™å­—æ¯", "url_uppercase_desc": "URL è·¯å¾„ä¸­æ··ç”¨äº†å¤§å†™å­—æ¯ã€‚", "url_uppercase_impact": "æœåŠ¡å™¨é€šå¸¸åŒºåˆ†å¤§å°å†™ï¼Œææ˜“é€ æˆä¸€é¡µå¤šå€ï¼ˆDuplicate Contentï¼‰å’Œ 404 é”™è¯¯ã€‚", "url_uppercase_sugg": "å¼ºåˆ¶æ‰€æœ‰ URL ä½¿ç”¨å°å†™å­—æ¯ã€‚",
        
        # Baidu specific
        "missing_keywords": "Missing Meta Keywords (Baidu)",
        "missing_keywords_desc": "No <meta name='keywords'> tag found.",
        "missing_keywords_impact": "Baidu still uses keywords as a ranking signal, unlike Google.",
        "missing_keywords_sugg": "Add meta keywords tag with 3-5 relevant keywords.",
        "missing_baidu_stats": "Missing Baidu Analytics",
        "missing_baidu_stats_desc": "Baidu Tongji script (hm.baidu.com) not found.",
        "missing_baidu_stats_impact": "Unable to track Baidu traffic effectively.",
        "missing_baidu_stats_sugg": "Install Baidu Tongji script.",
        "missing_baidu_verify": "Missing Baidu Verification",
        "missing_baidu_verify_desc": "No 'baidu-site-verification' tag found.",
        "missing_baidu_verify_impact": "May delay site indexing on Baidu.",
        "missing_baidu_verify_sugg": "Add verification tag.",
        "baidu_robots_missing": "Missing Baidu Rules",
        "baidu_robots_missing_desc": "No specific rules for 'Baiduspider' in Robots.txt.",
        "baidu_robots_missing_impact": "Inefficient crawling by Baidu.",
        "baidu_robots_missing_sugg": "Add User-agent: Baiduspider directives.",
        "missing_applicable_device": "Missing Applicable Device (Baidu)",
        "missing_applicable_device_desc": "Meta tag 'applicable-device' not found.",
        "missing_applicable_device_impact": "Baidu can't identify if page is PC/Mobile adapted.",
        "missing_applicable_device_sugg": "Add <meta name='applicable-device' content='pc,mobile'>.",
        "missing_no_transform": "Missing No-transform (Baidu)",
        "missing_no_transform_desc": "Cache-Control: no-transform not found.",
        "missing_no_transform_impact": "Baidu might transcode your page (Siteapp), breaking layout.",
        "missing_no_transform_sugg": "Add <meta http-equiv='Cache-Control' content='no-transform'>."
    }
}

# --- 6. æ ¸å¿ƒé€»è¾‘ (Data Layer) ---
def get_translated_text(issue_id, lang, args=None):
    if args is None: args = []
    t = TRANSLATIONS[lang]
    
    def safe_format(text, arguments):
        try: return text.format(*arguments)
        except IndexError: return text

    return {
        "title": t.get(issue_id, issue_id),
        "desc": safe_format(t.get(issue_id + "_desc", ""), args),
        "impact": t.get(issue_id + "_impact", ""),
        "suggestion": safe_format(t.get(issue_id + "_sugg", ""), args)
    }

def fetch_psi_data(url, api_key):
    if not api_key: return None
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available"}
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else: return {"error": f"API Error: {response.status_code}"}
    except Exception as e: return {"error": str(e)}

def check_cwv_issues(cwv_data, url, label=""):
    issues = []
    if not cwv_data or "error" in cwv_data: return issues
    category_key = "cwv_performance"
    
    # Thresholds
    # LCP: Good < 2.5, Poor > 4.0
    lcp = cwv_data.get("LCP", 0)
    if lcp > 2.5:
        issues.append({
            "id": "lcp_issue", "category": category_key, "severity": "Critical" if lcp > 4.0 else "High",
            "url": url, "args": [lcp], "examples": [f"{url} ({lcp:.2f}s) {label}"] 
        })
    
    # INP: Good < 200, Poor > 500
    inp = cwv_data.get("INP", 0)
    if inp > 200:
        issues.append({
            "id": "inp_issue", "category": category_key, "severity": "Critical" if inp > 500 else "High",
            "url": url, "args": [inp], "examples": [f"{url} ({inp}ms) {label}"]
        })

    # CLS: Good < 0.1, Poor > 0.25
    cls = cwv_data.get("CLS", 0)
    if cls > 0.1:
        issues.append({
            "id": "cls_issue", "category": category_key, "severity": "Critical" if cls > 0.25 else "High",
            "url": url, "args": [cls], "examples": [f"{url} ({cls:.3f}) {label}"]
        })
    
    # FCP: Good <= 1.8
    fcp = cwv_data.get("FCP", 0)
    if fcp > 1.8:
        issues.append({
            "id": "fcp_issue", "category": category_key, "severity": "Critical" if fcp > 3.0 else "Medium",
            "url": url, "args": [fcp], "examples": [f"{url} ({fcp:.2f}s) {label}"]
        })

    return issues

def check_site_level_assets(start_url, lang="zh", check_robots=True, crawl_sitemap_flag=True, manual_sitemaps=None, baidu_mode=False):
    issues = []
    sitemap_has_hreflang = False
    
    initial_netloc = urlparse(start_url).netloc
    base_url = f"{urlparse(start_url).scheme}://{initial_netloc}"
    headers = get_browser_headers()
    
    # 1. Robots.txt Logic
    robots_url = urljoin(base_url, "/robots.txt")
    if check_robots:
        try:
            r = requests.get(robots_url, headers=headers, timeout=10, allow_redirects=True, stream=True, verify=False)
            if r.status_code != 200:
                issues.append({"id": "no_robots", "category": "access", "severity": "Medium", "url": robots_url, "examples": [robots_url]})
            else:
                content = r.text.lower()
                if len(content.strip()) < 5:
                     issues.append({"id": "robots_quality_issue", "category": "access", "severity": "Medium", "url": robots_url, "args": ["File is empty or too short"], "examples": [robots_url]})
                if "user-agent" not in content:
                     issues.append({"id": "robots_quality_issue", "category": "access", "severity": "Medium", "url": robots_url, "args": ["Missing User-agent directive"], "examples": [robots_url]})
                if "disallow: /*.css" in content or "disallow: /*.js" in content:
                     issues.append({"id": "robots_quality_issue", "category": "access", "severity": "High", "url": robots_url, "args": ["Blocking CSS/JS resources"], "examples": [robots_url]})
                
                if "disallow: /" in content and "allow:" not in content:
                    issues.append({"id": "robots_bad_rule", "category": "access", "severity": "Critical", "url": robots_url, "examples": [robots_url]})
                
                if baidu_mode:
                    if "baiduspider" not in content:
                        issues.append({"id": "baidu_robots_missing", "category": "access", "severity": "Low", "url": robots_url, "examples": [robots_url]})
                
                if "sitemap:" not in content:
                    issues.append({"id": "robots_no_sitemap", "category": "access", "severity": "Low", "url": robots_url, "examples": [robots_url]})
                
                # Auto-discover Sitemap
                if crawl_sitemap_flag:
                    sitemaps_in_robots = re.findall(r'sitemap:\s*(https?://\S+)', content, re.IGNORECASE)
                    if sitemaps_in_robots:
                        if manual_sitemaps is None: manual_sitemaps = []
                        manual_sitemaps.extend(sitemaps_in_robots)
            r.close()
        except: 
            issues.append({"id": "no_robots", "category": "access", "severity": "Medium", "url": robots_url, "examples": [robots_url]})

    # 2. Sitemap Logic
    sitemap_urls = manual_sitemaps if manual_sitemaps else [urljoin(base_url, "/sitemap.xml")]
    any_valid = False
    for sm_url in sitemap_urls:
        if not sm_url.strip(): continue
        try:
            r = requests.get(sm_url, headers=headers, timeout=15, verify=False)
            if r.status_code == 200:
                try:
                    ET.fromstring(r.content)
                    any_valid = True
                    if 'hreflang' in r.text or 'xhtml' in r.text: sitemap_has_hreflang = True
                except:
                    if not sm_url.endswith('.gz'):
                        issues.append({"id": "sitemap_invalid", "category": "access", "severity": "Medium", "url": sm_url, "examples": [sm_url]})
            else:
                if manual_sitemaps: issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "url": sm_url, "examples": [sm_url]})
        except:
            if manual_sitemaps: issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "url": sm_url, "examples": [sm_url]})

    if not any_valid and not manual_sitemaps:
         issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "url": sitemap_urls[0], "examples": [sitemap_urls[0]]})

    # 3. Favicon
    try:
        r = requests.get(urljoin(base_url, "/favicon.ico"), headers=headers, timeout=5, verify=False)
        if r.status_code != 200 or int(r.headers.get('content-length', 0)) == 0:
            issues.append({"id": "no_favicon", "category": "image_ux", "severity": "Low", "url": base_url, "examples": [base_url]})
    except: pass

    return issues, sitemap_has_hreflang

def analyze_page(url, content, status, sitemap_has_hreflang, baidu_mode=False):
    soup = BeautifulSoup(content, 'html.parser')
    issues = []
    
    title = soup.title.string.strip() if soup.title else None
    desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = desc['content'].strip() if desc else None
    h1 = soup.find('h1')
    h1_content = h1.get_text().strip() if h1 else None
    
    can_tag = soup.find('link', attrs={'rel': 'canonical'})
    can_url = can_tag['href'] if can_tag else None

    if status == 200:
        is_self_canonical = True
        if can_url:
            def norm_u(u): return u.split('#')[0].rstrip('/')
            try:
                abs_can = urljoin(url, can_url)
                if norm_u(abs_can) != norm_u(url):
                    is_self_canonical = False
            except: pass

        if not can_url:
            issues.append({"id": "missing_canonical", "category": "indexability", "severity": "Medium", "url": url})
            is_self_canonical = True

        hreflangs = soup.find_all('link', hreflang=True)
        if hreflangs:
            has_x_default = False
            invalid = []
            pat = re.compile(r'^[a-z]{2}(-[a-zA-Z]{2})?$|x-default', re.IGNORECASE)
            for link in hreflangs:
                code = link.get('hreflang', '').strip()
                if code.lower() == 'x-default': has_x_default = True
                if not pat.match(code): invalid.append(code)
            if invalid:
                issues.append({"id": "hreflang_invalid", "category": "indexability", "severity": "High", "url": url, "args": [", ".join(invalid[:3])]})
            if not has_x_default:
                issues.append({"id": "hreflang_no_default", "category": "indexability", "severity": "Low", "url": url})
        elif not sitemap_has_hreflang:
             if is_self_canonical:
                issues.append({"id": "missing_hreflang", "category": "indexability", "severity": "Low", "url": url})

        if is_self_canonical:
            if not soup.find('meta', attrs={'name': 'viewport'}):
                issues.append({"id": "missing_viewport", "category": "technical", "severity": "Critical", "url": url})
            
            if not soup.find('script', type='application/ld+json'):
                 path = urlparse(url).path.lower()
                 rec = "BreadcrumbList"
                 if path in ["/", ""]: rec = "Organization/WebSite"
                 elif any(x in path for x in ["product", "shop"]): rec = "Product"
                 elif any(x in path for x in ["blog", "news"]): rec = "Article"
                 issues.append({"id": "missing_jsonld", "category": "technical", "severity": "Medium", "url": url, "args": [rec]})

            if '_' in url: issues.append({"id": "url_underscore", "category": "technical", "severity": "Low", "url": url})
            if any(c.isupper() for c in urlparse(url).path): issues.append({"id": "url_uppercase", "category": "technical", "severity": "Medium", "url": url})
            
            if soup.find('a', href=lambda x: x and x.lower().startswith('javascript:')):
                issues.append({"id": "js_links", "category": "access", "severity": "High", "url": url}) 

            imgs = soup.find_all('img')
            missing_alt = 0
            bad_alt = 0
            cls_risk = 0
            for img in imgs:
                alt = img.get('alt', '').strip()
                if not alt: missing_alt += 1
                elif len(alt) < 3 or any(x in alt.lower() for x in ["image", "photo", "img"]): bad_alt += 1
                if not img.get('width') or not img.get('height'): cls_risk += 1
            
            if missing_alt > 0: issues.append({"id": "missing_alt", "category": "image_ux", "severity": "Medium", "url": url})
            if bad_alt > 0: issues.append({"id": "alt_bad_quality", "category": "image_ux", "severity": "Low", "url": url})
            if cls_risk > 0: issues.append({"id": "cls_risk", "category": "cwv_performance", "severity": "Medium", "url": url})

            links = soup.find_all('a', href=True)
            bad_anchors = ["click here", "read more", "more"]
            if any(a.get_text().strip().lower() in bad_anchors for a in links):
                issues.append({"id": "anchor_bad_quality", "category": "access", "severity": "Low", "url": url})
            
            if not title: 
                issues.append({"id": "missing_title", "category": "content", "severity": "High", "url": url})
            else:
                px_w = estimate_pixel_width(title)
                if px_w < 200:
                    issues.append({"id": "short_title", "category": "content", "severity": "Medium", "url": url, "evidence": title, "args": [int(px_w)]})
                elif px_w > 600:
                    issues.append({"id": "long_title", "category": "content", "severity": "Low", "url": url, "evidence": title, "args": [int(px_w)]})

            if not desc_content: 
                issues.append({"id": "missing_desc", "category": "content", "severity": "High", "url": url})
            else:
                px_w_d = estimate_pixel_width(desc_content)
                if px_w_d < 400:
                    issues.append({"id": "short_desc", "category": "content", "severity": "Low", "url": url, "evidence": desc_content, "args": [int(px_w_d)]})

            if not h1_content: issues.append({"id": "missing_h1", "category": "content", "severity": "High", "url": url})

            if (title and "not found" in title.lower()) or (soup.find('h1') and "not found" in soup.find('h1').get_text().lower()):
                issues.append({"id": "soft_404", "category": "access", "severity": "Critical", "url": url})
        
        if baidu_mode:
            keywords = soup.find('meta', attrs={'name': 'keywords'})
            if not keywords or not keywords.get('content', '').strip():
                 issues.append({"id": "missing_keywords", "category": "content", "severity": "Medium", "url": url})
            
            if "hm.baidu.com" not in str(soup):
                 issues.append({"id": "missing_baidu_stats", "category": "technical", "severity": "Low", "url": url})
            
            if not soup.find('meta', attrs={'name': 'applicable-device'}):
                 issues.append({"id": "missing_applicable_device", "category": "technical", "severity": "Medium", "url": url})
            
            has_no_transform = False
            for meta in soup.find_all('meta'):
                if meta.get('http-equiv', '').lower() == 'cache-control' and 'no-transform' in meta.get('content', '').lower():
                    has_no_transform = True
                    break
            if not has_no_transform:
                 issues.append({"id": "missing_no_transform", "category": "technical", "severity": "Medium", "url": url})


    return {
        "URL": url, 
        "Status": status, 
        "Title": title, 
        "Description": desc_content,
        "H1": h1_content,
        "Canonical": can_url,
        "Content_Hash": hashlib.md5(soup.get_text().encode('utf-8')).hexdigest()
    }, issues

def crawl_website(start_url, max_pages, lang, manual_robots, manual_sitemaps, psi_key, list_url=None, detail_url=None, check_robots=True, crawl_sitemap=True, allow_sub=False, allow_outside=False, baidu_mode=False):
    visited = set()
    seen_hashes = {} 
    seen_urls = set()
    
    queue = [start_url]
    seen_urls.add(start_url)
    if list_url and is_valid_url(list_url):
         queue.append(list_url)
         seen_urls.add(list_url)
    if detail_url and is_valid_url(detail_url):
         queue.append(detail_url)
         seen_urls.add(detail_url)

    results_data = []
    all_issues = []
    first_error = None
    target_domain = None
    
    start_netloc = urlparse(start_url).netloc.replace('www.', '')
    start_path = urlparse(start_url).path
    if not start_path.endswith('/'): start_path += '/'
    
    def clean_url(u): return u.split('?')[0].split('#')[0]

    progress_bar = st.progress(0, text="Initializing...")
    sitemap_has_hreflang = False
    
    try:
        site_issues, sitemap_has_hreflang = check_site_level_assets(
            start_url, lang, check_robots, crawl_sitemap, manual_sitemaps, baidu_mode
        )
        all_issues.extend(site_issues)
        st.session_state['sitemap_hreflang_found'] = sitemap_has_hreflang
    except Exception as e:
        pass

    if psi_key:
        with st.spinner(TRANSLATIONS[lang]["psi_fetching"].format("Pages")):
            targets = [("Home", start_url)]
            if list_url and is_valid_url(list_url): targets.append(("List", list_url))
            if detail_url and is_valid_url(detail_url): targets.append(("Detail", detail_url))
            
            for label, t_url in targets:
                cwv_data = fetch_psi_data(t_url, psi_key)
                if cwv_data and "error" not in cwv_data:
                    if label == "Home": st.session_state['cwv_data'] = cwv_data
                    all_issues.extend(check_cwv_issues(cwv_data, t_url, label=f"({label})"))

    count = 0
    headers = get_browser_headers()
    
    while queue and count < max_pages:
        url = queue.pop(0)
        visited.add(url)
        
        if any(x in url.lower() for x in ['/login', '/signin', '/admin', '/cart', '/account']):
            continue

        count += 1
        progress_bar.progress(int(count/max_pages*100), text=f"Crawling ({count}/{max_pages}): {url}")
        time.sleep(0.1)
        
        try:
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
            current_url = response.url 
            
            if count == 1 and url == start_url:
                 start_netloc = urlparse(current_url).netloc.replace('www.', '')

            final_status = response.status_code

            if response.history:
                chain_list = [r.url for r in response.history] + [current_url]
                origin_netloc = urlparse(chain_list[0]).netloc.replace('www.', '')
                chain_display_parts = []
                for u in chain_list:
                    u_obj = urlparse(u)
                    u_netloc = u_obj.netloc.replace('www.', '')
                    if u_netloc != origin_netloc:
                        chain_display_parts.append(u) 
                    else:
                        p = u_obj.path
                        if not p: p = "/"
                        chain_display_parts.append(p)

                chain_str = " -> ".join(chain_display_parts)
                all_issues.append({"id": "http_3xx", "category": "access", "severity": "Medium", "url": url, "args": [chain_str]})

            if final_status >= 400:
                is_5xx = final_status >= 500
                all_issues.append({"id": "http_5xx" if is_5xx else "http_4xx", "category": "access", "severity": "Critical" if is_5xx else "High", "url": url, "args": [str(final_status)]})

            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                if 'type="password"' in response.text.lower():
                     continue

                page_data, page_issues = analyze_page(current_url, response.content, final_status, sitemap_has_hreflang, baidu_mode)
                
                if final_status == 200:
                    current_hash = page_data['Content_Hash']
                    current_canonical = page_data['Canonical']
                    current_clean = clean_url(current_url)
                    
                    if current_hash in seen_hashes:
                        original_url = seen_hashes[current_hash]
                        if current_url != original_url and not (current_canonical and current_canonical != current_url):
                            all_issues.append({
                                "id": "duplicate", "category": "indexability", 
                                "severity": "High", "url": current_url, 
                                "meta": original_url 
                            })
                    else:
                        seen_hashes[current_hash] = current_url

                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                soup = BeautifulSoup(response.content, 'html.parser')
                for a in soup.find_all('a', href=True):
                    raw_link = urljoin(current_url, a['href'])
                    link = raw_link.split('#')[0] 
                    
                    link_parsed = urlparse(link)
                    link_netloc = link_parsed.netloc.replace('www.', '')
                    link_path = link_parsed.path

                    is_internal = False
                    if not link_netloc: is_internal = True
                    elif allow_sub:
                        is_internal = link_netloc.endswith(start_netloc)
                    else:
                        is_internal = link_netloc == start_netloc

                    path_ok = True
                    if not allow_outside:
                        if not link_path.startswith(start_path): path_ok = False
                    
                    if is_internal and path_ok and link not in seen_urls:
                        if not any(link.lower().endswith(ext) for ext in ['.jpg', '.png', '.pdf', '.zip', '.css', '.js', '.json', '.xml']):
                            seen_urls.add(link)
                            queue.append(link)
            else:
                if count == 1: first_error = f"Content type: {content_type}"
        except Exception as e:
            if count == 1: first_error = str(e)
            pass
    
    progress_bar.empty()
    if not results_data and first_error: return None, None, first_error
    return results_data, all_issues, None

def draw_cwv_gauge(slide, metric, value, thresholds):
    good, poor = thresholds
    total_w = 4
    x_start = 7.2
    y_pos = 4.8
    h = 0.4

    r1 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(x_start), Inches(y_pos), Inches(total_w/3), Inches(h))
    r1.fill.solid()
    r1.fill.fore_color.rgb = RGBColor(12, 206, 107)
    r1.line.fill.background()

    r2 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(x_start + total_w/3), Inches(y_pos), Inches(total_w/3), Inches(h))
    r2.fill.solid()
    r2.fill.fore_color.rgb = RGBColor(255, 164, 0)
    r2.line.fill.background()
    
    r3 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(x_start + 2*total_w/3), Inches(y_pos), Inches(total_w/3), Inches(h))
    r3.fill.solid()
    r3.fill.fore_color.rgb = RGBColor(255, 78, 66)
    r3.line.fill.background()
    
    def add_label(x_offset, text):
        tb = slide.shapes.add_textbox(Inches(x_start + x_offset - 0.2), Inches(y_pos + 0.40), Inches(0.5), Inches(0.3))
        tb.margin_top = 0
        p = tb.text_frame.add_paragraph()
        p.text = text
        p.font.size = Pt(9)
        p.font.color.rgb = RGBColor(80, 80, 80)

    add_label(0, "0")
    add_label(total_w/3, str(good))
    add_label(2*total_w/3, str(poor))

    pos = 0
    if value <= good:
        pos = (value / good) * (total_w/3)
    elif value <= poor:
        pos = (total_w/3) + ((value - good) / (poor - good)) * (total_w/3)
    else:
        cap = poor * 1.5
        normalized = min(value, cap)
        pos = (2*total_w/3) + ((normalized - poor) / (cap - poor)) * (total_w/3)

    marker_x = x_start + pos
    
    tri = slide.shapes.add_shape(MSO_SHAPE.ISOSCELES_TRIANGLE, Inches(marker_x - 0.1), Inches(y_pos - 0.2), Inches(0.2), Inches(0.2))
    tri.rotation = 180
    tri.fill.solid()
    tri.fill.fore_color.rgb = RGBColor(50, 50, 50)
    tri.line.fill.background()
    
    tb = slide.shapes.add_textbox(Inches(marker_x - 0.5), Inches(y_pos - 0.8), Inches(1), Inches(0.3))
    p = tb.text_frame.add_paragraph()
    p.text = f"{value}"
    p.font.size = Pt(11)
    p.font.bold = True
    p.alignment = PP_ALIGN.CENTER


def create_styled_pptx(slides_data, lang):
    prs = Presentation()
    prs.slide_width = Inches(13.333)
    prs.slide_height = Inches(7.5)
    txt = TRANSLATIONS[lang] 
    
    def set_font(font_obj, size, bold=False, color=None):
        font_obj.size = Pt(size)
        font_obj.name = 'Microsoft YaHei' if lang == "zh" else 'Arial'
        font_obj.bold = bold
        if color: font_obj.color.rgb = color

    def draw_serp_preview(slide, issue_id, issue_title, evidence, url):
        box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(1.8))
        box.fill.solid()
        box.fill.fore_color.rgb = RGBColor(255, 255, 255)
        box.line.color.rgb = RGBColor(220, 220, 220)
        
        label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
        p = label.text_frame.add_paragraph()
        
        if "favicon" in issue_id:
             p.text = txt["visual_sim_title"]
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             circle = slide.shapes.add_shape(MSO_SHAPE.OVAL, Inches(7.2), Inches(4.3), Inches(0.25), Inches(0.25))
             circle.fill.solid()
             circle.fill.fore_color.rgb = RGBColor(200, 200, 200) 
             l1 = slide.shapes.add_shape(MSO_SHAPE.ARC, Inches(7.2), Inches(4.3), Inches(0.25), Inches(0.25))
             l1.line.color.rgb = RGBColor(150, 150, 150)
             l2 = slide.shapes.add_shape(MSO_SHAPE.CONNECTOR_STRAIGHT, Inches(7.325), Inches(4.3), Inches(7.325), Inches(4.55))
             l2.line.color.rgb = RGBColor(150, 150, 150)

             tb = slide.shapes.add_textbox(Inches(7.5), Inches(4.25), Inches(4), Inches(0.4))
             p2 = tb.text_frame.add_paragraph()
             p2.text = urlparse(url).netloc
             set_font(p2.font, 12, False, RGBColor(32, 33, 36))
             
             tb_t = slide.shapes.add_textbox(Inches(7.2), Inches(4.6), Inches(5), Inches(0.4))
             p_t = tb_t.text_frame.add_paragraph()
             p_t.text = "Page Title Example"
             set_font(p_t.font, 16, False, RGBColor(26, 13, 171))
             
             tb_d = slide.shapes.add_textbox(Inches(7.2), Inches(5.0), Inches(5), Inches(0.4))
             p_d = tb_d.text_frame.add_paragraph()
             p_d.text = "This simulates a missing favicon result on Google mobile SERP."
             set_font(p_d.font, 12, False, RGBColor(80, 80, 80))

        elif "alt" in issue_id:
             p.text = "Screen Reader View:"
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             
             img_box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(7.2), Inches(4.2), Inches(1.5), Inches(1.0))
             img_box.line.color.rgb = RGBColor(100, 100, 100)
             img_box.fill.background() 
             
             bubble_text = '<img src="..." />'
             color = RGBColor(200, 0, 0)
             if "quality" in issue_id:
                 bubble_text = '<img src="..." alt="image" />'
                 color = RGBColor(255, 165, 0)
             
             callout = slide.shapes.add_shape(MSO_SHAPE.LINE_CALLOUT_2, Inches(8.8), Inches(4.2), Inches(3.5), Inches(0.8))
             callout.fill.solid()
             callout.fill.fore_color.rgb = RGBColor(240, 240, 240)
             callout.text_frame.text = bubble_text
             callout.text_frame.paragraphs[0].font.color.rgb = color
             
        elif "lcp" in issue_id:
             p.text = txt["cwv_sim_title"] + " LCP"
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             val = float(re.findall(r"(\d+\.\d+)", evidence)[0]) if re.findall(r"(\d+\.\d+)", evidence) else 3.0
             draw_cwv_gauge(slide, "LCP", val, (2.5, 4.0))

        elif "inp" in issue_id:
             p.text = txt["cwv_sim_title"] + " INP"
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             val = float(re.findall(r"(\d+)", evidence)[0]) if re.findall(r"(\d+)", evidence) else 300
             draw_cwv_gauge(slide, "INP", val, (200, 500))

        elif "cls" in issue_id:
             p.text = txt["cwv_sim_title"] + " CLS"
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             val = float(re.findall(r"(\d+\.\d+)", evidence)[0]) if re.findall(r"(\d+\.\d+)", evidence) else 0.2
             draw_cwv_gauge(slide, "CLS", val, (0.1, 0.25))

        elif "fcp" in issue_id:
             p.text = txt["cwv_sim_title"] + " FCP"
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             val = float(re.findall(r"(\d+\.\d+)", evidence)[0]) if re.findall(r"(\d+\.\d+)", evidence) else 2.0
             draw_cwv_gauge(slide, "FCP", val, (1.8, 3.0))
             
        elif "3xx" in issue_id:
             p.text = "Redirect Flow:"
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             
             parts = evidence.split(' -> ')
             if len(parts) > 1:
                 x = 7.2
                 display_parts = parts[:3]
                 if len(parts) > 3: display_parts[-1] = "Final"

                 for i, part in enumerate(display_parts):
                     box_w = 2.0 if len(part) > 20 else 1.5
                     b = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(x), Inches(4.5), Inches(box_w), Inches(0.6))
                     b.fill.solid()
                     b.fill.fore_color.rgb = RGBColor(240, 240, 240) if i < len(display_parts)-1 else RGBColor(220, 255, 220)
                     b.text_frame.text = part
                     b.text_frame.paragraphs[0].font.size = Pt(9)
                     b.text_frame.paragraphs[0].font.color.rgb = RGBColor(0,0,0)
                     
                     if i < len(display_parts) - 1:
                         ar = slide.shapes.add_shape(MSO_SHAPE.RIGHT_ARROW, Inches(x+box_w), Inches(4.7), Inches(0.3), Inches(0.2))
                         ar.fill.solid()
                         ar.fill.fore_color.rgb = RGBColor(100, 100, 100)
                         x += (box_w + 0.3)
             
        else:
             p.text = txt["serp_sim_title"]
             set_font(p.font, 12, True, RGBColor(100, 100, 100))
             tf = box.text_frame
             tf.margin_left = Inches(0.2)
             tf.margin_top = Inches(0.2)
             p_serp = tf.add_paragraph()
             p_serp.text = f"{urlparse(url).netloc} â€º ..."
             set_font(p_serp.font, 12, False, RGBColor(32, 33, 36))
             
             if "short_desc" in issue_id or "missing_desc" in issue_id:
                 p_serp = tf.add_paragraph()
                 p_serp.space_before = Pt(5)
                 p_serp.text = evidence[:60] + "..." if evidence else "Title of the page"
                 set_font(p_serp.font, 18, False, RGBColor(26, 13, 171))
                 
                 p_serp = tf.add_paragraph()
                 p_serp.space_before = Pt(3)
                 if "missing" in issue_id:
                    p_serp.text = "No description available in code..."
                 else:
                    p_serp.text = evidence 
                 set_font(p_serp.font, 14, False, RGBColor(77, 81, 86))
             
             elif "long_title" in issue_id:
                 p_serp = tf.add_paragraph()
                 p_serp.space_before = Pt(5)
                 p_serp.text = evidence[:55] + " ..."
                 set_font(p_serp.font, 18, False, RGBColor(26, 13, 171))

                 p_serp = tf.add_paragraph()
                 p_serp.space_before = Pt(3)
                 p_serp.text = "The meta description of the page would appear here..."
                 set_font(p_serp.font, 14, False, RGBColor(77, 81, 86))

             else:
                 p_serp = tf.add_paragraph()
                 p_serp.space_before = Pt(5)
                 p_serp.text = evidence if evidence else "Untitled Page"
                 set_font(p_serp.font, 18, False, RGBColor(26, 13, 171)) 

    def draw_code_preview(slide):
        box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(1.5))
        box.fill.solid() # Fix
        box.fill.fore_color.rgb = RGBColor(245, 245, 245)
        box.line.color.rgb = RGBColor(200, 200, 200)
        tf = box.text_frame
        tf.margin_left = Inches(0.1)
        p = tf.add_paragraph()
        p.text = '<a href="javascript:void(0)">\n  Click Here\n</a>'
        set_font(p.font, 14, True, RGBColor(200, 0, 0)) # Red code
        
        label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
        p = label.text_frame.add_paragraph()
        p.text = txt["code_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100))

    def draw_hreflang_preview(slide, url, missing_type):
        box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(2.0))
        box.fill.solid()
        box.fill.fore_color.rgb = RGBColor(245, 245, 245)
        box.line.color.rgb = RGBColor(200, 200, 200)
        
        tf = box.text_frame
        tf.margin_left = Inches(0.2)
        tf.margin_top = Inches(0.2)
        
        p = tf.add_paragraph()
        p.text = "<!-- Correct Implementation -->"
        set_font(p.font, 10, False, RGBColor(128, 128, 128))
        
        p = tf.add_paragraph()
        clean_url = url.split('?')[0][:40] + "..."
        if "default" in missing_type:
             p.text = f'<link rel="alternate" hreflang="x-default" href="{clean_url}" />'
             set_font(p.font, 11, True, RGBColor(200, 0, 0)) # Red highlight
        else:
             p.text = f'<link rel="alternate" hreflang="en" href="{clean_url}" />\n<link rel="alternate" hreflang="fr" href="..." />'
             set_font(p.font, 11, False, RGBColor(0, 0, 128))

        label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
        p = label.text_frame.add_paragraph()
        p.text = txt["code_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100))

    def draw_rich_snippet_preview(slide, url):
        box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(2.0)) # Moved down
        box.fill.solid() # Fix
        box.fill.fore_color.rgb = RGBColor(255, 255, 255)
        box.line.color.rgb = RGBColor(220, 220, 220)
        tf = box.text_frame
        tf.margin_left = Inches(0.2)
        tf.margin_top = Inches(0.2)
        p = tf.add_paragraph()
        p.text = f"{urlparse(url).netloc} â€º product"
        set_font(p.font, 12, False, RGBColor(32, 33, 36))
        p = tf.add_paragraph()
        p.space_before = Pt(5)
        p.text = "Product Name Example - Best Choice"
        set_font(p.font, 18, False, RGBColor(26, 13, 171)) 
        p = tf.add_paragraph()
        p.space_before = Pt(3)
        p.text = "â˜…â˜…â˜…â˜…â˜… Rating: 4.8 Â· $199.00 Â· In stock"
        set_font(p.font, 12, False, RGBColor(231, 113, 27))
        p = tf.add_paragraph()
        p.space_before = Pt(3)
        p.text = "This is a rich result enabled by Schema..."
        set_font(p.font, 14, False, RGBColor(77, 81, 86))
        label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
        p = label.text_frame.add_paragraph()
        p.text = txt["rich_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100))

    # Cover
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg = slide.shapes.add_shape(1, 0, 0, Inches(13.333), Inches(7.5))
    bg.fill.solid() # Fix
    bg.fill.fore_color.rgb = RGBColor(18, 52, 86)
    title = slide.shapes.add_textbox(Inches(1), Inches(2.5), Inches(11), Inches(2))
    p = title.text_frame.add_paragraph()
    p.text = txt["ppt_cover_title"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 54, True, RGBColor(255, 255, 255))
    sub = slide.shapes.add_textbox(Inches(1), Inches(4), Inches(11), Inches(1))
    p = sub.text_frame.add_paragraph()
    p.text = txt["ppt_cover_sub"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 24, False, RGBColor(200, 200, 200))

    # Slides
    for s in slides_data:
        t_data = get_translated_text(s['id'], lang, s.get('args'))
        slide = prs.slides.add_slide(prs.slide_layouts[6])
        
        # Header
        h_shape = slide.shapes.add_shape(1, 0, 0, Inches(13.333), Inches(1.2))
        h_shape.fill.solid()
        h_shape.fill.fore_color.rgb = RGBColor(240, 242, 246)
        h_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(10), Inches(0.8))
        p = h_box.text_frame.add_paragraph()
        p.text = t_data['title']
        set_font(p.font, 32, True, RGBColor(50, 50, 50))
        
        sev_color = RGBColor(220, 53, 69) if s['severity'] == "Critical" else RGBColor(253, 126, 20)
        sev_box = slide.shapes.add_textbox(Inches(11), Inches(0.35), Inches(2), Inches(0.5))
        p = sev_box.text_frame.add_paragraph()
        p.text = s['severity']
        p.alignment = PP_ALIGN.CENTER
        set_font(p.font, 18, True, sev_color)
        
        # Category
        cat_key = f"cat_{s['category']}"
        cat_label = txt.get(cat_key, s['category'].upper())
        cat_box = slide.shapes.add_textbox(Inches(0.5), Inches(1.3), Inches(4), Inches(0.4))
        p = cat_box.text_frame.add_paragraph()
        p.text = cat_label
        set_font(p.font, 14, True, RGBColor(0, 102, 204))

        # Desc
        d_title = slide.shapes.add_textbox(Inches(0.5), Inches(1.8), Inches(6), Inches(0.5))
        p = d_title.text_frame.add_paragraph()
        p.text = txt["ppt_desc"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        d_box = slide.shapes.add_textbox(Inches(0.5), Inches(2.3), Inches(6), Inches(1.2))
        tf = d_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = t_data['desc']
        set_font(p.font, 14, False, RGBColor(80, 80, 80))
        
        # Impact
        i_title = slide.shapes.add_textbox(Inches(0.5), Inches(3.6), Inches(6), Inches(0.5))
        p = i_title.text_frame.add_paragraph()
        p.text = txt["ppt_business_impact"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        i_box = slide.shapes.add_textbox(Inches(0.5), Inches(4.1), Inches(6), Inches(1.2))
        tf = i_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = t_data['impact']
        set_font(p.font, 14, False, RGBColor(220, 53, 69))

        # Suggestion (Fixed Layout: Absolute Bottom)
        s_bg = slide.shapes.add_shape(1, Inches(0.5), Inches(5.8), Inches(12.333), Inches(1.5))
        s_bg.fill.solid()
        s_bg.fill.fore_color.rgb = RGBColor(230, 244, 234)
        s_bg.line.color.rgb = RGBColor(40, 167, 69)
        s_box = slide.shapes.add_textbox(Inches(0.7), Inches(5.9), Inches(11.9), Inches(1.3))
        tf = s_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = txt["ppt_slide_sugg_title"]
        set_font(p.font, 16, True, RGBColor(21, 87, 36))
        p = tf.add_paragraph()
        p.text = t_data['suggestion']
        set_font(p.font, 14, False, RGBColor(21, 87, 36))

        # Right Column Split (URL List)
        ex_title = slide.shapes.add_textbox(Inches(7), Inches(1.5), Inches(5.8), Inches(0.5))
        p = ex_title.text_frame.add_paragraph()
        p.text = txt["ppt_slide_ex_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        ex_box = slide.shapes.add_textbox(Inches(7), Inches(2.0), Inches(5.8), Inches(1.5)) 
        tf = ex_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        
        for idx, url in enumerate(s['examples'][:4]): 
            p = tf.add_paragraph()
            # Visualize Duplicate Logic
            if s['id'] == 'duplicate' and "Duplicate Group:" in url:
                 parts = url.split("\n")
                 p.text = f"Group {idx+1}:\n   â€¢ {parts[1].replace('- ', '').strip()}\n   â€¢ {parts[2].replace('- ', '').strip()}"
                 set_font(p.font, 10, False, RGBColor(80, 80, 80))
            else:
                 p.text = f"â€¢ {url}"
                 set_font(p.font, 11, False, RGBColor(0, 102, 204))
            
            p.space_after = Pt(6)

        # Visualization (Visual Area)
        is_serp = any(k in s['id'] for k in ["title", "desc", "favicon", "alt", "lcp", "inp", "cls", "3xx", "fcp"])
        is_rich = "jsonld" in s['id']
        is_code = "js_links" in s['id'] or "anchor" in s['id']
        is_hreflang = "hreflang" in s['id']
        is_cwv = any(k in s['id'] for k in ["lcp", "inp", "cls", "fcp", "risk"])
        is_img = "alt" in s['id'] or "favicon" in s['id']
        is_3xx = "3xx" in s['id'] 
        
        ev = s.get('example_evidence', '')
        ex_url = s['examples'][0] if s['examples'] else "example.com"
        # Cleaning URL for display
        if "Duplicate" in ex_url: ex_url = ex_url.split("\n")[1].replace("- ", "").strip()
        if "3xx" in s['id'] and s.get('args'): ev = s['args'][0]

        if is_code:
            draw_code_preview(slide)
        elif is_hreflang:
            type_str = s['id']
            if "invalid" in type_str and s.get('args'):
                type_str = f"invalid: {s['args'][0]}"
            draw_hreflang_preview(slide, ex_url, type_str)
        elif is_rich:
            draw_rich_snippet_preview(slide, ex_url)
        elif is_serp:
            draw_serp_preview(slide, s['id'], t_data['title'], ev, ex_url)

    out = BytesIO()
    prs.save(out)
    out.seek(0)
    return out

# --- 7. UI Logic ---
# åˆå§‹åŒ– Session State
if 'audit_data' not in st.session_state: st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state: st.session_state['audit_issues'] = []
if 'language' not in st.session_state: st.session_state['language'] = "zh"
if 'cwv_data' not in st.session_state: st.session_state['cwv_data'] = None
if 'sitemap_hreflang_found' not in st.session_state: st.session_state['sitemap_hreflang_found'] = False

lang = st.session_state['language']
ui = TRANSLATIONS[lang]

with st.sidebar:
    st.title(ui["sidebar_title"])
    st.caption(ui["sidebar_caption"])
    st.divider()
    
    sl = st.radio(ui["lang_label"], ["ä¸­æ–‡", "English"], index=0 if lang=="zh" else 1)
    if (sl == "ä¸­æ–‡" and lang == "en") or (sl == "English" and lang == "zh"):
        st.session_state['language'] = "zh" if sl == "ä¸­æ–‡" else "en"
        st.rerun()

    st.divider()
    opts = ui["nav_options"]
    keys = ["input", "dashboard", "matrix", "ppt"]
    sel = st.radio(ui["nav_label"], opts)
    menu_key = keys[opts.index(sel)]
    
    st.divider()
    if st.session_state['audit_data']:
        st.success(ui["cache_info"].format(len(st.session_state['audit_data'])))
        st.markdown(f"**{ui['sitemap_status_title']}**")
        if st.session_state['sitemap_hreflang_found']: st.caption(ui["sitemap_found_href"])
        else: st.caption(ui["sitemap_no_href"])
        
        if st.button(ui["clear_data"]):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.session_state['cwv_data'] = None
            st.rerun()

if menu_key == "input":
    st.header(ui["input_header"])
    st.info(ui["input_info"])
    c1, c2 = st.columns([3, 1])
    with c1: target_url = st.text_input(ui["input_label"], placeholder=ui["input_placeholder"])
    with c2: max_pages = st.number_input(ui.get("max_pages_label", "Max Pages"), min_value=1, max_value=1000, value=100)
    
    with st.expander(ui.get("adv_settings", "Advanced")):
        allow_sub = st.checkbox(ui["allow_subdomains_label"], value=False)
        allow_out = st.checkbox(ui["allow_outside_folder_label"], value=False)
        check_robots_flag = st.checkbox(ui["check_robots_label"], value=True)
        crawl_sitemap_flag = st.checkbox(ui["crawl_sitemap_label"], value=True)
        baidu_mode_flag = st.checkbox(ui["baidu_mode_label"], value=False) # New
        manual_sitemaps_text = st.text_area(ui.get("manual_sitemaps", "Manual Sitemaps"), placeholder="https://example.com/sitemap.xml")
        manual_sitemaps = [s.strip() for s in manual_sitemaps_text.split('\n') if s.strip()]
    
    with st.expander(ui.get("psi_settings", "Google PSI")):
        psi_key = st.text_input(ui.get("psi_api_key_label", "API Key"), type="password", help=ui.get("psi_api_help", ""))
        psi_list_url = st.text_input(ui.get("psi_list_url_label", "List URL"))
        psi_detail_url = st.text_input(ui.get("psi_detail_url_label", "Detail URL"))
        st.caption(ui["psi_get_key"])

    if st.button(ui["start_btn"], type="primary"):
        if not target_url or not is_valid_url(target_url): 
            st.error(ui["error_url"])
        else:
            with st.spinner(ui["spinner_crawl"].format(max_pages)):
                data, issues, error_msg = crawl_website(
                    target_url, max_pages, lang, None, manual_sitemaps, psi_key, 
                    psi_list_url, psi_detail_url, check_robots_flag, crawl_sitemap_flag,
                    allow_sub, allow_out, baidu_mode_flag
                )
                if not data:
                    st.error(ui["error_no_data"].format(error_msg or "Unknown Error"))
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(ui["success_audit"].format(len(data)))
                    st.balloons()

elif menu_key == "dashboard":
    st.header(ui["dashboard_header"])
    if not st.session_state['audit_data']: st.warning(ui["warn_no_data"])
    else:
        if st.session_state.get('cwv_data'):
            c = st.session_state['cwv_data']
            st.subheader(ui["cwv_title"])
            st.caption(ui["cwv_source"])
            c1, c2, c3, c4 = st.columns(4)
            def metric_color(val, good, poor):
                if val <= good: return "normal"
                if val >= poor: return "inverse"
                return "off"
            c1.metric("LCP (Loading)", f"{c['LCP']:.2f}s", delta_color=metric_color(c['LCP'], 2.5, 4.0))
            c2.metric("CLS (Visual)", f"{c['CLS']:.3f}", delta_color=metric_color(c['CLS'], 0.1, 0.25))
            c3.metric("INP (Interact)", f"{c['INP']}ms", delta_color=metric_color(c['INP'], 200, 500))
            c4.metric("FCP", f"{c['FCP']:.2f}s")
            st.divider()

        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        total = len(issues)
        score = max(0, 100 - int(total * 0.5))
        critical = len([i for i in issues if i['severity'] == 'Critical'])
        
        k1, k2, k3, k4 = st.columns(4)
        k1.metric(ui["kpi_health"], f"{score}/100")
        k2.metric(ui["kpi_pages"], str(len(df)))
        k3.metric(ui["kpi_issues"], str(total), delta_color="inverse")
        k4.metric(ui["kpi_critical"], str(critical), delta_color="inverse")
        
        st.divider()
        c1, c2 = st.columns(2)
        with c1:
            st.subheader(ui["chart_issues"])
            if issues:
                issue_counts = pd.DataFrame(issues)['id'].value_counts().reset_index()
                issue_counts.columns = ['id', 'count']
                issue_counts['name'] = issue_counts['id'].apply(lambda x: get_translated_text(x, lang)['title'])
                st.bar_chart(issue_counts.set_index('name'))
            else: st.info(ui["chart_no_issues"])
        with c2:
            st.subheader(ui["chart_status"])
            if not df.empty: st.bar_chart(df['Status'].value_counts())

elif menu_key == "matrix":
    st.header(ui["matrix_header"])
    if not st.session_state['audit_data']: st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(df, use_container_width=True)
        st.download_button(ui["download_csv"], df.to_csv().encode('utf-8'), "audit.csv")

elif menu_key == "ppt":
    st.header(ui["ppt_header"])
    if not st.session_state['audit_issues']: st.warning(ui["warn_no_data"])
    else:
        raw = st.session_state['audit_issues']
        grouped = {}
        for i in raw:
            iid = i['id']
            if iid not in grouped:
                grouped[iid] = {
                    "id": iid, "category": i['category'], "severity": i['severity'],
                    "count": 0, "examples": [], "args": i.get('args', []),
                    "example_evidence": i.get("evidence", "")
                }
            grouped[iid]['count'] += 1
            if len(grouped[iid]['examples']) < 5:
                if iid == "duplicate" and "meta" in i:
                     # Clean grouping for duplicate
                     grouped[iid]['examples'].append(f"Duplicate Group:\n- {i['url']}\n- {i['meta']}")
                else:
                     grouped[iid]['examples'].append(i['url'])
        
        slides = sorted(list(grouped.values()), key=lambda x: (
            CATEGORY_ORDER.index(x['category']),
            get_issue_priority(x['id']),
            SEVERITY_ORDER.get(x['severity'], 3)
        ))
        
        st.write(f"### {ui['ppt_download_header']}")
        st.info(ui["ppt_info"])
        if st.button(ui["ppt_btn"]):
            with st.spinner("Generating..."):
                f = create_styled_pptx(slides, lang)
                st.download_button(ui["ppt_btn"], f, f"seo_audit_{lang}.pptx")
        
        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(slides): st.session_state.slide_index = 0
        
        s = slides[st.session_state.slide_index]
        t_data = get_translated_text(s['id'], lang, s['args'])
        
        with st.container(border=True):
            st.caption(f"ğŸ“‚ {ui.get('cat_'+s['category'], s['category'])}")
            st.markdown(f"### {ui['ppt_slide_title']} {t_data['title']}")
            
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if s['severity'] == "Critical" else "orange"
                st.markdown(f"**{ui['ppt_severity']}** :{color}[{s['severity']}]")
                st.markdown(f"**{ui['ppt_impact']}** {ui['ppt_impact_desc'].format(s['count'])}")
                
                st.markdown(f"**{ui['ppt_desc']}**")
                st.write(t_data['desc'])
                
                st.markdown(f"**{ui['ppt_business_impact']}**") 
                st.error(t_data['impact']) 
                
                st.info(f"{ui['ppt_sugg']} {t_data['suggestion']}")
            with c2:
                # Visualization Logic
                is_serp = any(k in s['id'] for k in ["title", "desc", "favicon", "alt", "lcp", "inp", "cls", "3xx", "fcp"])
                is_rich = "jsonld" in s['id']
                is_code = "js_links" in s['id'] or "anchor" in s['id']
                is_hreflang = "hreflang" in s['id']
                is_cwv = any(k in s['id'] for k in ["lcp", "inp", "cls", "fcp", "risk"])
                is_img = "alt" in s['id'] or "favicon" in s['id']
                is_3xx = "3xx" in s['id'] 
                
                ev = s.get('example_evidence', '')
                ex_url = s['examples'][0] if s['examples'] else "example.com"
                # Cleaning URL for display
                if "Duplicate" in ex_url: ex_url = ex_url.split("\n")[1].replace("- ", "").strip()
                if "3xx" in s['id'] and s.get('args'): ev = s['args'][0]

                if is_code:
                    draw_code_preview(slide)
                elif is_hreflang:
                    type_str = s['id']
                    if "invalid" in type_str and s.get('args'):
                        type_str = f"invalid: {s['args'][0]}"
                    draw_hreflang_preview(slide, ex_url, type_str)
                elif is_rich:
                    draw_rich_snippet_preview(slide, ex_url)
                elif is_serp:
                    draw_serp_preview(slide, s['id'], t_data['title'], ev, ex_url)

    out = BytesIO()
    prs.save(out)
    out.seek(0)
    return out
