import streamlit as st
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. çˆ¬è™«æ ¸å¿ƒå¼•æ“ (çœŸå®é€»è¾‘) ---

def is_valid_url(url):
    """æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æ­£ç¡®"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def analyze_page(url, html_content, status_code):
    """åˆ†æå•ä¸ªé¡µé¢çš„SEOæŒ‡æ ‡ï¼Œè¿”å›æ•°æ®å’Œé—®é¢˜åˆ—è¡¨"""
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    
    # A. æ ‡é¢˜åˆ†æ
    title_tag = soup.title
    title = title_tag.string.strip() if title_tag and title_tag.string else None
    
    if not title:
        issues.append({
            "severity": "High",
            "title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title Tag)",
            "desc": "é¡µé¢æ²¡æœ‰ <title> æ ‡ç­¾ï¼Œæœç´¢å¼•æ“æ— æ³•ç†è§£é¡µé¢ä¸»é¢˜ã€‚",
            "suggestion": "åœ¨ <head> ä¸­æ·»åŠ æè¿°æ€§çš„æ ‡é¢˜ã€‚",
            "url": url
        })
    elif len(title) < 10:
         issues.append({
            "severity": "Medium",
            "title": "æ ‡é¢˜è¿‡çŸ­",
            "desc": f"æ ‡é¢˜ä»…æœ‰ {len(title)} ä¸ªå­—ç¬¦ï¼Œéš¾ä»¥è¦†ç›–æ ¸å¿ƒå…³é”®è¯ã€‚",
            "suggestion": "å»ºè®®å°†æ ‡é¢˜æ‰©å……è‡³ 30-60 ä¸ªå­—ç¬¦ã€‚",
            "url": url
        })

    # B. H1 åˆ†æ
    h1 = soup.find('h1')
    h1_text = h1.get_text().strip() if h1 else "No H1"
    
    if not h1:
        issues.append({
            "severity": "High",
            "title": "ç¼ºå¤± H1 æ ‡ç­¾",
            "desc": "é¡µé¢ç¼ºä¹ä¸»æ ‡é¢˜ (H1)ï¼Œå½±å“é¡µé¢å±‚çº§ç»“æ„ã€‚",
            "suggestion": "æ·»åŠ ä¸”ä»…æ·»åŠ ä¸€ä¸ªåŒ…å«æ ¸å¿ƒå…³é”®è¯çš„ H1 æ ‡ç­¾ã€‚",
            "url": url
        })

    # C. è½¯ 404 æ£€æµ‹ (ç®€å•çš„å…³é”®è¯åŒ¹é…)
    text_content = soup.get_text().lower()
    if status_code == 200 and ("page not found" in text_content or "404 error" in text_content):
        issues.append({
            "severity": "Critical",
            "title": "ç–‘ä¼¼è½¯ 404 (Soft 404)",
            "desc": "é¡µé¢è¿”å› 200 çŠ¶æ€ç ï¼Œä½†å†…å®¹æ˜¾ç¤º'æœªæ‰¾åˆ°'ã€‚",
            "suggestion": "é…ç½®æœåŠ¡å™¨ï¼Œå¯¹ä¸å­˜åœ¨çš„é¡µé¢è¿”å›çœŸæ­£çš„ 404 çŠ¶æ€ç ã€‚",
            "url": url
        })
        
    # D. å›¾ç‰‡ Alt å±æ€§æ£€æµ‹
    images = soup.find_all('img')
    missing_alt = 0
    for img in images:
        if not img.get('alt'):
            missing_alt += 1
    if missing_alt > 0:
        issues.append({
            "severity": "Medium",
            "title": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§", # ç»Ÿä¸€ Title ä»¥ä¾¿èšåˆ
            "desc": "å›¾ç‰‡ç¼ºå°‘æ›¿ä»£æ–‡æœ¬ï¼Œå½±å“å›¾ç‰‡æœç´¢æ’åå’Œæ— éšœç¢è®¿é—®ã€‚",
            "suggestion": "ä¸ºæ‰€æœ‰ img æ ‡ç­¾æ·»åŠ æè¿°æ€§çš„ alt å±æ€§ã€‚",
            "url": url,
            "meta": f"è¯¥é¡µé¢æœ‰ {missing_alt} å¼ å›¾ç‰‡ç¼ºå¤± Alt" # é¢å¤–ä¿¡æ¯
        })

    # E. æå–å†…éƒ¨é“¾æ¥ (ç”¨äºç»§ç»­çˆ¬å–)
    internal_links = set()
    base_domain = urlparse(url).netloc
    for a in soup.find_all('a', href=True):
        link = urljoin(url, a['href'])
        parsed_link = urlparse(link)
        # åªæ”¶é›†åŒä¸€åŸŸåä¸‹çš„é“¾æ¥ï¼Œé˜²æ­¢çˆ¬å‡ºå»
        if parsed_link.netloc == base_domain:
            # è¿‡æ»¤æ‰å›¾ç‰‡ã€PDFç­‰éHTMLé“¾æ¥
            if not any(link.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.pdf', '.css', '.js']):
                internal_links.add(link)

    return {
        "URL": url,
        "Status": status_code,
        "Title": title or "No Title",
        "H1": h1_text,
        "Links_Count": len(internal_links),
        "Issues_Count": len(issues)
    }, issues, internal_links

def crawl_website(start_url, max_pages=100):
    """æ‰§è¡Œå¹¿åº¦ä¼˜å…ˆçˆ¬å–"""
    visited = set()
    queue = [start_url]
    results_data = []
    all_issues = []
    
    # ç•Œé¢ä¸Šçš„è¿›åº¦æ¡
    progress_bar = st.progress(0, text="åˆå§‹åŒ–çˆ¬è™«å¼•æ“...")
    status_text = st.empty()
    
    pages_crawled = 0
    
    while queue and pages_crawled < max_pages:
        url = queue.pop(0)
        
        # å»é‡å¤„ç†
        if url in visited:
            continue
        visited.add(url)
        
        pages_crawled += 1
        
        # æ›´æ–°è¿›åº¦æ˜¾ç¤º
        progress = int((pages_crawled / max_pages) * 100)
        progress_bar.progress(progress, text=f"æ­£åœ¨çˆ¬å– ({pages_crawled}/{max_pages}): {url}")
        
        try:
            # æ¨¡æ‹Ÿæµè§ˆå™¨ User-Agent
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            
            # ç¡®ä¿æ˜¯ HTML é¡µé¢
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                page_data, page_issues, new_links = analyze_page(url, response.content, response.status_code)
                
                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                # å°†æ–°å‘ç°çš„é“¾æ¥åŠ å…¥é˜Ÿåˆ—
                for link in new_links:
                    if link not in visited and link not in queue:
                        queue.append(link)
            else:
                pass # å¿½ç•¥é HTML æ–‡ä»¶

        except Exception as e:
            # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­ç¨‹åº
            st.toast(f"æ— æ³•è®¿é—® {url}: {str(e)}")
    
    progress_bar.progress(100, text="åˆ†æå®Œæˆï¼æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
    time.sleep(0.5)
    progress_bar.empty()
    
    return results_data, all_issues

# --- 3. åˆå§‹åŒ– Session State (ç¼“å­˜æ•°æ®) ---
if 'audit_data' not in st.session_state:
    st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state:
    st.session_state['audit_issues'] = []

# --- 4. ä¾§è¾¹æ å¯¼èˆª ---
with st.sidebar:
    st.title("ğŸ” AuditAI Pro")
    st.caption("Live Crawler Edition")
    
    menu = st.radio(
        "åŠŸèƒ½å¯¼èˆª",
        ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"]
    )
    
    st.divider()
    if st.session_state['audit_data'] is not None:
        st.success(f"å·²ç¼“å­˜ {len(st.session_state['audit_data'])} ä¸ªé¡µé¢")
        if st.button("æ¸…é™¤æ•°æ®å¹¶é‡ç½®"):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.rerun()

# --- 5. ä¸»ç•Œé¢é€»è¾‘ ---

if menu == "è¾“å…¥ç½‘å€":
    st.header("å¼€å§‹æ–°çš„å®¡è®¡")
    st.info("è¯´æ˜: è¿™æ˜¯ä¸€ä¸ªçœŸå®çˆ¬è™«ã€‚è¾“å…¥ç½‘å€åï¼Œç³»ç»Ÿå°†å®æ—¶è®¿é—®è¯¥ç½‘ç«™å¹¶åˆ†æå‰ 100 ä¸ªé¡µé¢ã€‚")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        url_input = st.text_input("è¾“å…¥ç›®æ ‡ç½‘å€ (ä¾‹å¦‚ https://example.com)", placeholder="https://...")
    with col2:
        start_btn = st.button("å¼€å§‹çœŸå®çˆ¬å–", type="primary", use_container_width=True)
    
    if start_btn and url_input:
        if not is_valid_url(url_input):
            st.error("ç½‘å€æ ¼å¼é”™è¯¯ï¼Œè¯·ç¡®ä¿åŒ…å« http:// æˆ– https://")
        else:
            with st.spinner("æ­£åœ¨å¯åŠ¨çˆ¬è™«ï¼Œçˆ¬å– 100 ä¸ªé¡µé¢å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…..."):
                # è¿™é‡Œä¿®æ”¹ä¸º 100 é¡µ
                data, issues = crawl_website(url_input, max_pages=100)
                
                if not data:
                    st.error("æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦å¯è®¿é—®ï¼Œæˆ–ç½‘ç«™æ˜¯å¦æœ‰åçˆ¬è™«æœºåˆ¶ã€‚")
                else:
                    # å­˜å…¥ Session State
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(f"å®¡è®¡å®Œæˆï¼å…±åˆ†æ {len(data)} ä¸ªé¡µé¢ï¼Œå‘ç° {len(issues)} ä¸ªé—®é¢˜ã€‚")
                    st.balloons()

elif menu == "ä»ªè¡¨ç›˜":
    st.header("æ‰§è¡Œæ‘˜è¦ (Executive Summary)")
    
    if st.session_state['audit_data'] is None:
        st.warning("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆå‰å¾€'è¾“å…¥ç½‘å€'é¡µé¢è¿›è¡Œçˆ¬å–ã€‚")
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        
        # è®¡ç®—å¥åº·åº¦ (æ¨¡æ‹Ÿç®—æ³•)
        total_issues = len(issues)
        health_score = max(0, 100 - int(total_issues * 0.5)) # é™ä½æ‰£åˆ†æƒé‡å› ä¸ºé¡µé¢å¤šäº†
        
        kpi1, kpi2, kpi3, kpi4 = st.columns(4)
        kpi1.metric("ç½‘ç«™å¥åº·åº¦", f"{health_score}/100")
        kpi2.metric("å·²åˆ†æé¡µé¢", str(len(df)))
        kpi3.metric("å‘ç°é—®é¢˜æ€»æ•°", str(total_issues), delta_color="inverse")
        
        critical_count = len([i for i in issues if i['severity'] == 'Critical'])
        kpi4.metric("ä¸¥é‡é—®é¢˜ (Critical)", str(critical_count), delta_color="inverse")
        
        st.divider()
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("é—®é¢˜ç±»å‹åˆ†å¸ƒ")
            if issues:
                issue_types = pd.DataFrame(issues)['title'].value_counts()
                st.bar_chart(issue_types)
            else:
                st.info("æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚")
                
        with col2:
            st.subheader("HTTP çŠ¶æ€ç åˆ†å¸ƒ")
            if not df.empty:
                status_counts = df['Status'].value_counts()
                st.bar_chart(status_counts)

elif menu == "æ•°æ®çŸ©é˜µ":
    st.header("å…¨ç«™æ•°æ®æ˜ç»† (Big Sheet)")
    
    if st.session_state['audit_data'] is None:
        st.warning("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆçˆ¬å–ã€‚")
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        
        st.dataframe(
            df,
            column_config={
                "URL": st.column_config.LinkColumn("Page URL"),
                "Status": st.column_config.NumberColumn("Status Code", format="%d"),
            },
            use_container_width=True,
            hide_index=True
        )
        
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("ä¸‹è½½ CSV æŠ¥å‘Š", csv, "audit_report.csv", "text/csv")

elif menu == "PPT ç”Ÿæˆå™¨":
    st.header("æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)")
    
    if st.session_state['audit_data'] is None:
        st.warning("æš‚æ— æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆ PPTã€‚")
    elif not st.session_state['audit_issues']:
        st.success("æ­å–œï¼æœªå‘ç°ä¸¥é‡é—®é¢˜ï¼Œæ— éœ€ç”Ÿæˆä¿®å¤å»ºè®® PPTã€‚")
    else:
        # --- èšåˆé€»è¾‘å¼€å§‹ ---
        raw_issues = st.session_state['audit_issues']
        grouped_issues = {}
        
        for issue in raw_issues:
            title = issue['title']
            if title not in grouped_issues:
                # åˆå§‹åŒ–è¯¥ç±»é—®é¢˜
                grouped_issues[title] = {
                    "title": title,
                    "severity": issue['severity'],
                    "desc": issue['desc'],
                    "suggestion": issue['suggestion'],
                    "count": 0,
                    "examples": [], # å­˜å‚¨å—å½±å“çš„URL
                    "meta": issue.get('meta', '') # é¢å¤–ä¿¡æ¯
                }
            
            grouped_issues[title]['count'] += 1
            if len(grouped_issues[title]['examples']) < 3: # åªå­˜å‰3ä¸ªä¾‹å­
                grouped_issues[title]['examples'].append(issue['url'])
        
        # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œå¹¶æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº (Critical > High > Medium)
        severity_order = {"Critical": 0, "High": 1, "Medium": 2}
        ppt_slides = sorted(
            list(grouped_issues.values()), 
            key=lambda x: (severity_order.get(x['severity'], 3), -x['count'])
        )
        # --- èšåˆé€»è¾‘ç»“æŸ ---

        st.caption(f"ç³»ç»Ÿå·²è‡ªåŠ¨èšåˆç›¸åŒç±»å‹çš„é—®é¢˜ï¼Œå…±ç”Ÿæˆ {len(ppt_slides)} å¼ å…³é”®å¹»ç¯ç‰‡ã€‚")
        
        if 'slide_index' not in st.session_state:
            st.session_state.slide_index = 0
            
        # é˜²æ­¢ç´¢å¼•è¶Šç•Œ
        if st.session_state.slide_index >= len(ppt_slides):
            st.session_state.slide_index = 0
            
        slide = ppt_slides[st.session_state.slide_index]
        
        # æ¨¡æ‹Ÿ PPT æ¡†æ¶ (16:9)
        with st.container(border=True):
            # æ ‡é¢˜åŒºåŸŸå±•ç¤ºç»Ÿè®¡æ•°æ®
            st.markdown(f"### é—®é¢˜ç±»å‹: {slide['title']}")
            
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if slide['severity'] == "Critical" else "orange" if slide['severity'] == "High" else "blue"
                st.markdown(f"**ä¸¥é‡ç¨‹åº¦:** :{color}[{slide['severity']}]")
                st.markdown(f"**å½±å“èŒƒå›´:** å…¨ç«™å…±å‘ç° **{slide['count']}** ä¸ªé¡µé¢å­˜åœ¨æ­¤é—®é¢˜ã€‚")
                
                st.markdown("**é—®é¢˜æè¿°:**")
                st.write(slide['desc'])
                
                st.info(f"ğŸ’¡ **ä¿®å¤å»ºè®®:** {slide['suggestion']}")
            
            with c2:
                # å±•ç¤ºç¤ºä¾‹ URL åˆ—è¡¨
                st.markdown("**ğŸ” å—å½±å“é¡µé¢ç¤ºä¾‹:**")
                for ex_url in slide['examples']:
                    st.markdown(f"- `{ex_url}`")
                if slide['count'] > 3:
                    st.caption(f"...ä»¥åŠå…¶ä»– {slide['count'] - 3} ä¸ªé¡µé¢ã€‚")
                    
                st.markdown("---")
                # æˆªå›¾å ä½ç¬¦
                st.image("https://placehold.co/600x300/EEE/31343C?text=Screenshot+Example", caption="è¯·æˆªå–ä¸Šè¿°ç¤ºä¾‹é¡µé¢ä¹‹ä¸€ä½œä¸ºè¯æ®")

        # ç¿»é¡µæŒ‰é’®
        col_prev, col_info, col_next = st.columns([1, 2, 1])
        with col_prev:
            if st.button("â¬…ï¸ ä¸Šä¸€é¡µ"):
                if st.session_state.slide_index > 0:
                    st.session_state.slide_index -= 1
                    st.rerun()
        with col_info:
            st.markdown(f"<div style='text-align: center'>Slide {st.session_state.slide_index + 1} / {len(ppt_slides)}</div>", unsafe_allow_html=True)
        with col_next:
            if st.button("ä¸‹ä¸€é¡µ â¡ï¸"):
                if st.session_state.slide_index < len(ppt_slides) - 1:
                    st.session_state.slide_index += 1
                    st.rerun()
