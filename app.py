import streamlit as st
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. çˆ¬è™«æ ¸å¿ƒå¼•æ“ (çœŸå®é€»è¾‘) ---

def is_valid_url(url):
    """æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æ­£ç¡®"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def check_site_level_assets(start_url):
    """æ£€æŸ¥ç«™ç‚¹çº§åˆ«çš„ SEO èµ„äº§ (Robots.txt, Sitemap)"""
    issues = []
    parsed_url = urlparse(start_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    
    # 1. Robots.txt æ£€æŸ¥
    robots_url = urljoin(base_url, "/robots.txt")
    try:
        r = requests.head(robots_url, timeout=5)
        if r.status_code != 200:
            issues.append({
                "severity": "Medium",
                "title": "ç¼ºå¤± Robots.txt",
                "desc": "æ— æ³•åœ¨æ ¹ç›®å½•æ‰¾åˆ° robots.txt æ–‡ä»¶ï¼Œå¯èƒ½å¯¼è‡´çˆ¬å–æ§åˆ¶æ··ä¹±ã€‚",
                "suggestion": "åœ¨ç½‘ç«™æ ¹ç›®å½•åˆ›å»º robots.txt æ–‡ä»¶ä»¥æŒ‡å¯¼çˆ¬è™«ã€‚",
                "url": robots_url,
                "meta": f"Status: {r.status_code}"
            })
    except:
        pass # ç½‘ç»œé”™è¯¯å¿½ç•¥

    # 2. Sitemap.xml æ£€æŸ¥ (ç®€å•æ£€æŸ¥æ ¹ç›®å½•)
    sitemap_url = urljoin(base_url, "/sitemap.xml")
    try:
        r = requests.head(sitemap_url, timeout=5)
        if r.status_code != 200:
             # æœ‰äº›ç½‘ç«™Sitemapä¸åœ¨æ ¹ç›®å½•ï¼Œè¿™é‡Œç»™ä¸ªä½ä¼˜å…ˆçº§çš„æç¤º
            issues.append({
                "severity": "Low",
                "title": "æ ¹ç›®å½•æœªå‘ç° Sitemap.xml",
                "desc": "æ ¹ç›®å½•æ—  sitemap.xmlã€‚å¦‚æœæ‚¨çš„ Sitemap ä½äºå…¶ä»–ä½ç½®ï¼Œè¯·ç¡®ä¿åœ¨ robots.txt ä¸­å£°æ˜ã€‚",
                "suggestion": "ç¡®ä¿ Sitemap å¯è®¿é—®å¹¶åœ¨ robots.txt ä¸­å¼•ç”¨ã€‚",
                "url": sitemap_url,
                "meta": f"Status: {r.status_code}"
            })
    except:
        pass

    return issues

def analyze_page(url, html_content, status_code):
    """åˆ†æå•ä¸ªé¡µé¢çš„SEOæŒ‡æ ‡ï¼Œè¿”å›æ•°æ®å’Œé—®é¢˜åˆ—è¡¨"""
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    
    # --- A. åŸºç¡€å†…å®¹æ£€æŸ¥ ---
    
    # 1. æ ‡é¢˜ (Title)
    title_tag = soup.title
    title = title_tag.string.strip() if title_tag and title_tag.string else None
    
    if not title:
        issues.append({
            "severity": "High",
            "title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title Tag)",
            "desc": "é¡µé¢æ²¡æœ‰ <title> æ ‡ç­¾ï¼Œæœç´¢å¼•æ“æ— æ³•ç†è§£é¡µé¢ä¸»é¢˜ã€‚",
            "suggestion": "åœ¨ <head> ä¸­æ·»åŠ æè¿°æ€§çš„æ ‡é¢˜ã€‚",
            "url": url
        })
    elif len(title) < 10:
         issues.append({
            "severity": "Medium",
            "title": "æ ‡é¢˜è¿‡çŸ­",
            "desc": f"æ ‡é¢˜ä»…æœ‰ {len(title)} ä¸ªå­—ç¬¦ï¼Œéš¾ä»¥è¦†ç›–æ ¸å¿ƒå…³é”®è¯ã€‚",
            "suggestion": "å»ºè®®å°†æ ‡é¢˜æ‰©å……è‡³ 30-60 ä¸ªå­—ç¬¦ã€‚",
            "url": url
        })
    elif len(title) > 60:
         issues.append({
            "severity": "Low",
            "title": "æ ‡é¢˜è¿‡é•¿",
            "desc": f"æ ‡é¢˜é•¿è¾¾ {len(title)} å­—ç¬¦ï¼Œåœ¨æœç´¢ç»“æœä¸­å¯èƒ½ä¼šè¢«æˆªæ–­ã€‚",
            "suggestion": "å»ºè®®å°†æ ‡é¢˜æ§åˆ¶åœ¨ 60 ä¸ªå­—ç¬¦ä»¥å†…ã€‚",
            "url": url
        })

    # 2. å…ƒæè¿° (Meta Description)
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = meta_desc['content'].strip() if meta_desc and meta_desc.get('content') else None
    
    if not desc_content:
        issues.append({
            "severity": "High",
            "title": "ç¼ºå¤±å…ƒæè¿° (Meta Description)",
            "desc": "ç¼ºå¤±å…ƒæè¿°ä¼šé™ä½æœç´¢ç»“æœçš„ç‚¹å‡»ç‡ (CTR)ã€‚",
            "suggestion": "æ·»åŠ  <meta name='description'> æ ‡ç­¾ï¼Œæ¦‚æ‹¬é¡µé¢å†…å®¹ã€‚",
            "url": url
        })
    elif len(desc_content) < 50:
        issues.append({
            "severity": "Low",
            "title": "å…ƒæè¿°è¿‡çŸ­",
            "desc": "æè¿°å†…å®¹å¤ªå°‘ï¼Œæ— æ³•æœ‰æ•ˆå¸å¼•ç”¨æˆ·ç‚¹å‡»ã€‚",
            "suggestion": "å»ºè®®æ‰©å……è‡³ 120-160 ä¸ªå­—ç¬¦ã€‚",
            "url": url
        })

    # 3. H1 æ ‡ç­¾
    h1 = soup.find('h1')
    h1_text = h1.get_text().strip() if h1 else "No H1"
    if not h1:
        issues.append({
            "severity": "High",
            "title": "ç¼ºå¤± H1 æ ‡ç­¾",
            "desc": "é¡µé¢ç¼ºä¹ä¸»æ ‡é¢˜ (H1)ï¼Œå½±å“é¡µé¢å±‚çº§ç»“æ„ã€‚",
            "suggestion": "æ·»åŠ ä¸”ä»…æ·»åŠ ä¸€ä¸ªåŒ…å«æ ¸å¿ƒå…³é”®è¯çš„ H1 æ ‡ç­¾ã€‚",
            "url": url
        })

    # --- B. æŠ€æœ¯ä¸ä»£ç æ£€æŸ¥ ---

    # 4. ç§»åŠ¨ç«¯è§†å£ (Mobile Viewport)
    viewport = soup.find('meta', attrs={'name': 'viewport'})
    if not viewport:
        issues.append({
            "severity": "Critical",
            "title": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®",
            "desc": "é¡µé¢æœªé…ç½® Viewport Meta æ ‡ç­¾ï¼ŒGoogle å¯èƒ½ä¸ä¼šå°†å…¶è§†ä¸ºç§»åŠ¨å‹å¥½é¡µé¢ã€‚",
            "suggestion": "æ·»åŠ  <meta name='viewport' content='width=device-width, initial-scale=1'>ã€‚",
            "url": url
        })

    # 5. è§„èŒƒæ ‡ç­¾ (Canonical)
    canonical = soup.find('link', attrs={'rel': 'canonical'})
    if not canonical:
        issues.append({
            "severity": "Medium",
            "title": "ç¼ºå¤±è§„èŒƒæ ‡ç­¾ (Canonical)",
            "desc": "æœªæŒ‡å®šè§„èŒƒé“¾æ¥ï¼Œå¯èƒ½å¯¼è‡´å‚æ•°ä¸åŒçš„åŒä¸€é¡µé¢è¢«è§†ä¸ºé‡å¤å†…å®¹ã€‚",
            "suggestion": "æ·»åŠ  <link rel='canonical' href='...' /> æŒ‡å‘å½“å‰é¡µé¢çš„æ ‡å‡† URLã€‚",
            "url": url
        })

    # 6. Favicon (ç½‘ç«™å›¾æ ‡)
    # æ£€æŸ¥ link rel="icon" æˆ– "shortcut icon"
    favicon = soup.find('link', rel=lambda x: x and 'icon' in x.lower())
    if not favicon:
         issues.append({
            "severity": "Low",
            "title": "ç¼ºå¤± Favicon (ç½‘ç«™å›¾æ ‡)",
            "desc": "æœªæ£€æµ‹åˆ° Favicon è®¾ç½®ã€‚è¿™ä¼šå½±å“åœ¨æœç´¢ç»“æœé¡µ(SERP)ä¸­çš„å“ç‰Œå±•ç¤ºã€‚",
            "suggestion": "åœ¨ <head> ä¸­æ·»åŠ  <link rel='icon' href='...'>ã€‚",
            "url": url
        })

    # 7. ç»“æ„åŒ–æ•°æ® (Structured Data / Schema)
    # Google æ¨èä½¿ç”¨ JSON-LD
    schema = soup.find('script', type='application/ld+json')
    if not schema:
         issues.append({
            "severity": "Medium",
            "title": "æœªæ£€æµ‹åˆ°ç»“æ„åŒ–æ•°æ® (JSON-LD)",
            "desc": "ç»“æ„åŒ–æ•°æ®æœ‰åŠ©äº Google ç†è§£å†…å®¹å¹¶ç”Ÿæˆå¯Œåª’ä½“æœç´¢ç»“æœ (Rich Snippets)ã€‚",
            "suggestion": "æ·»åŠ é€‚åˆé¡µé¢çš„ JSON-LD ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ Organization, Article, Productï¼‰ã€‚",
            "url": url
        })

    # 8. Hreflang (å¤šè¯­è¨€æ”¯æŒ)
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ hreflang æ ‡ç­¾
    hreflang = soup.find('link', hreflang=True)
    if not hreflang:
        # è¿™é‡Œç»™ä¸€ä¸ªä½ä¼˜å…ˆçº§çš„æç¤ºï¼Œå› ä¸ºå¹¶éæ‰€æœ‰ç½‘ç«™éƒ½éœ€è¦å¤šè¯­è¨€
        issues.append({
            "severity": "Low",
            "title": "æœªå‘ç° Hreflang æ ‡è®°",
            "desc": "å¦‚æœæ‚¨é’ˆå¯¹ä¸åŒåœ°åŒº/è¯­è¨€çš„ç”¨æˆ·æä¾›å†…å®¹ï¼Œç¼ºå¤± hreflang ä¼šå¯¼è‡´ç´¢å¼•æ··ä¹±ã€‚",
            "suggestion": "å¦‚æœç½‘ç«™æ˜¯å¤šè¯­è¨€çš„ï¼Œè¯·æ·»åŠ  <link rel='alternate' hreflang='...' />ã€‚",
            "url": url
        })

    # --- C. URL ç»“æ„ä¸çˆ¬å–æ•ˆç‡ ---

    # 9. URL ç»“æ„æ£€æŸ¥
    parsed_url = urlparse(url)
    path = parsed_url.path
    
    if '_' in path:
         issues.append({
            "severity": "Low",
            "title": "URL åŒ…å«ä¸‹åˆ’çº¿",
            "desc": "Google å»ºè®®åœ¨ URL ä¸­ä½¿ç”¨è¿å­—ç¬¦ (-) è€Œéä¸‹åˆ’çº¿ (_) åˆ†éš”å•è¯ã€‚",
            "suggestion": "ä¼˜åŒ– URL ç»“æ„ï¼Œä½¿ç”¨è¿å­—ç¬¦ä»£æ›¿ä¸‹åˆ’çº¿ã€‚",
            "url": url
        })
    
    if any(c.isupper() for c in path):
         issues.append({
            "severity": "Medium",
            "title": "URL åŒ…å«å¤§å†™å­—æ¯",
            "desc": "URL æ˜¯åŒºåˆ†å¤§å°å†™çš„ã€‚æ··åˆå¤§å°å†™å®¹æ˜“å¯¼è‡´é‡å¤å†…å®¹é—®é¢˜å’Œå¤–éƒ¨é“¾æ¥é”™è¯¯ã€‚",
            "suggestion": "ç»Ÿä¸€ä½¿ç”¨å…¨å°å†™å­—æ¯çš„ URLã€‚",
            "url": url
        })
    
    if len(url) > 100:
         issues.append({
            "severity": "Low",
            "title": "URL è¿‡é•¿",
            "desc": "è¿‡é•¿çš„ URL ä¸åˆ©äºç”¨æˆ·é˜…è¯»å’Œåˆ†äº«ï¼Œä¹Ÿå¯èƒ½è¢«æˆªæ–­ã€‚",
            "suggestion": "ä¿æŒ URL ç®€çŸ­ä¸”å…·æœ‰æè¿°æ€§ã€‚",
            "url": url
        })

    # 10. JavaScript é“¾æ¥é™·é˜±
    # æ£€æŸ¥ href="javascript:..."
    js_links = soup.find_all('a', href=lambda x: x and x.lower().startswith('javascript:'))
    if js_links:
        issues.append({
            "severity": "High",
            "title": "å‘ç° JavaScript ä¼ªé“¾æ¥",
            "desc": f"å‘ç° {len(js_links)} ä¸ªé“¾æ¥ä½¿ç”¨ href='javascript:'ï¼Œçˆ¬è™«æ— æ³•è·Ÿè¸ªæ­¤ç±»é“¾æ¥ã€‚",
            "suggestion": "ä½¿ç”¨æ ‡å‡†çš„ <a href='URL'> æ ‡ç­¾ï¼Œä»…åœ¨ onclick äº‹ä»¶ä¸­ä½¿ç”¨ JSã€‚",
            "url": url,
            "meta": f"Count: {len(js_links)}"
        })

    # --- D. å†…å®¹è´¨é‡ ---

    # 11. è½¯ 404 æ£€æµ‹
    text_content = soup.get_text().lower()
    if status_code == 200 and ("page not found" in text_content or "404 error" in text_content):
        issues.append({
            "severity": "Critical",
            "title": "ç–‘ä¼¼è½¯ 404 (Soft 404)",
            "desc": "é¡µé¢è¿”å› 200 çŠ¶æ€ç ï¼Œä½†å†…å®¹æ˜¾ç¤º'æœªæ‰¾åˆ°'ã€‚",
            "suggestion": "é…ç½®æœåŠ¡å™¨ï¼Œå¯¹ä¸å­˜åœ¨çš„é¡µé¢è¿”å›çœŸæ­£çš„ 404 çŠ¶æ€ç ã€‚",
            "url": url
        })
        
    # 12. å›¾ç‰‡ Alt å±æ€§
    images = soup.find_all('img')
    missing_alt = 0
    for img in images:
        if not img.get('alt'):
            missing_alt += 1
    if missing_alt > 0:
        issues.append({
            "severity": "Medium",
            "title": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§",
            "desc": "å›¾ç‰‡ç¼ºå°‘æ›¿ä»£æ–‡æœ¬ï¼Œå½±å“å›¾ç‰‡æœç´¢æ’åå’Œæ— éšœç¢è®¿é—®ã€‚",
            "suggestion": "ä¸ºæ‰€æœ‰ img æ ‡ç­¾æ·»åŠ æè¿°æ€§çš„ alt å±æ€§ã€‚",
            "url": url,
            "meta": f"è¯¥é¡µé¢æœ‰ {missing_alt} å¼ å›¾ç‰‡ç¼ºå¤± Alt"
        })

    # E. æå–å†…éƒ¨é“¾æ¥
    internal_links = set()
    base_domain = urlparse(url).netloc
    for a in soup.find_all('a', href=True):
        link = urljoin(url, a['href'])
        parsed_link = urlparse(link)
        if parsed_link.netloc == base_domain:
            # è¿‡æ»¤éHTML
            if not any(link.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.pdf', '.css', '.js', '.zip']):
                internal_links.add(link)

    return {
        "URL": url,
        "Status": status_code,
        "Title": title or "No Title",
        "H1": h1_text,
        "Links_Count": len(internal_links),
        "Issues_Count": len(issues)
    }, issues, internal_links

def crawl_website(start_url, max_pages=100):
    """æ‰§è¡Œå¹¿åº¦ä¼˜å…ˆçˆ¬å–"""
    visited = set()
    queue = [start_url]
    results_data = []
    all_issues = []
    
    progress_bar = st.progress(0, text="åˆå§‹åŒ–çˆ¬è™«å¼•æ“...")
    
    # 0. ç«™ç‚¹çº§æ£€æŸ¥ (æ‰§è¡Œä¸€æ¬¡)
    try:
        site_issues = check_site_level_assets(start_url)
        all_issues.extend(site_issues)
    except Exception as e:
        st.toast(f"ç«™ç‚¹çº§æ£€æŸ¥å¤±è´¥: {str(e)}")

    pages_crawled = 0
    
    while queue and pages_crawled < max_pages:
        url = queue.pop(0)
        
        if url in visited:
            continue
        visited.add(url)
        pages_crawled += 1
        
        progress = int((pages_crawled / max_pages) * 100)
        progress_bar.progress(progress, text=f"æ­£åœ¨çˆ¬å– ({pages_crawled}/{max_pages}): {url}")
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; SEOAuditBot/1.0)'}
            response = requests.get(url, headers=headers, timeout=10)
            
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                page_data, page_issues, new_links = analyze_page(url, response.content, response.status_code)
                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                for link in new_links:
                    if link not in visited and link not in queue:
                        queue.append(link)
        except Exception as e:
            pass # å¿½ç•¥å•ä¸ªé¡µé¢é”™è¯¯
    
    progress_bar.progress(100, text="åˆ†æå®Œæˆï¼")
    time.sleep(0.5)
    progress_bar.empty()
    
    return results_data, all_issues

# --- 3. åˆå§‹åŒ– Session State ---
if 'audit_data' not in st.session_state:
    st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state:
    st.session_state['audit_issues'] = []

# --- 4. ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ” AuditAI Pro")
    st.caption("Live Crawler Edition v2.0")
    
    menu = st.radio(
        "åŠŸèƒ½å¯¼èˆª",
        ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"]
    )
    
    st.divider()
    if st.session_state['audit_data'] is not None:
        st.success(f"å·²ç¼“å­˜ {len(st.session_state['audit_data'])} ä¸ªé¡µé¢")
        if st.button("æ¸…é™¤æ•°æ®å¹¶é‡ç½®"):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.rerun()

# --- 5. ä¸»ç•Œé¢é€»è¾‘ ---

if menu == "è¾“å…¥ç½‘å€":
    st.header("å¼€å§‹æ–°çš„å®¡è®¡")
    st.info("è¯´æ˜: å‡çº§ç‰ˆçˆ¬è™«ï¼Œæ”¯æŒ Robots.txtã€Sitemapã€ç»“æ„åŒ–æ•°æ®åŠ URL è§„èŒƒæ£€æŸ¥ã€‚")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        url_input = st.text_input("è¾“å…¥ç›®æ ‡ç½‘å€", placeholder="https://example.com")
    with col2:
        start_btn = st.button("å¼€å§‹çœŸå®çˆ¬å–", type="primary", use_container_width=True)
    
    if start_btn and url_input:
        if not is_valid_url(url_input):
            st.error("ç½‘å€æ ¼å¼é”™è¯¯")
        else:
            with st.spinner("æ­£åœ¨å¯åŠ¨çˆ¬è™« (Max 100 pages)..."):
                data, issues = crawl_website(url_input, max_pages=100)
                if not data:
                    st.error("æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚")
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(f"å®¡è®¡å®Œæˆï¼å…±åˆ†æ {len(data)} ä¸ªé¡µé¢ã€‚")
                    st.balloons()

elif menu == "ä»ªè¡¨ç›˜":
    st.header("æ‰§è¡Œæ‘˜è¦ (Executive Summary)")
    if st.session_state['audit_data'] is None:
        st.warning("æš‚æ— æ•°æ®ã€‚")
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        
        total_issues = len(issues)
        health_score = max(0, 100 - int(total_issues * 0.5))
        
        kpi1, kpi2, kpi3, kpi4 = st.columns(4)
        kpi1.metric("ç½‘ç«™å¥åº·åº¦", f"{health_score}/100")
        kpi2.metric("å·²åˆ†æé¡µé¢", str(len(df)))
        kpi3.metric("å‘ç°é—®é¢˜æ€»æ•°", str(total_issues), delta_color="inverse")
        critical_count = len([i for i in issues if i['severity'] == 'Critical'])
        kpi4.metric("ä¸¥é‡é—®é¢˜", str(critical_count), delta_color="inverse")
        
        st.divider()
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("é—®é¢˜ç±»å‹åˆ†å¸ƒ")
            if issues:
                issue_types = pd.DataFrame(issues)['title'].value_counts()
                st.bar_chart(issue_types)
            else:
                st.info("æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚")
        with col2:
            st.subheader("HTTP çŠ¶æ€ç åˆ†å¸ƒ")
            if not df.empty:
                status_counts = df['Status'].value_counts()
                st.bar_chart(status_counts)

elif menu == "æ•°æ®çŸ©é˜µ":
    st.header("å…¨ç«™æ•°æ®æ˜ç»† (Big Sheet)")
    if st.session_state['audit_data'] is None:
        st.warning("æš‚æ— æ•°æ®ã€‚")
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(
            df,
            column_config={
                "URL": st.column_config.LinkColumn("Page URL"),
                "Status": st.column_config.NumberColumn("Status Code", format="%d"),
            },
            use_container_width=True,
            hide_index=True
        )
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("ä¸‹è½½ CSV æŠ¥å‘Š", csv, "audit_report.csv", "text/csv")

elif menu == "PPT ç”Ÿæˆå™¨":
    st.header("æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)")
    if st.session_state['audit_data'] is None:
        st.warning("æš‚æ— æ•°æ®ã€‚")
    elif not st.session_state['audit_issues']:
        st.success("æ— ä¸¥é‡é—®é¢˜ã€‚")
    else:
        # èšåˆé€»è¾‘
        raw_issues = st.session_state['audit_issues']
        grouped_issues = {}
        for issue in raw_issues:
            title = issue['title']
            if title not in grouped_issues:
                grouped_issues[title] = {
                    "title": title, "severity": issue['severity'],
                    "desc": issue['desc'], "suggestion": issue['suggestion'],
                    "count": 0, "examples": [], "meta": issue.get('meta', '')
                }
            grouped_issues[title]['count'] += 1
            if len(grouped_issues[title]['examples']) < 3:
                grouped_issues[title]['examples'].append(issue['url'])
        
        severity_order = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}
        ppt_slides = sorted(list(grouped_issues.values()), key=lambda x: (severity_order.get(x['severity'], 3), -x['count']))

        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(ppt_slides): st.session_state.slide_index = 0
            
        slide = ppt_slides[st.session_state.slide_index]
        
        with st.container(border=True):
            st.markdown(f"### é—®é¢˜ç±»å‹: {slide['title']}")
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if slide['severity'] == "Critical" else "orange" if slide['severity'] == "High" else "blue"
                st.markdown(f"**ä¸¥é‡ç¨‹åº¦:** :{color}[{slide['severity']}]")
                st.markdown(f"**å½±å“èŒƒå›´:** å…¨ç«™å…± **{slide['count']}** ä¸ªé¡µé¢ã€‚")
                st.markdown(f"**æè¿°:** {slide['desc']}")
                st.info(f"ğŸ’¡ **å»ºè®®:** {slide['suggestion']}")
            with c2:
                st.markdown("**ğŸ” ç¤ºä¾‹:**")
                for ex in slide['examples']: st.markdown(f"- `{ex}`")
                st.image("https://placehold.co/600x300/EEE/31343C?text=Screenshot+Evidence", caption="ç¤ºä¾‹æˆªå›¾")

        c_prev, c_txt, c_next = st.columns([1, 2, 1])
        with c_prev:
            if st.button("â¬…ï¸ ä¸Šä¸€é¡µ"):
                st.session_state.slide_index = max(0, st.session_state.slide_index - 1)
                st.rerun()
        with c_txt:
            st.markdown(f"<div style='text-align: center'>Slide {st.session_state.slide_index + 1} / {len(ppt_slides)}</div>", unsafe_allow_html=True)
        with c_next:
            if st.button("ä¸‹ä¸€é¡µ â¡ï¸"):
                st.session_state.slide_index = min(len(ppt_slides) - 1, st.session_state.slide_index + 1)
                st.rerun()
