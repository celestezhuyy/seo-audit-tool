import streamlit as st
import time
import pandas as pd
import requests
import hashlib
import re
import urllib3
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
    from pptx.enum.shapes import MSO_SHAPE
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. å›½é™…åŒ–å­—å…¸ (i18n) ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "æ·±åº¦å®¡è®¡ç‰ˆ v3.3.2",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        "sitemap_status_title": "Sitemap çŠ¶æ€:",
        "sitemap_found_href": "âœ… å‘ç° Hreflang é…ç½®", # Fixed: Added missing key
        "sitemap_no_href": "âš ï¸ æœªå‘ç° Hreflang",     # Fixed: Added missing key
        
        # PSI ç›¸å…³
        "psi_settings": "Google PSI API è®¾ç½® (å¯é€‰)",
        "psi_api_key_label": "è¾“å…¥ Google PageSpeed API Key",
        "psi_api_help": "ç•™ç©ºåˆ™ä»…è¿›è¡Œé™æ€ä»£ç æ£€æŸ¥ã€‚å¡«å…¥ Key å¯è·å–é¦–é¡µçš„çœŸå®ç”¨æˆ·ä½“éªŒæ•°æ® (LCP, CLS, INP)ã€‚",
        "psi_get_key": "æ²¡æœ‰ API Key? [ç‚¹å‡»è¿™é‡Œå…è´¹ç”³è¯·](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "æ­£åœ¨è°ƒç”¨ Google API è·å–çœŸå® CWV æ•°æ®...",
        "psi_success": "æˆåŠŸè·å–çœŸå®ç”¨æˆ·æ•°æ®ï¼",
        "psi_error": "API è°ƒç”¨å¤±è´¥æˆ–æ—  CrUX æ•°æ®",
        
        "input_header": "å¼€å§‹æ·±åº¦å®¡è®¡",
        "input_info": "è¯´æ˜: å¢å¼ºåçˆ¬è™«ç»•è¿‡èƒ½åŠ›ï¼Œé›†æˆè¯¦ç»†é”™è¯¯è¯Šæ–­ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°",
        "adv_settings": "é«˜çº§è®¾ç½® (Advanced Settings)", 
        "manual_robots": "æ‰‹åŠ¨ Robots.txt åœ°å€ (å¯é€‰)", 
        "manual_sitemaps": "æ‰‹åŠ¨ Sitemap åœ°å€ (æ¯è¡Œä¸€ä¸ª, å¯é€‰)", 
        "start_btn": "å¼€å§‹æ·±åº¦çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨æ‰§è¡Œæ·±åº¦å®¡è®¡ (Max {} pages)...", 
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚åŸå› : {}", 
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP çŠ¶æ€ç åˆ†å¸ƒ",
        "cwv_title": "æ ¸å¿ƒ Web æŒ‡æ ‡ (Core Web Vitals) - çœŸå®æ•°æ®",
        "cwv_source": "æ•°æ®æ¥æº: Google Chrome User Experience Report (CrUX)",
        
        "matrix_header": "çˆ¬å–æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼ŒåŒ…å« SERP é¢„è§ˆå’Œæ·±åº¦ä»£ç åˆ†æã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "åœ¨å·²çˆ¬å–æ ·æœ¬ä¸­å‘ç° **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "æè¿°:",
        "ppt_sugg": "ğŸ’¡ å»ºè®®:",
        "ppt_examples": "ğŸ” ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        
        "ppt_cover_title": "SEO æ·±åº¦æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro v3.3",
        "ppt_slide_desc_title": "é—®é¢˜æè¿° & å½±å“",
        "ppt_slide_count_title": "æ ·æœ¬ä¸­å—å½±å“é¡µé¢æ•°: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹ & è¯æ®",
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
        "serp_sim_title": "Google æœç´¢ç»“æœæ¨¡æ‹Ÿ:",
    },
    "en": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "Deep Audit Edition v3.3.2",
        "nav_label": "Navigation",
        "nav_options": ["Input URL", "Dashboard", "Data Matrix", "PPT Generator"],
        "lang_label": "Language / è¯­è¨€",
        "clear_data": "Clear Data & Reset",
        "cache_info": "Cached {} pages",
        "sitemap_status_title": "Sitemap Status:",
        "sitemap_found_href": "âœ… Hreflang Found", # Fixed: Added missing key
        "sitemap_no_href": "âš ï¸ No Hreflang",       # Fixed: Added missing key
        
        # PSI Related
        "psi_settings": "Google PSI API Settings (Optional)",
        "psi_api_key_label": "Enter Google PageSpeed API Key",
        "psi_api_help": "Leave empty for static check only. Enter Key to fetch Real User Metrics (LCP, CLS, INP) for the home page.",
        "psi_get_key": "No API Key? [Get one for free here](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "Fetching real CWV data from Google API...",
        "psi_success": "Real user data fetched!",
        "psi_error": "API Failed or No CrUX Data",
        
        "input_header": "Start Deep Audit",
        "input_info": "Note: Improved WAF bypass & Detailed error diagnostics.",
        "input_label": "Target URL",
        "input_placeholder": "https://example.com",
        "max_pages_label": "Max Pages to Crawl",
        "adv_settings": "Advanced Settings", 
        "manual_robots": "Manual Robots.txt URL (Optional)", 
        "manual_sitemaps": "Manual Sitemap URLs (One per line, Optional)", 
        "start_btn": "Start Deep Crawl",
        "error_url": "Invalid URL format",
        "spinner_crawl": "Running Deep Audit (Max {} pages)...", 
        "error_no_data": "No pages crawled. Reason: {}", 
        "success_audit": "Audit Complete! Analyzed {} pages.",
        
        "dashboard_header": "Executive Summary",
        "warn_no_data": "No data available.",
        "kpi_health": "Health Score",
        "kpi_pages": "Analyzed Pages",
        "kpi_issues": "Total Issues",
        "kpi_critical": "Critical Issues",
        "chart_issues": "Issue Distribution",
        "chart_no_issues": "No significant issues found.",
        "chart_status": "HTTP Status Codes",
        "cwv_title": "Core Web Vitals - Real User Data",
        "cwv_source": "Source: Google Chrome User Experience Report (CrUX)",
        
        "matrix_header": "Crawled Data Matrix",
        "download_csv": "Download CSV Report",
        
        "ppt_header": "Pitch Deck Preview",
        "ppt_success_no_issues": "No critical issues found.",
        "ppt_download_header": "ğŸ“¥ Export Report",
        "ppt_info": "Note: PPT optimized for 16:9 with SERP simulation and deep code analysis.",
        "ppt_btn": "Generate & Download .pptx",
        "ppt_preview_header": "Web Preview",
        "ppt_slide_title": "Issue Type:",
        "ppt_severity": "Severity:",
        "ppt_impact": "Impact:",
        "ppt_impact_desc": "Affects **{}** pages in crawled sample.",
        "ppt_desc": "Description:",
        "ppt_sugg": "ğŸ’¡ Suggestion:",
        "ppt_examples": "ğŸ” Examples:",
        "ppt_prev": "â¬…ï¸ Previous",
        "ppt_next": "Next â¡ï¸",
        
        "ppt_cover_title": "SEO Technical Audit",
        "ppt_cover_sub": "Generated by AuditAI Pro v3.3",
        "ppt_slide_desc_title": "Description & Impact",
        "ppt_slide_count_title": "Affected Pages (in sample): {}",
        "ppt_slide_ex_title": "Example URLs & Evidence",
        "ppt_slide_sugg_title": "ğŸ’¡ Recommendation:",
        "serp_sim_title": "Google SERP Simulation:",
    }
}

# --- 3. çˆ¬è™«æ ¸å¿ƒå¼•æ“ (æ”¯æŒå¤šè¯­è¨€) ---

def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def get_browser_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.google.com/'
    }

# --- Google PSI API Integration ---
def fetch_psi_data(url, api_key):
    """Call Google PageSpeed Insights API"""
    if not api_key: return None
    
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            # Extract Crux Data (loadingExperience)
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available for this URL."}
            
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else:
            return {"error": f"API Error: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

def check_site_level_assets(start_url, lang="zh", manual_robots=None, manual_sitemaps=None):
    issues = []
    sitemap_has_hreflang = False
    
    parsed_url = urlparse(start_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    headers = get_browser_headers()
    
    # è¯­è¨€åŒ…
    txt = {
        "zh": {
            "no_robots": "ç¼ºå¤± Robots.txt",
            "robots_bad_rule": "Robots.txt è§„åˆ™é£é™©",
            "robots_no_sitemap": "Robots.txt æœªå£°æ˜ Sitemap",
            "no_sitemap": "Sitemap è®¿é—®å¤±è´¥",
            "sitemap_invalid": "Sitemap æ ¼å¼é”™è¯¯",
            "no_favicon": "ç«™ç‚¹ç¼ºå¤± Favicon",
        },
        "en": {
            "no_robots": "Missing Robots.txt",
            "robots_bad_rule": "Robots.txt Blocking Risk",
            "robots_no_sitemap": "Sitemap not in Robots.txt",
            "no_sitemap": "Sitemap Access Failed",
            "sitemap_invalid": "Invalid Sitemap Format",
            "no_favicon": "Site Missing Favicon",
        }
    }
    t = txt[lang]

    # --- 1. Robots.txt é€»è¾‘æ£€æŸ¥ ---
    robots_url = manual_robots if manual_robots else urljoin(base_url, "/robots.txt")
    try:
        # ä½¿ç”¨ verify=False è·³è¿‡ SSL éªŒè¯
        r = requests.get(robots_url, headers=headers, timeout=10, allow_redirects=True, stream=True, verify=False)
        if r.status_code != 200:
            issues.append({"severity": "Medium", "title": t["no_robots"], "desc": f"Status: {r.status_code}", "suggestion": "Ensure robots.txt exists.", "url": robots_url})
        else:
            # æ£€æŸ¥ Robots å†…å®¹
            content = r.text.lower()
            if "disallow: /" in content and "allow:" not in content:
                 issues.append({"severity": "Critical", "title": t["robots_bad_rule"], "desc": "Found 'Disallow: /' which blocks ALL crawling.", "suggestion": "Remove global disallow rule.", "url": robots_url})
            if "sitemap:" not in content:
                 issues.append({"severity": "Low", "title": t["robots_no_sitemap"], "desc": "Sitemap location not specified in robots.txt.", "suggestion": "Add 'Sitemap: [URL]' directive.", "url": robots_url})
        r.close()
    except: 
        issues.append({"severity": "Medium", "title": t["no_robots"], "desc": "Connection failed.", "suggestion": "Check server config.", "url": robots_url})

    # --- 2. Sitemap æ·±åº¦æ£€æŸ¥ ---
    sitemap_urls_to_check = manual_sitemaps if manual_sitemaps else [urljoin(base_url, "/sitemap.xml")]
    any_sitemap_valid = False
    
    for sitemap_url in sitemap_urls_to_check:
        sitemap_url = sitemap_url.strip()
        if not sitemap_url: continue
        try:
            r = requests.get(sitemap_url, headers=headers, timeout=15, allow_redirects=True, verify=False)
            if r.status_code == 200:
                # å°è¯•è§£æ XML
                try:
                    root = ET.fromstring(r.content)
                    any_sitemap_valid = True
                    # æ£€æŸ¥ hreflang
                    if 'xhtml' in r.text or 'hreflang' in r.text:
                        sitemap_has_hreflang = True
                except ET.ParseError:
                    if not sitemap_url.endswith('.gz'): # å¿½ç•¥å‹ç¼©æ ¼å¼æŠ¥é”™
                        issues.append({"severity": "Medium", "title": t["sitemap_invalid"], "desc": "XML parsing failed.", "suggestion": "Check XML syntax.", "url": sitemap_url})
            else:
                if manual_sitemaps:
                    issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": f"Status: {r.status_code}", "suggestion": "Check URL.", "url": sitemap_url})
        except:
            if manual_sitemaps: issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": "Connection failed.", "suggestion": "Check URL.", "url": sitemap_url})

    if not any_sitemap_valid and not manual_sitemaps:
         issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": "Default sitemap not found.", "suggestion": "Ensure sitemap.xml exists.", "url": sitemap_urls_to_check[0]})

    return issues, sitemap_has_hreflang

def analyze_page(url, html_content, status_code, lang="zh", sitemap_has_hreflang=False):
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    
    # è¯­è¨€åŒ… - åŒ…å«æ‰€æœ‰æ£€æŸ¥é¡¹
    txt = {
        "zh": {
            "hreflang_invalid": "Hreflang ä»£ç æ ¼å¼é”™è¯¯",
            "hreflang_invalid_desc": "è¯­è¨€ä»£ç ä¸ç¬¦åˆ ISO 639-1 æ ‡å‡† (å¦‚ 'en-US', 'zh-CN')ã€‚",
            "hreflang_no_default": "Hreflang ç¼ºå¤± x-default",
            "hreflang_no_default_desc": "æœªé…ç½® 'x-default' å›é€€ç‰ˆæœ¬ï¼Œå½±å“éåŒ¹é…åœ°åŒºç”¨æˆ·ä½“éªŒã€‚",
            "alt_bad_quality": "å›¾ç‰‡ Alt è´¨é‡å·®",
            "alt_bad_quality_desc": "Alt æ–‡æœ¬åŒ…å«æ— æ„ä¹‰è¯æ±‡ï¼ˆå¦‚æ–‡ä»¶åã€'image' ç­‰ï¼‰æˆ–è¿‡çŸ­ã€‚",
            "anchor_bad_quality": "é”šæ–‡æœ¬è´¨é‡å·® (Generic Anchor)",
            "anchor_bad_quality_desc": "ä½¿ç”¨äº†é€šç”¨è¯æ±‡ï¼ˆå¦‚ 'Click here'ï¼‰ï¼Œæ— æ³•ä¼ é€’é“¾æ¥ç›¸å…³æ€§ã€‚",
            "cls_risk": "å­˜åœ¨ CLS å¸ƒå±€åç§»é£é™© (CWV)",
            "cls_risk_desc": "æ£€æµ‹åˆ° img æ ‡ç­¾ç¼ºå¤± width æˆ– height å±æ€§ï¼Œä¼šå¯¼è‡´é¡µé¢åŠ è½½æ—¶æŠ–åŠ¨ã€‚",
            "missing_title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title)", "missing_title_desc": "é¡µé¢æ²¡æœ‰ <title> æ ‡ç­¾ã€‚æœç´¢å¼•æ“æ— æ³•æŠ“å–é¡µé¢ä¸»é¢˜ï¼Œä¸¥é‡å½±å“å…³é”®è¯æ’åã€‚", "missing_title_sugg": "æ·»åŠ æè¿°æ€§æ ‡é¢˜ã€‚",
            "short_title": "æ ‡é¢˜è¿‡çŸ­", "short_title_desc": "æ ‡é¢˜è¿‡çŸ­ï¼Œéš¾ä»¥è¦†ç›–æ ¸å¿ƒå…³é”®è¯ã€‚", "short_title_sugg": "æ‰©å……æ ‡é¢˜é•¿åº¦ã€‚",
            "long_title": "æ ‡é¢˜è¿‡é•¿", "long_title_desc": "æ ‡é¢˜è¿‡é•¿ï¼Œå¯èƒ½åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­ã€‚", "long_title_sugg": "ç²¾ç®€æ ‡é¢˜é•¿åº¦ã€‚",
            "missing_desc": "ç¼ºå¤±å…ƒæè¿°", "missing_desc_desc": "ç¼ºå¤± Meta Descriptionã€‚å½±å“ç‚¹å‡»ç‡ã€‚", "missing_desc_sugg": "æ·»åŠ  Meta Descriptionã€‚",
            "short_desc": "å…ƒæè¿°è¿‡çŸ­", "short_desc_desc": "å†…å®¹è¿‡å°‘ï¼Œå¸å¼•åŠ›ä¸è¶³ã€‚", "short_desc_sugg": "æ‰©å……æè¿°å†…å®¹ã€‚",
            "missing_h1": "ç¼ºå¤± H1 æ ‡ç­¾", "missing_h1_desc": "é¡µé¢ç¼ºä¹ H1 ä¸»æ ‡é¢˜ã€‚å½±å“å†…å®¹å±‚çº§ç†è§£ã€‚", "missing_h1_sugg": "æ·»åŠ å”¯ä¸€çš„ H1 æ ‡ç­¾ã€‚",
            "missing_viewport": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®", "missing_viewport_desc": "æœªé…ç½® Viewportã€‚å½±å“ç§»åŠ¨ç«¯æ’åã€‚", "missing_viewport_sugg": "æ·»åŠ  viewport meta æ ‡ç­¾ã€‚",
            "missing_canonical": "ç¼ºå¤± Canonical æ ‡ç­¾", "missing_canonical_desc": "æœªæŒ‡å®šè§„èŒƒé“¾æ¥ã€‚å¯èƒ½å¯¼è‡´é‡å¤å†…å®¹ã€‚", "missing_canonical_sugg": "æ·»åŠ  canonical æ ‡ç­¾ã€‚",
            "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®", "missing_jsonld_desc": "æœªæ£€æµ‹åˆ° Schema æ ‡è®°ã€‚é”™å¤±å¯Œåª’ä½“ç»“æœã€‚", "missing_jsonld_sugg": "æ ¹æ®é¡µé¢ç±»å‹æ·»åŠ  JSON-LDã€‚",
            "missing_hreflang": "ç¼ºå¤± Hreflang", "missing_hreflang_desc": "æœªå‘ç°è¯­è¨€åŒºåŸŸæ ‡è®°ï¼ˆHTML/Sitemapï¼‰ã€‚", "missing_hreflang_sugg": "æ·»åŠ  hreflang æ ‡ç­¾ã€‚",
            "soft_404": "ç–‘ä¼¼è½¯ 404 (Soft 404)", "soft_404_desc": "é¡µé¢è¿”å› 200 ä½†å†…å®¹æ˜¾ç¤ºæœªæ‰¾åˆ°ã€‚æµªè´¹çˆ¬å–é¢„ç®—ã€‚", "soft_404_sugg": "é…ç½® 404 çŠ¶æ€ç ã€‚",
            "missing_alt": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§", "missing_alt_desc": "å›¾ç‰‡ç¼ºå°‘æ›¿ä»£æ–‡æœ¬ã€‚å½±å“å›¾ç‰‡æœç´¢ã€‚", "missing_alt_sugg": "æ·»åŠ  alt å±æ€§ã€‚",
            "duplicate": "å‘ç°é‡å¤å†…å®¹", "duplicate_desc": "å†…å®¹é«˜åº¦é‡å¤ã€‚å¯¼è‡´å…³é”®è¯ç«äº‰ã€‚", "duplicate_sugg": "ä½¿ç”¨ Canonical æˆ–åˆå¹¶ã€‚",
            "js_links": "å‘ç° JS ä¼ªé“¾æ¥", "js_links_desc": "href='javascript:' çˆ¬è™«æ— æ³•æŠ“å–ã€‚", "js_links_sugg": "ä½¿ç”¨æ ‡å‡†é“¾æ¥ã€‚",
            "url_underscore": "URL åŒ…å«ä¸‹åˆ’çº¿", "url_underscore_desc": "å»ºè®®ä½¿ç”¨è¿å­—ç¬¦ã€‚", "url_underscore_sugg": "ä¼˜åŒ– URL ç»“æ„ã€‚",
            "url_uppercase": "URL åŒ…å«å¤§å†™å­—æ¯", "url_uppercase_desc": "å»ºè®®ä½¿ç”¨å°å†™ã€‚", "url_uppercase_sugg": "ç»Ÿä¸€ä¸ºå°å†™ URLã€‚"
        },
        "en": {
            "hreflang_invalid": "Invalid Hreflang Code",
            "hreflang_invalid_desc": "Language code format is invalid (Use 'en-US', 'fr-FR').",
            "hreflang_no_default": "Missing x-default Hreflang",
            "hreflang_no_default_desc": "No 'x-default' fallback found for unmatched regions.",
            "alt_bad_quality": "Poor Quality Alt Text",
            "alt_bad_quality_desc": "Alt text uses filenames or generic words like 'image'.",
            "anchor_bad_quality": "Poor Anchor Text (Generic)",
            "anchor_bad_quality_desc": "Generic text (e.g., 'Click here') found. Use descriptive text.",
            "cls_risk": "CLS Layout Shift Risk (CWV)",
            "cls_risk_desc": "Images missing width/height attributes, causing layout jumps.",
            "missing_title": "Missing Title Tag", "missing_title_desc": "Page has no <title> tag. Impacts ranking.", "missing_title_sugg": "Add descriptive title.",
            "short_title": "Title Too Short", "short_title_desc": "Title is too short for keywords.", "short_title_sugg": "Increase length.",
            "long_title": "Title Too Long", "long_title_desc": "Title may be truncated.", "long_title_sugg": "Shorten title.",
            "missing_desc": "Missing Meta Description", "missing_desc_desc": "Missing description affects CTR.", "missing_desc_sugg": "Add description.",
            "short_desc": "Meta Description Too Short", "short_desc_desc": "Content too thin.", "short_desc_sugg": "Expand description.",
            "missing_h1": "Missing H1 Tag", "missing_h1_desc": "No main H1 heading.", "missing_h1_sugg": "Add unique H1.",
            "missing_viewport": "Missing Mobile Viewport", "missing_viewport_desc": "No viewport tag. Hurts mobile ranking.", "missing_viewport_sugg": "Add viewport meta.",
            "missing_canonical": "Missing Canonical Tag", "missing_canonical_desc": "Canonical URL not specified.", "missing_canonical_sugg": "Add canonical tag.",
            "missing_jsonld": "Missing Structured Data", "missing_jsonld_desc": "No Schema markup found.", "missing_jsonld_sugg": "Add JSON-LD.",
            "missing_hreflang": "Missing Hreflang", "missing_hreflang_desc": "No language targeting found.", "missing_hreflang_sugg": "Add hreflang.",
            "soft_404": "Suspected Soft 404", "soft_404_desc": "Returns 200 but content says Not Found.", "soft_404_sugg": "Return 404 status.",
            "missing_alt": "Images Missing Alt Text", "missing_alt_desc": "Images lack alt text.", "missing_alt_sugg": "Add alt attributes.",
            "duplicate": "Duplicate Content", "duplicate_desc": "Content highly matches another page.", "duplicate_sugg": "Use canonicals.",
            "js_links": "JavaScript Pseudo-links", "js_links_desc": "Crawlers cannot follow JS links.", "js_links_sugg": "Use standard links.",
            "url_underscore": "URL Contains Underscores", "url_underscore_desc": "Hyphens preferred.", "url_underscore_sugg": "Use hyphens.",
            "url_uppercase": "URL Contains Uppercase", "url_uppercase_desc": "URLs are case-sensitive.", "url_uppercase_sugg": "Use lowercase."
        }
    }
    t = txt[lang]
    
    for script in soup(["script", "style"]): script.extract()
    text_content = soup.get_text().strip()
    content_hash = get_content_hash(text_content)

    # --- 1. Hreflang æ·±åº¦éªŒè¯ ---
    hreflangs = soup.find_all('link', hreflang=True)
    if hreflangs:
        has_x_default = False
        invalid_codes = []
        code_pattern = re.compile(r'^[a-z]{2}(-[a-zA-Z]{2})?$|x-default', re.IGNORECASE)
        
        for link in hreflangs:
            code = link.get('hreflang', '').strip()
            if code.lower() == 'x-default': has_x_default = True
            if not code_pattern.match(code): invalid_codes.append(code)
        
        if invalid_codes:
            issues.append({"severity": "High", "title": t["hreflang_invalid"], "desc": f"{t['hreflang_invalid_desc']} Found: {', '.join(invalid_codes[:3])}", "suggestion": "Use ISO 639-1 codes.", "url": url})
        if not has_x_default:
            issues.append({"severity": "Low", "title": t["hreflang_no_default"], "desc": t["hreflang_no_default_desc"], "suggestion": "Add hreflang='x-default'.", "url": url})
    elif not sitemap_has_hreflang:
        issues.append({"severity": "Low", "title": t["missing_hreflang"], "desc": t["missing_hreflang_desc"], "suggestion": t["missing_hreflang_sugg"], "url": url})

    # --- 2. å›¾ç‰‡ Alt è´¨é‡æ£€æŸ¥ & CWV (CLS) ---
    images = soup.find_all('img')
    missing_alt = 0
    bad_alt_count = 0
    cls_risk_count = 0
    bad_keywords = ["image", "photo", "picture", "img", "untitled", ".jpg", ".png"]
    
    for img in images:
        alt = img.get('alt', '').strip()
        if not alt: 
            missing_alt += 1
        else:
            if len(alt) < 3 or any(bk in alt.lower() for bk in bad_keywords):
                bad_alt_count += 1
        
        if not img.get('width') or not img.get('height'):
            cls_risk_count += 1

    if missing_alt > 0:
        issues.append({"severity": "Medium", "title": t["missing_alt"], "desc": f"{missing_alt} {t['missing_alt_desc']}", "suggestion": t["missing_alt_sugg"], "url": url})
    if bad_alt_count > 0:
        issues.append({"severity": "Low", "title": t["alt_bad_quality"], "desc": t["alt_bad_quality_desc"], "suggestion": "Avoid generic keywords.", "url": url, "evidence": f"{bad_alt_count} poor alts"})
    if cls_risk_count > 0:
        issues.append({"severity": "Medium", "title": t["cls_risk"], "desc": t["cls_risk_desc"], "suggestion": "Always specify width/height.", "url": url, "evidence": f"{cls_risk_count} images without dims"})

    # --- 3. é”šæ–‡æœ¬è´¨é‡æ£€æŸ¥ ---
    links = soup.find_all('a', href=True)
    bad_anchors = ["click here", "read more", "learn more", "more", "here", "link", "ç‚¹å‡»è¿™é‡Œ", "æ›´å¤š", "è¯¦æƒ…"]
    found_bad_anchors = []
    
    for link in links:
        anchor_text = link.get_text().strip().lower()
        if anchor_text in bad_anchors:
            found_bad_anchors.append(anchor_text)
            
    if found_bad_anchors:
        unique_bad = list(set(found_bad_anchors))
        issues.append({"severity": "Low", "title": t["anchor_bad_quality"], "desc": f"{t['anchor_bad_quality_desc']} Found: {', '.join(unique_bad[:3])}", "suggestion": "Use descriptive keywords.", "url": url})

    # --- 4. åŸºç¡€æ£€æŸ¥ (Title, H1, etc.) ---
    title_tag = soup.title
    title = title_tag.string.strip() if title_tag and title_tag.string else None
    if not title:
        issues.append({"severity": "High", "title": t["missing_title"], "desc": t["missing_title_desc"], "suggestion": t["missing_title_sugg"], "url": url})
    elif len(title) < 10:
         issues.append({"severity": "Medium", "title": t["short_title"], "desc": t["short_title_desc"], "suggestion": t["short_title_sugg"], "url": url, "evidence": title})
    elif len(title) > 60:
         issues.append({"severity": "Low", "title": t["long_title"], "desc": t["long_title_desc"], "suggestion": t["long_title_sugg"], "url": url, "evidence": title})

    meta_desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = meta_desc['content'].strip() if meta_desc and meta_desc.get('content') else None
    if not desc_content:
        issues.append({"severity": "High", "title": t["missing_desc"], "desc": t["missing_desc_desc"], "suggestion": t["missing_desc_sugg"], "url": url})
    elif len(desc_content) < 50:
        issues.append({"severity": "Low", "title": t["short_desc"], "desc": t["short_desc_desc"], "suggestion": t["short_desc_sugg"], "url": url, "evidence": desc_content})

    h1 = soup.find('h1')
    if not h1: issues.append({"severity": "High", "title": t["missing_h1"], "desc": t["missing_h1_desc"], "suggestion": t["missing_h1_sugg"], "url": url})

    if not soup.find('meta', attrs={'name': 'viewport'}):
        issues.append({"severity": "Critical", "title": t["missing_viewport"], "desc": t["missing_viewport_desc"], "suggestion": t["missing_viewport_sugg"], "url": url})

    canonical_tag = soup.find('link', attrs={'rel': 'canonical'})
    canonical_url = canonical_tag['href'] if canonical_tag else None
    if not canonical_url:
        issues.append({"severity": "Medium", "title": t["missing_canonical"], "desc": t["missing_canonical_desc"], "suggestion": t["missing_canonical_sugg"], "url": url})

    if not soup.find('script', type='application/ld+json'):
         issues.append({"severity": "Medium", "title": t["missing_jsonld"], "desc": t["missing_jsonld_desc"], "suggestion": t["missing_jsonld_sugg"], "url": url})

    # URL Checks
    parsed_url = urlparse(url)
    path = parsed_url.path
    if '_' in path:
         issues.append({"severity": "Low", "title": t["url_underscore"], "desc": t["url_underscore_desc"], "suggestion": t["url_underscore_sugg"], "url": url})
    if any(c.isupper() for c in path):
         issues.append({"severity": "Medium", "title": t["url_uppercase"], "desc": t["url_uppercase_desc"], "suggestion": t["url_uppercase_sugg"], "url": url})

    # JS Links
    js_links = soup.find_all('a', href=lambda x: x and x.lower().startswith('javascript:'))
    if js_links:
        issues.append({"severity": "High", "title": t["js_links"], "desc": t["js_links_desc"], "suggestion": t["js_links_sugg"], "url": url, "meta": f"Count: {len(js_links)}"})

    if status_code == 200:
        error_kws = ["page not found", "404 error", "é¡µé¢æœªæ‰¾åˆ°"]
        is_s404 = False
        if title and any(k in title.lower() for k in error_kws): is_s404 = True
        elif soup.find('h1') and any(k in soup.find('h1').get_text().lower() for k in error_kws): is_s404 = True
        if is_s404:
            issues.append({"severity": "Critical", "title": t["soft_404"], "desc": t["soft_404_desc"], "suggestion": t["soft_404_sugg"], "url": url})

    return {
        "URL": url, "Status": status_code, "Title": title or "No Title",
        "H1": h1.get_text().strip() if h1 else "No H1", "Links_Count": len(soup.find_all('a')),
        "Issues_Count": len(issues), "Content_Hash": content_hash, "Canonical": canonical_url
    }, issues, []

def crawl_website(start_url, max_pages=100, lang="zh", manual_robots=None, manual_sitemaps=None, psi_key=None):
    visited = set()
    seen_hashes = {} 
    queue = [start_url]
    results_data = []
    all_issues = []
    first_error = None
    
    t_dup_title = "Duplicate Content Detected" if lang == "en" else "å‘ç°é‡å¤å†…å®¹"
    t_dup_desc = "Content matches another page." if lang == "en" else "å†…å®¹é‡å¤ã€‚"
    t_dup_sugg = "Add canonical." if lang == "en" else "æ·»åŠ  Canonicalã€‚"
    
    progress_bar = st.progress(0, text="Initializing...")
    sitemap_has_hreflang = False
    
    try:
        site_issues, sitemap_has_hreflang = check_site_level_assets(
            start_url, lang=lang, manual_robots=manual_robots, manual_sitemaps=manual_sitemaps
        )
        all_issues.extend(site_issues)
        st.session_state['sitemap_hreflang_found'] = sitemap_has_hreflang
    except Exception as e:
        pass

    if psi_key:
        with st.spinner(TRANSLATIONS[lang]["psi_fetching"]):
            cwv_data = fetch_psi_data(start_url, psi_key)
            if cwv_data and "error" not in cwv_data:
                st.session_state['cwv_data'] = cwv_data
            else:
                st.session_state['cwv_data'] = None
                if cwv_data and "error" in cwv_data:
                    st.toast(f"PSI Error: {cwv_data['error']}")

    pages_crawled = 0
    headers = get_browser_headers()
    
    while queue and pages_crawled < max_pages:
        url = queue.pop(0)
        if url in visited: continue
        visited.add(url)
        pages_crawled += 1
        
        progress = int((pages_crawled / max_pages) * 100)
        progress_bar.progress(progress, text=f"Crawling ({pages_crawled}/{max_pages}): {url}")
        
        try:
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                page_data, page_issues, _ = analyze_page(
                    url, response.content, response.status_code, 
                    lang=lang, sitemap_has_hreflang=sitemap_has_hreflang
                )
                
                current_hash = page_data['Content_Hash']
                current_canonical = page_data['Canonical']
                
                if current_hash in seen_hashes:
                    original_url = seen_hashes[current_hash]
                    is_ok = current_canonical and current_canonical != url
                    if not is_ok:
                        page_issues.append({
                            "severity": "High", "title": t_dup_title, "desc": f"{t_dup_desc} (vs {original_url})", 
                            "suggestion": t_dup_sugg, "url": url, "meta": f"Duplicate of: {original_url}"
                        })
                else:
                    seen_hashes[current_hash] = url

                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                soup = BeautifulSoup(response.content, 'html.parser')
                base_domain = urlparse(url).netloc
                for a in soup.find_all('a', href=True):
                    link = urljoin(url, a['href'])
                    if urlparse(link).netloc == base_domain and link not in visited and link not in queue:
                        if not any(link.lower().endswith(ext) for ext in ['.jpg', '.png', '.pdf']):
                            queue.append(link)
            else:
                if pages_crawled == 1: first_error = f"Content type is {content_type}, expected HTML"
        except Exception as e:
            if pages_crawled == 1: first_error = str(e)
            pass
    
    progress_bar.progress(100, text="Done!")
    time.sleep(0.5)
    progress_bar.empty()
    
    if not results_data and first_error:
        return None, None, first_error
        
    return results_data, all_issues, None

def create_styled_pptx(slides_data, lang="zh"):
    prs = Presentation()
    prs.slide_width = Inches(13.333)
    prs.slide_height = Inches(7.5)
    txt = TRANSLATIONS[lang] 
    
    def set_font(font_obj, size, bold=False, color=None):
        font_obj.size = Pt(size)
        font_obj.name = 'Microsoft YaHei' if lang == "zh" else 'Arial'
        font_obj.bold = bold
        if color: font_obj.color.rgb = color

    def draw_serp_preview(slide, issue_title, evidence, url):
        box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(2), Inches(5.8), Inches(2.5))
        box.fill.solid()
        box.fill.fore_color.rgb = RGBColor(255, 255, 255)
        box.line.color.rgb = RGBColor(220, 220, 220)
        tf = box.text_frame
        tf.margin_left = Inches(0.2)
        tf.margin_top = Inches(0.2)
        
        p = tf.add_paragraph()
        domain = urlparse(url).netloc
        p.text = f"{domain} â€º ..."
        set_font(p.font, 12, False, RGBColor(32, 33, 36))
        
        p = tf.add_paragraph()
        p.space_before = Pt(5)
        display_title = evidence if evidence else "Untitled Page"
        if len(display_title) > 60 and ("Long" in issue_title or "è¿‡é•¿" in issue_title):
            display_title = display_title[:55] + " ..."
        p.text = display_title
        set_font(p.font, 18, False, RGBColor(26, 13, 171)) 
        
        p = tf.add_paragraph()
        p.space_before = Pt(3)
        p.text = "Please provide a meta description..."
        set_font(p.font, 14, False, RGBColor(77, 81, 86))

        label = slide.shapes.add_textbox(Inches(7), Inches(1.6), Inches(3), Inches(0.3))
        p = label.text_frame.add_paragraph()
        p.text = txt["serp_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100))

    # Cover
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg = slide.shapes.add_shape(1, 0, 0, Inches(13.333), Inches(7.5))
    bg.fill.solid()
    bg.fill.fore_color.rgb = RGBColor(18, 52, 86)
    
    title = slide.shapes.add_textbox(Inches(1), Inches(2.5), Inches(11), Inches(2))
    p = title.text_frame.add_paragraph()
    p.text = txt["ppt_cover_title"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 54, True, RGBColor(255, 255, 255))
    
    sub = slide.shapes.add_textbox(Inches(1), Inches(4), Inches(11), Inches(1))
    p = sub.text_frame.add_paragraph()
    p.text = txt["ppt_cover_sub"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p_sub.font, 24, False, RGBColor(200, 200, 200))

    # Slides
    for issue in slides_data:
        slide = prs.slides.add_slide(prs.slide_layouts[6])
        
        h_shape = slide.shapes.add_shape(1, 0, 0, Inches(13.333), Inches(1.2))
        h_shape.fill.solid()
        h_shape.fill.fore_color.rgb = RGBColor(240, 242, 246)
        
        h_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(10), Inches(0.8))
        p = h_box.text_frame.add_paragraph()
        p.text = issue['title']
        set_font(p.font, 32, True, RGBColor(50, 50, 50))
        
        sev_color = RGBColor(220, 53, 69) if issue['severity'] == "Critical" else RGBColor(253, 126, 20)
        sev_box = slide.shapes.add_textbox(Inches(11), Inches(0.35), Inches(2), Inches(0.5))
        p = sev_box.text_frame.add_paragraph()
        p.text = f"{issue['severity']}"
        p.alignment = PP_ALIGN.CENTER
        set_font(p.font, 18, True, sev_color)
        
        d_title = slide.shapes.add_textbox(Inches(0.5), Inches(1.5), Inches(6), Inches(0.5))
        p = d_title.text_frame.add_paragraph()
        p.text = txt["ppt_slide_desc_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        d_box = slide.shapes.add_textbox(Inches(0.5), Inches(2.0), Inches(6), Inches(2.5))
        tf = d_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = issue['desc']
        set_font(p.font, 14, False, RGBColor(80, 80, 80))
        
        c_box = slide.shapes.add_textbox(Inches(0.5), Inches(3.5), Inches(6), Inches(0.5))
        p = c_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_count_title"].format(issue['count'])
        set_font(p.font, 14, True, RGBColor(100, 100, 100))

        is_serp = any(k in issue['title'] for k in ["Title", "æ ‡é¢˜", "Meta", "å…ƒæè¿°"])
        ev = issue.get('example_evidence', '')
        ex_url = issue['examples'][0] if issue['examples'] else "example.com"
        
        if is_serp:
            draw_serp_preview(slide, issue['title'], ev, ex_url)
        else:
            e_title = slide.shapes.add_textbox(Inches(7), Inches(1.5), Inches(5.8), Inches(0.5))
            p = e_title.text_frame.add_paragraph()
            p.text = txt["ppt_slide_ex_title"]
            set_font(p.font, 18, True, RGBColor(30, 30, 30))
            
            e_box = slide.shapes.add_textbox(Inches(7), Inches(2.0), Inches(5.8), Inches(2.5))
            tf = e_box.text_frame
            tf.word_wrap = True
            tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
            for url in issue['examples'][:5]:
                p = tf.add_paragraph()
                p.text = f"â€¢ {url}"
                set_font(p.font, 12, False, RGBColor(0, 102, 204))
                p.space_after = Pt(6)

        s_bg = slide.shapes.add_shape(1, Inches(0.5), Inches(5.8), Inches(12.333), Inches(1.5))
        s_bg.fill.solid()
        s_bg.fill.fore_color.rgb = RGBColor(230, 244, 234)
        s_bg.line.color.rgb = RGBColor(40, 167, 69)
        
        s_box = slide.shapes.add_textbox(Inches(0.7), Inches(5.9), Inches(11.9), Inches(1.3))
        tf = s_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = txt["ppt_slide_sugg_title"]
        set_font(p.font, 16, True, RGBColor(21, 87, 36))
        p = tf.add_paragraph()
        p.text = issue['suggestion']
        set_font(p.font, 14, False, RGBColor(21, 87, 36))
        p.space_before = Pt(5)

    out = BytesIO()
    prs.save(out)
    out.seek(0)
    return out

# --- 4. Init Session ---
if 'audit_data' not in st.session_state: st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state: st.session_state['audit_issues'] = []
if 'language' not in st.session_state: st.session_state['language'] = "zh" 
if 'sitemap_hreflang_found' not in st.session_state: st.session_state['sitemap_hreflang_found'] = False
if 'cwv_data' not in st.session_state: st.session_state['cwv_data'] = None

# --- 5. Sidebar ---
lang = st.session_state['language']
ui = TRANSLATIONS[lang]

with st.sidebar:
    st.title(ui["sidebar_title"])
    st.caption(ui["sidebar_caption"])
    
    st.divider()
    selected_lang = st.radio(ui["lang_label"], ["ä¸­æ–‡", "English"], index=0 if lang=="zh" else 1)
    new_lang = "zh" if selected_lang == "ä¸­æ–‡" else "en"
    if new_lang != lang:
        st.session_state['language'] = new_lang
        st.rerun()
    
    st.divider()
    menu_options = ui["nav_options"]
    menu_map = {ui["nav_options"][i]: ["input", "dashboard", "matrix", "ppt"][i] for i in range(4)}
    selected_menu = st.radio(ui["nav_label"], menu_options)
    menu_key = menu_map[selected_menu]
    
    st.divider()
    if st.session_state['audit_data'] is not None:
        st.success(ui["cache_info"].format(len(st.session_state['audit_data'])))
        st.markdown(f"**{ui['sitemap_status_title']}**")
        if st.session_state['sitemap_hreflang_found']: st.caption(ui["sitemap_found_href"])
        else: st.caption(ui["sitemap_no_href"])
        if st.button(ui["clear_data"]):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.session_state['sitemap_hreflang_found'] = False
            st.session_state['cwv_data'] = None
            st.rerun()

# --- 6. Main Logic ---
if menu_key == "input":
    st.header(ui["input_header"])
    st.info(ui["input_info"])
    
    col1, col2 = st.columns([3, 1])
    with col1:
        url_input = st.text_input(ui["input_label"], placeholder=ui["input_placeholder"])
    with col2:
        max_pages = st.number_input(ui.get("max_pages_label", "Max Pages"), min_value=1, max_value=1000, value=100)
    
    with st.expander(ui.get("adv_settings", "Advanced")):
        manual_robots = st.text_input(ui.get("manual_robots", "Manual Robots.txt"), placeholder="https://example.com/robots.txt")
        manual_sitemaps_text = st.text_area(ui.get("manual_sitemaps", "Manual Sitemaps"), placeholder="https://example.com/sitemap.xml")
        manual_sitemaps = [s.strip() for s in manual_sitemaps_text.split('\n') if s.strip()]
    
    with st.expander(ui.get("psi_settings", "Google PSI")):
        psi_key = st.text_input(ui.get("psi_api_key_label", "API Key"), type="password", help=ui.get("psi_api_help", ""))
        st.caption(ui["psi_get_key"])

    start_btn = st.button(ui["start_btn"], type="primary", use_container_width=True)
    
    if start_btn and url_input:
        if not is_valid_url(url_input):
            st.error(ui["error_url"])
        else:
            with st.spinner(ui["spinner_crawl"].format(max_pages)):
                data, issues, error_msg = crawl_website(url_input, max_pages, lang, manual_robots, manual_sitemaps, psi_key)
                if not data:
                    st.error(ui["error_no_data"].format(error_msg or "Unknown Error"))
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(ui["success_audit"].format(len(data)))
                    st.balloons()

elif menu_key == "dashboard":
    st.header(ui["dashboard_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        if st.session_state.get('cwv_data'):
            cwv = st.session_state['cwv_data']
            st.subheader(ui["cwv_title"])
            st.caption(ui["cwv_source"])
            c1, c2, c3, c4 = st.columns(4)
            def metric_color(val, good, poor):
                if val <= good: return "normal"
                if val >= poor: return "inverse"
                return "off"
            c1.metric("LCP (Loading)", f"{cwv.get('LCP',0):.2f}s", delta_color=metric_color(cwv.get('LCP',0), 2.5, 4.0))
            c2.metric("CLS (Visual)", f"{cwv.get('CLS',0):.3f}", delta_color=metric_color(cwv.get('CLS',0), 0.1, 0.25))
            c3.metric("INP (Interact)", f"{cwv.get('INP',0)}ms", delta_color=metric_color(cwv.get('INP',0), 200, 500))
            c4.metric("FCP", f"{cwv.get('FCP',0):.2f}s")
            st.divider()

        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        total = len(issues)
        score = max(0, 100 - int(total * 0.5))
        critical = len([i for i in issues if i['severity'] == 'Critical'])
        
        k1, k2, k3, k4 = st.columns(4)
        k1.metric(ui["kpi_health"], f"{score}/100")
        k2.metric(ui["kpi_pages"], str(len(df)))
        k3.metric(ui["kpi_issues"], str(total), delta_color="inverse")
        k4.metric(ui["kpi_critical"], str(critical), delta_color="inverse")
        
        st.divider()
        c1, c2 = st.columns(2)
        with c1:
            st.subheader(ui["chart_issues"])
            if issues: st.bar_chart(pd.DataFrame(issues)['title'].value_counts())
            else: st.info(ui["chart_no_issues"])
        with c2:
            st.subheader(ui["chart_status"])
            if not df.empty: st.bar_chart(df['Status'].value_counts())

elif menu_key == "matrix":
    st.header(ui["matrix_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(df, use_container_width=True, hide_index=True)
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(ui["download_csv"], csv, "audit_report.csv", "text/csv")

elif menu_key == "ppt":
    st.header(ui["ppt_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    elif not st.session_state['audit_issues']:
        st.success(ui["ppt_success_no_issues"])
    else:
        raw_issues = st.session_state['audit_issues']
        grouped = {}
        for i in raw_issues:
            t = i['title']
            if t not in grouped:
                grouped[t] = {
                    "title": t, "severity": i['severity'], "desc": i['desc'], 
                    "suggestion": i['suggestion'], "count": 0, "examples": [],
                    "example_evidence": i.get("evidence", "")
                }
            grouped[t]['count'] += 1
            if len(grouped[t]['examples']) < 5: grouped[t]['examples'].append(i['url'])
        
        sov_order = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}
        slides = sorted(list(grouped.values()), key=lambda x: (sov_order.get(x['severity'], 3), -x['count']))

        st.write(f"### {ui['ppt_download_header']}")
        st.info(ui["ppt_info"])
        if st.button(ui["ppt_btn"]):
            with st.spinner("Generating..."):
                pptx = create_styled_pptx(slides, lang=lang)
                fname = f"seo_audit_report_{lang}.pptx"
                st.download_button(ui["ppt_btn"], pptx, fname, "application/vnd.openxmlformats-officedocument.presentationml.presentation")

        st.divider()
        st.subheader(ui["ppt_preview_header"])
        
        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(slides): st.session_state.slide_index = 0
        
        s = slides[st.session_state.slide_index]
        with st.container(border=True):
            st.markdown(f"### {ui['ppt_slide_title']} {s['title']}")
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if s['severity'] == "Critical" else "orange" if s['severity'] == "High" else "blue"
                st.markdown(f"**{ui['ppt_severity']}** :{color}[{s['severity']}]")
                st.markdown(f"**{ui['ppt_impact']}** {ui['ppt_impact_desc'].format(s['count'])}")
                st.markdown(f"**{ui['ppt_desc']}** {s['desc']}")
                st.info(f"{ui['ppt_sugg']} {s['suggestion']}")
            with c2:
                is_serp = any(k in s['title'] for k in ["Title", "æ ‡é¢˜", "Meta", "å…ƒæè¿°"])
                if is_serp:
                    st.markdown(f"**{ui.get('serp_sim_title', 'SERP Preview')}**")
                    ev = s.get('example_evidence', '')
                    ex_url = s['examples'][0] if s['examples'] else "example.com"
                    display_title = ev if ev else "Untitled Page"
                    if len(display_title) > 60: display_title = display_title[:55] + " ..."
                    st.markdown(f"""
                    <div style="font-family: Arial, sans-serif; border: 1px solid #dfe1e5; border-radius: 8px; padding: 15px; background: white; box-shadow: 0 1px 6px rgba(32,33,36,0.28);">
                        <div style="font-size: 14px; color: #202124;">{urlparse(ex_url).netloc} <span style="color: #5f6368">â€º ...</span></div>
                        <div style="font-size: 20px; color: #1a0dab; margin-top: 5px;">{display_title}</div>
                        <div style="font-size: 14px; color: #4d5156; margin-top: 3px;">
                            Please provide a meta description...
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown(f"**{ui['ppt_examples']}**")
                    for ex in s['examples']: st.markdown(f"- `{ex}`")

        cp, ct, cn = st.columns([1, 2, 1])
        with cp:
            if st.button(ui["ppt_prev"]):
                st.session_state.slide_index = max(0, st.session_state.slide_index - 1)
                st.rerun()
        with ct:
            st.markdown(f"<div style='text-align: center'>Slide {st.session_state.slide_index + 1} / {len(slides)}</div>", unsafe_allow_html=True)
        with cn:
            if st.button(ui["ppt_next"]):
                st.session_state.slide_index = min(len(slides) - 1, st.session_state.slide_index + 1)
                st.rerun()
