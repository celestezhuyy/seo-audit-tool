import streamlit as st
import time
import pandas as pd
import requests
import hashlib
import re
import urllib3
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO
import socket

# --- Level 0: é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
    from pptx.enum.shapes import MSO_SHAPE, MSO_CONNECTOR
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- Level 1: åŸºç¡€å·¥å…·å‡½æ•° ---
def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def estimate_pixel_width(text, font_size=18):
    """ä¼°ç®—æ–‡æœ¬åœ¨ Google SERP ä¸­çš„åƒç´ å®½åº¦"""
    if not text: return 0
    width = 0
    for char in text:
        if ord(char) > 127: 
            width += font_size
        elif char.isupper():
            width += font_size * 0.7 
        else:
            width += font_size * 0.55 
    return width

def get_browser_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Connection': 'keep-alive',
    }

def fetch_psi_data(url, api_key):
    if not api_key: return None
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available"}
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else: return {"error": f"API Error: {response.status_code}"}
    except Exception as e: return {"error": str(e)}

def check_server_location(url):
    """æ£€æµ‹æœåŠ¡å™¨åœ°ç†ä½ç½®"""
    try:
        domain = urlparse(url).netloc
        ip = socket.gethostbyname(domain)
        response = requests.get(f"http://ip-api.com/json/{ip}", timeout=3)
        if response.status_code == 200:
            data = response.json()
            return data.get("countryCode", "Unknown"), data.get("country", "Unknown")
    except:
        pass
    return None, None

# --- Level 2: æ’åºä¸é…ç½®å¸¸é‡ ---
CATEGORY_ORDER = ["access", "indexability", "technical", "content", "image_ux", "cwv_performance"]
SEVERITY_ORDER = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}

ISSUE_PRIORITY_LIST = [
    "no_robots", "robots_bad_rule", "robots_quality_issue", "baidu_robots_missing", "robots_no_sitemap", "no_sitemap", "sitemap_invalid",
    "http_5xx", "http_4xx", "soft_404", "http_3xx",
    "server_not_in_china",
    "duplicate", "missing_canonical", "hreflang_invalid", "hreflang_no_default", "missing_hreflang",
    "missing_viewport", "missing_jsonld", "js_links", "url_underscore", "url_uppercase",
    "missing_baidu_stats", "missing_baidu_verify", "missing_applicable_device", "missing_no_transform", "missing_icp", "content_not_chinese",
    "missing_title", "short_title", "long_title", "missing_desc", "short_desc", "missing_h1", "missing_keywords", 
    "no_favicon", "missing_alt", "alt_bad_quality", "anchor_bad_quality", 
    "lcp_issue", "inp_issue", "cls_issue", "fcp_issue", "fcp_baidu_issue", "cls_risk"
]

def get_issue_priority(issue_id):
    try: return ISSUE_PRIORITY_LIST.index(issue_id)
    except ValueError: return 999 

# --- Level 3: å›½é™…åŒ–å­—å…¸ ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "æ——èˆ°å®¡è®¡ç‰ˆ v14.12",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        "sitemap_status_title": "Sitemap çŠ¶æ€:",
        "sitemap_found_href": "âœ… å‘ç° Hreflang é…ç½®", 
        "sitemap_no_href": "âš ï¸ æœªå‘ç° Hreflang",     
        "sitemap_missing": "âŒ æœªæ‰¾åˆ° Sitemap",
        "psi_settings": "Google PSI API è®¾ç½® (æ¨è)",
        "psi_api_key_label": "è¾“å…¥ Google PageSpeed API Key",
        "psi_api_help": "å»ºè®®å¡«å…¥ä»¥è·å– LCP/CLS/INP çœŸå®æ•°æ®ã€‚ç•™ç©ºåˆ™åªè¿›è¡Œä»£ç å®¡è®¡ã€‚",
        "psi_list_url_label": "äº§å“åˆ—è¡¨é¡µ URL (å¯é€‰)",
        "psi_detail_url_label": "äº§å“è¯¦æƒ…é¡µ URL (å¯é€‰)",
        "psi_get_key": "æ²¡æœ‰ API Key? [ç‚¹å‡»è¿™é‡Œå…è´¹ç”³è¯·](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "æ­£åœ¨è°ƒç”¨ Google API è·å– {} æ•°æ®...",
        "psi_success": "æˆåŠŸè·å–çœŸå®ç”¨æˆ·æ•°æ®ï¼",
        "psi_error": "API è°ƒç”¨å¤±è´¥æˆ–æ—  CrUX æ•°æ®",
        "input_header": "å¼€å§‹æ·±åº¦å®¡è®¡",
        "input_info": "è¯´æ˜: v14.12 ä¿®å¤äº†ç¼©è¿›é”™è¯¯ï¼ŒåŒ…å«ç™¾åº¦ SEO å¢å¼ºåŠŸèƒ½ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€ (é¦–é¡µ)",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°",
        "adv_settings": "é«˜çº§è®¾ç½® (Advanced Settings)", 
        "check_robots_label": "æ£€æŸ¥å¹¶éµå¾ª Robots.txt è§„åˆ™", 
        "crawl_sitemap_label": "è‡ªåŠ¨æŠ“å– Robots.txt ä¸­çš„ Sitemap", 
        "baidu_mode_label": "å¯ç”¨ç™¾åº¦ SEO å®¡è®¡æ¨¡å¼", 
        "allow_subdomains_label": "å…è®¸æŠ“å–å­åŸŸå (å¦‚ blog.site.com)",
        "allow_outside_folder_label": "å…è®¸æŠ“å–çˆ¶çº§ç›®å½• (å¦‚ä» /en/ å¼€å§‹æŠ“å– /fr/)",
        "manual_sitemaps": "æ‰‹åŠ¨ Sitemap åœ°å€ (æ¯è¡Œä¸€ä¸ª, è¡¥å……ç”¨)", 
        "manual_pages_label": "æ‰‹åŠ¨æ·»åŠ é¡µé¢åˆ—è¡¨ (æ¯è¡Œä¸€ä¸ª URL)",
        "sitemap_content_label": "ç²˜è´´ Sitemap XML å†…å®¹ (ç›´æ¥è§£æ)",
        "start_btn": "å¼€å§‹æ·±åº¦çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨æ‰§è¡Œæ·±åº¦å®¡è®¡ (Max {} pages)...", 
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚åŸå› : {}", 
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP Status Codes",
        "cwv_title": "é¦–é¡µæ ¸å¿ƒ Web æŒ‡æ ‡ (Core Web Vitals) - çœŸå®æ•°æ®",
        "cwv_source": "æ•°æ®æ¥æº: Google Chrome User Experience Report (CrUX) - ä»…é¦–é¡µ",
        "matrix_header": "çˆ¬å–æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼ŒåŒ…å«å¢å¼ºç‰ˆå¯è§†åŒ–é¢„è§ˆã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_category": "åˆ†ç±»:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "åœ¨å·²çˆ¬å–æ ·æœ¬ä¸­å‘ç° **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "ğŸ”´ é—®é¢˜æè¿°:",
        "ppt_business_impact": "ğŸ“‰ Business & SEO Impact:", 
        "ppt_sugg": "âœ… ä¿®å¤å»ºè®®:",
        "ppt_examples": "ğŸ” å—å½±å“é¡µé¢ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        "cat_access": "å¯è®¿é—®æ€§ä¸ç´¢å¼• (Access & Indexing)",
        "cat_indexability": "ç´¢å¼•è§„èŒƒæ€§ (Indexability)",
        "cat_technical": "æŠ€æœ¯ä¸æ¶æ„ (Technical SEO)",
        "cat_content": "é¡µé¢å†…å®¹ (On-Page Content)",
        "cat_image_ux": "ç”¨æˆ·ä½“éªŒä¸èµ„æº (UX & Assets)",
        "cat_cwv_performance": "æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ (Core Web Vitals)",
        "ppt_cover_title": "SEO æ·±åº¦æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro v14.12",
        "ppt_slide_desc_title": "æ·±åº¦åˆ†æ",
        "ppt_slide_count_title": "æ ·æœ¬ä¸­å—å½±å“é¡µé¢æ•°: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹", 
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
        "serp_sim_title": "Google æœç´¢ç»“æœæ¨¡æ‹Ÿ (SERP):",
        "rich_sim_title": "å¯Œåª’ä½“ç»“æœæ¨¡æ‹Ÿ (Rich Results):",
        "code_sim_title": "Code Snippet:",
        "visual_sim_title": "è§†è§‰ä½“éªŒæ¨¡æ‹Ÿ:",
        "cwv_sim_title": "CWV æ€§èƒ½ä»ªè¡¨ç›˜ (Performance):",
        "lcp_issue": "LCP (æœ€å¤§å†…å®¹ç»˜åˆ¶) è¶…æ ‡", "lcp_issue_desc": "LCP æ—¶é—´ä¸º {:.2f}s (ç›®æ ‡ <2.5s)ã€‚é¡µé¢ä¸»è¦å†…å®¹åŠ è½½è¿‡äºç¼“æ…¢ã€‚", "lcp_issue_impact": "LCP æ˜¯ Google æ ¸å¿ƒæ’åå› ç´ ã€‚åŠ è½½ç¼“æ…¢ä¼šå¯¼è‡´ç”¨æˆ·è·³å‡ºç‡é£™å‡ï¼Œå¹¶ç›´æ¥é™ä½åœ¨ç§»åŠ¨ç«¯çš„æœç´¢æ’åã€‚", "lcp_issue_sugg": "å‹ç¼©å›¾ç‰‡ä½“ç§¯ï¼ˆä½¿ç”¨ WebPï¼‰ï¼Œä½¿ç”¨ CDN åˆ†å‘å†…å®¹ï¼Œæ¨è¿Ÿéå…³é”® JS æ‰§è¡Œï¼Œå¹¶é¢„åŠ è½½ LCP å…³é”®å…ƒç´ ã€‚",
        "cls_issue": "CLS (ç´¯ç§¯å¸ƒå±€åç§») è¶…æ ‡", "cls_issue_desc": "é¡µé¢åŠ è½½è¿‡ç¨‹ä¸­å…ƒç´ å‘ç”Ÿæ„å¤–ä½ç§» (Score > 0.1)ã€‚", "cls_issue_impact": "ä½œä¸ºæ ¸å¿ƒæ’åå› ç´ ï¼Œå¸ƒå±€ä¸ç¨³å®šä¼šå¯¼è‡´ç”¨æˆ·è¯¯è§¦å¹¿å‘Šæˆ–æŒ‰é’®ï¼Œä¸¥é‡æŸå®³å“ç‰Œä¿¡èª‰å’Œç”¨æˆ·ä½“éªŒã€‚", "cls_issue_sugg": "ä¸ºæ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘å…ƒç´ æŒ‡å®šæ˜ç¡®çš„å®½åº¦å’Œé«˜åº¦å±æ€§ï¼Œé¿å…åœ¨é¡¶éƒ¨åŠ¨æ€æ’å…¥å†…å®¹ã€‚",
        "inp_issue": "INP (äº¤äº’åˆ°ç»˜åˆ¶å»¶è¿Ÿ) è¶…æ ‡", "inp_issue_desc": "ç”¨æˆ·ç‚¹å‡»æŒ‰é’®åï¼Œé¡µé¢å“åº”å»¶è¿Ÿè¶…è¿‡ 200msã€‚", "inp_issue_impact": "Google æ–°å¼•å…¥çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚é«˜å»¶è¿Ÿä¼šè®©ç”¨æˆ·è§‰å¾—ç½‘ç«™â€œå¡é¡¿â€æˆ–æ— å“åº”ï¼Œä¸¥é‡å½±å“è½¬åŒ–ç‡ã€‚", "inp_issue_sugg": "å‡å°‘ä¸»çº¿ç¨‹é˜»å¡ï¼Œå°†é•¿ä»»åŠ¡ (Long Tasks) æ‹†åˆ†ä¸ºå°ä»»åŠ¡ï¼Œå¹¶ä¼˜åŒ–å¤æ‚çš„ JavaScript äº‹ä»¶å¤„ç†é€»è¾‘ã€‚",
        "fcp_issue": "FCP (é¦–æ¬¡å†…å®¹ç»˜åˆ¶) ç¼“æ…¢", "fcp_issue_desc": "FCP æ—¶é—´ä¸º {:.2f}s (ç›®æ ‡ <1.8s)ã€‚ç”¨æˆ·çœ‹åˆ°é¡µé¢ç¬¬ä¸€ä¸ªå†…å®¹çš„æ—¶é—´è¿‡é•¿ã€‚", "fcp_issue_impact": "FCP æ…¢ä¼šè®©ç”¨æˆ·æ„Ÿè§‰æœåŠ¡å™¨å“åº”è¿Ÿé’ï¼Œç›´æ¥å¢åŠ è·³å‡ºç‡ã€‚", "fcp_issue_sugg": "ä¼˜åŒ–æœåŠ¡å™¨å“åº”æ—¶é—´ (TTFB)ï¼Œæ¶ˆé™¤é˜»å¡æ¸²æŸ“çš„ CSS/JS èµ„æºã€‚",
        "fcp_baidu_issue": "FCP ä¸æ»¡è¶³ç™¾åº¦ç§’å¼€è¦æ±‚", "fcp_baidu_issue_desc": "FCP æ—¶é—´ä¸º {:.2f}s (ç™¾åº¦ç›®æ ‡ < 2.0s)ã€‚", "fcp_baidu_issue_impact": "ç™¾åº¦å¯¹è½åœ°é¡µé¦–å±åŠ è½½é€Ÿåº¦æœ‰ä¸¥æ ¼è¦æ±‚ï¼Œè¶…è¿‡2ç§’å°†å¯¼è‡´ç§»åŠ¨æœç´¢é™æƒã€‚", "fcp_baidu_issue_sugg": "ä½¿ç”¨å›½å†… CDNï¼Œç²¾ç®€é¦–å±èµ„æºï¼Œç¡®ä¿ FCP < 2sã€‚",
        "no_robots": "ç¼ºå¤± Robots.txt", "no_robots_desc": "æ— æ³•è®¿é—®æ ¹ç›®å½•çš„ robots.txt æ–‡ä»¶ï¼Œæˆ–è€…æœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç ã€‚", "no_robots_impact": "çˆ¬è™«å¯èƒ½æŠ“å–æ— ç”¨çš„åå°é¡µé¢ï¼Œä¸ä»…æ¶ˆè€—æœåŠ¡å™¨èµ„æºï¼Œè¿˜ä¼šæµªè´¹å®è´µçš„çˆ¬å–é¢„ç®—ã€‚", "no_robots_sugg": "åœ¨ç½‘ç«™æ ¹ç›®å½•åˆ›å»ºæ ‡å‡†çš„ robots.txt æ–‡ä»¶ï¼Œå¹¶ç¡®ä¿å…¶å¯¹æœç´¢å¼•æ“çˆ¬è™«å…¬å¼€å¯è§ã€‚",
        "robots_bad_rule": "Robots.txt å°ç¦é£é™©", "robots_bad_rule_desc": "æ£€æµ‹åˆ°å…¨ç«™å°ç¦è§„åˆ™ (Disallow: /)ï¼Œä¸”æœªå‘ç°é’ˆå¯¹ Googlebot çš„ä¾‹å¤–è§„åˆ™ã€‚", "robots_bad_rule_impact": "è¿™å°†ç›´æ¥å¯¼è‡´æœç´¢å¼•æ“åœæ­¢æŠ“å–å¹¶ç´¢å¼•æ‚¨çš„ç½‘ç«™ï¼Œæ‰€æœ‰è‡ªç„¶æœç´¢æµé‡å°†å½’é›¶ã€‚", "robots_bad_rule_sugg": "ç«‹å³ç§»é™¤ 'Disallow: /' è§„åˆ™ï¼Œæˆ–è€…ä¸ºæœç´¢å¼•æ“çˆ¬è™«æ·»åŠ å…·ä½“çš„ 'Allow' è§„åˆ™ã€‚",
        "robots_quality_issue": "Robots.txt è§„åˆ™é…ç½®ä¸å½“", "robots_quality_issue_desc": "Robots.txt æ–‡ä»¶å­˜åœ¨æ½œåœ¨é—®é¢˜ï¼š{}ã€‚", "robots_quality_issue_impact": "å¯èƒ½å¯¼è‡´Googlebotè¡Œä¸ºå¼‚å¸¸ï¼ˆå¦‚è¯¯åˆ¤å±è”½æˆ–æ¸²æŸ“å¤±è´¥ï¼‰ã€‚", "robots_quality_issue_sugg": "æ£€æŸ¥ Robots.txtï¼Œç§»é™¤åºŸå¼ƒæŒ‡ä»¤ï¼ˆå¦‚ Noindexï¼‰ï¼Œå¹¶ç¡®ä¿å…è®¸è®¿é—® CSS/JS èµ„æºã€‚",
        "robots_no_sitemap": "Robots æœªå£°æ˜ Sitemap", "robots_no_sitemap_desc": "robots.txt æ–‡ä»¶ä¸­æœªæŒ‡æ˜ Sitemap XML æ–‡ä»¶çš„ä½ç½®ã€‚", "robots_no_sitemap_impact": "ä¼šé™ä½æœç´¢å¼•æ“å‘ç°æ–°é¡µé¢å’Œæ›´æ–°æ—§å†…å®¹çš„é€Ÿåº¦ï¼Œå°¤å…¶å¯¹äºå¤§å‹ç½‘ç«™å½±å“æ›´æ˜æ˜¾ã€‚", "robots_no_sitemap_sugg": "åœ¨ robots.txt æ–‡ä»¶åº•éƒ¨æ·»åŠ ä¸€è¡Œï¼šSitemap: https://yourdomain.com/sitemap.xml",
        "no_sitemap": "Sitemap è®¿é—®å¤±è´¥", "no_sitemap_desc": "æ— æ³•è®¿é—® Sitemap æ–‡ä»¶ï¼ŒæœåŠ¡å™¨è¿”å› 4xx æˆ– 5xx é”™è¯¯ã€‚", "no_sitemap_impact": "æœç´¢å¼•æ“éš¾ä»¥å‘ç°æ·±å±‚é“¾æ¥æˆ–å­¤å²›é¡µé¢ï¼Œå¯¼è‡´æ•´ä½“æ”¶å½•ç‡ä¸‹é™ã€‚", "no_sitemap_sugg": "æ£€æŸ¥ Sitemap æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä»¥åŠæœåŠ¡å™¨æƒé™è®¾ç½®æ˜¯å¦å…è®¸å¤–éƒ¨è®¿é—®ã€‚",
        "sitemap_invalid": "Sitemap æ ¼å¼é”™è¯¯", "sitemap_invalid_desc": "XML è§£æå¤±è´¥ï¼Œæ–‡ä»¶æ ¼å¼ä¸ç¬¦åˆæ ‡å‡†åè®®ã€‚", "sitemap_invalid_impact": "æœç´¢å¼•æ“æ— æ³•è¯»å–å…¶ä¸­çš„é“¾æ¥ï¼Œå¯¼è‡´ Sitemap å®Œå…¨å¤±æ•ˆã€‚", "sitemap_invalid_sugg": "ä½¿ç”¨ XML éªŒè¯å·¥å…·æ£€æŸ¥æ–‡ä»¶è¯­æ³•ï¼Œç¡®ä¿æ²¡æœ‰æœªé—­åˆçš„æ ‡ç­¾æˆ–éæ³•å­—ç¬¦ã€‚",
        "no_favicon": "ç¼ºå¤± Favicon", "no_favicon_desc": "æœªåœ¨é¦–é¡µæ£€æµ‹åˆ° Favicon å›¾æ ‡ã€‚", "no_favicon_impact": "é™ä½å“ç‰Œåœ¨æµè§ˆå™¨æ ‡ç­¾é¡µå’Œæœç´¢ç»“æœé¡µ (SERP) ä¸­çš„è¾¨è¯†åº¦ï¼Œè¿›è€Œå¯¼è‡´ç‚¹å‡»ç‡ (CTR) ä¸‹é™ã€‚", "no_favicon_sugg": "åˆ¶ä½œä¸€ä¸ª .ico æˆ– .png æ ¼å¼çš„å›¾æ ‡ï¼Œå¹¶åœ¨ <head> ä¸­é€šè¿‡ <link rel='icon'> å¼•ç”¨ã€‚",
        "duplicate": "å‘ç°æœªè§„èŒƒåŒ–çš„é‡å¤å†…å®¹", "duplicate_desc": "æ£€æµ‹åˆ°é«˜åº¦ç›¸ä¼¼çš„å†…å®¹é¡µé¢ï¼Œä¸”æœªæ­£ç¡®é…ç½® Canonical æ ‡ç­¾ã€‚", "duplicate_impact": "å¯¼è‡´å…³é”®è¯å†…éƒ¨ç«äº‰ (Cannibalization)ï¼Œåˆ†æ•£é¡µé¢æƒé‡ï¼Œä½¿æ‰€æœ‰ç›¸å…³é¡µé¢éƒ½éš¾ä»¥è·å¾—é«˜æ’åã€‚", "duplicate_sugg": "ä¿ç•™ä¸€ä¸ªé¦–é€‰ URLï¼Œå¹¶åœ¨å…¶ä»–å‰¯æœ¬é¡µé¢ä¸Šæ·»åŠ  rel='canonical' æŒ‡å‘è¯¥é¦–é€‰ URLã€‚",
        "http_3xx": "å†…éƒ¨é“¾æ¥é‡å®šå‘ (3xx)", "http_3xx_desc": "å†…éƒ¨é“¾æ¥å‘ç”Ÿè·³è½¬ (é“¾æ¡: {})ã€‚", "http_3xx_impact": "æµªè´¹çˆ¬è™«é¢„ç®—ï¼Œå¢åŠ é¡µé¢åŠ è½½å»¶è¿Ÿï¼Œä¸”æ¯æ¬¡è·³è½¬éƒ½ä¼šæŸè€—å°‘é‡é“¾æ¥ä¼ é€’çš„æƒé‡ (Link Equity)ã€‚", "http_3xx_sugg": "æ‰¹é‡æ›´æ–°å†…éƒ¨é“¾æ¥ï¼Œä½¿å…¶ç›´æ¥æŒ‡å‘æœ€ç»ˆçš„ç›®æ ‡ URLï¼Œé¿å…ä¸­é—´è·³è½¬ã€‚",
        "http_4xx": "æ­»é“¾/å®¢æˆ·ç«¯é”™è¯¯ (4xx)", "http_4xx_desc": "å†…éƒ¨é“¾æ¥è¿”å› 404 (æœªæ‰¾åˆ°) æˆ– 403 (ç¦æ­¢è®¿é—®) é”™è¯¯ã€‚", "http_4xx_impact": "ä¸¥é‡ç ´åç”¨æˆ·ä½“éªŒï¼Œä¸­æ–­æƒé‡ä¼ é€’è·¯å¾„ï¼Œå¹¶å¯èƒ½å¯¼è‡´å·²ç´¢å¼•çš„é¡µé¢è¢« Google ç§»é™¤ã€‚", "http_4xx_sugg": "ç§»é™¤æ­»é“¾ï¼Œæˆ–è€…å°†å…¶é‡å®šå‘åˆ°æœ€ç›¸å…³çš„æœ‰æ•ˆé¡µé¢ã€‚",
        "http_5xx": "æœåŠ¡å™¨é”™è¯¯ (5xx)", "http_5xx_desc": "æœåŠ¡å™¨å“åº” 500/502/503 ç­‰å†…éƒ¨é”™è¯¯ã€‚", "http_5xx_impact": "è¡¨æ˜æœåŠ¡å™¨æå…¶ä¸ç¨³å®šï¼ŒGooglebot ä¼šå› æ­¤é™ä½å¯¹è¯¥ç«™ç‚¹çš„çˆ¬å–é¢‘ç‡ä»¥å‡è½»è´Ÿè½½ã€‚", "http_5xx_sugg": "æ£€æŸ¥æœåŠ¡å™¨é”™è¯¯æ—¥å¿—ï¼Œä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æˆ–å‡çº§æœåŠ¡å™¨é…ç½®ã€‚",
        "hreflang_invalid": "Hreflang æ ¼å¼é”™è¯¯", "hreflang_invalid_desc": "è¯­è¨€ä»£ç ä¸ç¬¦åˆ ISO 639-1 æ ‡å‡† (å¦‚ä½¿ç”¨äº† {} ç­‰é”™è¯¯æ ¼å¼)ã€‚", "hreflang_invalid_impact": "Google æ— æ³•è¯†åˆ«ç›®æ ‡è¯­è¨€ï¼Œå¯¼è‡´å›½é™…åŒ–å®šä½å¤±æ•ˆã€‚", "hreflang_invalid_sugg": "ä½¿ç”¨æ ‡å‡†çš„ ISO è¯­è¨€ä»£ç  (ä¾‹å¦‚ 'en-US' è€Œä¸æ˜¯ 'en_US')ã€‚",
        "hreflang_no_default": "Hreflang ç¼ºå¤± x-default", "hreflang_no_default_desc": "æœªé…ç½® 'x-default' å›é€€ç‰ˆæœ¬ã€‚", "hreflang_no_default_impact": "å½“ç”¨æˆ·æ¥è‡ªæœªæŒ‡å®šçš„è¯­è¨€/åœ°åŒºæ—¶ï¼Œå¯èƒ½æ— æ³•è‡ªåŠ¨åŒ¹é…åˆ°æœ€åˆé€‚çš„é€šç”¨ç‰ˆæœ¬ï¼ˆé€šå¸¸æ˜¯è‹±è¯­ï¼‰ã€‚", "hreflang_no_default_sugg": "æ·»åŠ  hreflang='x-default' æ ‡ç­¾ï¼ŒæŒ‡å®šé»˜è®¤çš„è¯­è¨€ç‰ˆæœ¬ã€‚",
        "alt_bad_quality": "å›¾ç‰‡ Alt è´¨é‡å·®", "alt_bad_quality_desc": "Alt æ–‡æœ¬ä½¿ç”¨äº†æ— æ„ä¹‰è¯æ±‡ï¼ˆå¦‚ image1.jpg, photoï¼‰æˆ–è¿‡çŸ­ã€‚", "alt_bad_quality_impact": "æœç´¢å¼•æ“æ— æ³•ç†è§£å›¾ç‰‡å†…å®¹ï¼Œé”™å¤±å›¾ç‰‡æœç´¢æµé‡ï¼Œä¸”å¯¹è§†éšœç”¨æˆ·æä¸å‹å¥½ã€‚", "alt_bad_quality_sugg": "ä½¿ç”¨æè¿°æ€§æ–‡æœ¬å‡†ç¡®æè¿°å›¾ç‰‡å†…å®¹ï¼ŒåŒ…å«ç›¸å…³çš„å…³é”®è¯ã€‚",
        "anchor_bad_quality": "é”šæ–‡æœ¬è´¨é‡å·®", "anchor_bad_quality_desc": "ä½¿ç”¨äº†â€œç‚¹å‡»è¿™é‡Œâ€ã€â€œæ›´å¤šâ€ç­‰é€šç”¨è¯æ±‡ä½œä¸ºé“¾æ¥æ–‡æœ¬ã€‚", "anchor_bad_quality_impact": "æ— æ³•å‘æœç´¢å¼•æ“ä¼ é€’ç›®æ ‡é¡µé¢çš„å…³é”®è¯ç›¸å…³æ€§ï¼Œé™ä½äº†ç›®æ ‡é¡µé¢çš„æ’åæ½œåŠ›ã€‚", "anchor_bad_quality_sugg": "ä½¿ç”¨æè¿°æ€§ keywords in the anchor text.",
        "cls_risk": "CLS å¸ƒå±€åç§»é£é™© (é™æ€æ£€æµ‹)", "cls_risk_desc": "æ£€æµ‹åˆ° <img> æ ‡ç­¾ç¼ºå¤± width æˆ– height å±æ€§ã€‚", "cls_risk_impact": "å›¾ç‰‡åŠ è½½æ—¶ä¼šæ’‘å¼€é¡µé¢ï¼Œå¯¼è‡´å¸ƒå±€å‘ç”Ÿæ„å¤–æŠ–åŠ¨ï¼Œç›´æ¥æ¶åŒ– CLS æŒ‡æ ‡ã€‚", "cls_risk_sugg": "åœ¨ HTML ä¸­æ˜¾å¼æŒ‡å®šå›¾ç‰‡å’Œè§†é¢‘çš„å®½åº¦å’Œé«˜åº¦å±æ€§ã€‚",
        "missing_title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title)", "missing_title_desc": "é¡µé¢ä»£ç ä¸­æœªæ‰¾åˆ° <title> æ ‡ç­¾ã€‚", "missing_title_impact": "Title æ˜¯æœ€é‡è¦çš„ SEO æ ‡ç­¾ã€‚ç¼ºå¤±å°†å¯¼è‡´æœç´¢å¼•æ“æ— æ³•åˆ¤æ–­é¡µé¢ä¸»é¢˜ï¼Œå…³é”®è¯æ’åæå·®ã€‚", "missing_title_sugg": "ä¸ºæ¯ä¸ªé¡µé¢æ·»åŠ ç‹¬ç‰¹ã€åŒ…å«æ ¸å¿ƒå…³é”®è¯çš„æ ‡é¢˜ã€‚",
        "short_title": "æ ‡é¢˜è¿‡çŸ­", "short_title_desc": "æ ‡é¢˜é•¿åº¦ä¸è¶³ (çº¦ {} px)ï¼Œéš¾ä»¥å®Œæ•´è¡¨è¾¾é¡µé¢æ„å›¾ã€‚", "short_title_impact": "æµªè´¹äº†å®è´µçš„æ ‡é¢˜ç©ºé—´ï¼Œé”™å¤±äº†è¦†ç›–é•¿å°¾å…³é”®è¯æ’åçš„æœºä¼šã€‚", "short_title_sugg": "ä¸°å¯Œæ ‡é¢˜å†…å®¹ï¼ŒåŠ å…¥å“ç‰Œè¯æˆ–ä¿®é¥°è¯ï¼Œå»ºè®®é•¿åº¦åœ¨ 285-575 px ä¹‹é—´ã€‚",
        "long_title": "æ ‡é¢˜è¿‡é•¿", "long_title_desc": "æ ‡é¢˜è¶…è¿‡å»ºè®®æ˜¾ç¤ºå®½åº¦ (çº¦ {} px)ã€‚", "long_title_impact": "æ ‡é¢˜å°†åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­ï¼Œé™ä½å¯è¯»æ€§å’Œç‚¹å‡»ç‡ã€‚", "long_title_sugg": "ç²¾ç®€æ ‡é¢˜é•¿åº¦ï¼Œå°†æ ¸å¿ƒä¿¡æ¯å‰ç½®ï¼Œæ§åˆ¶åœ¨ 600 px ä»¥å†…ã€‚",
        "missing_desc": "ç¼ºå¤±å…ƒæè¿°", "missing_desc_desc": "é¡µé¢æœªåŒ…å« <meta name='description'> æ ‡ç­¾ã€‚", "missing_desc_impact": "è™½ç„¶ä¸ç›´æ¥å½±å“æ’åï¼Œä½† Google ä¼šéšæœºæŠ“å–æ­£æ–‡ä½œä¸ºæ‘˜è¦ï¼Œé€šå¸¸ä¸å¯æ§ä¸”ç‚¹å‡»ç‡ä½ã€‚", "missing_desc_sugg": "æ·»åŠ å¸å¼•äººçš„å…ƒæè¿°ï¼Œæ¦‚æ‹¬é¡µé¢å†…å®¹å¹¶åŒ…å«å·å¬æ€§ç”¨è¯­ã€‚",
        "short_desc": "å…ƒæè¿°è¿‡çŸ­", "short_desc_desc": "å†…å®¹è¿‡å°‘ (çº¦ {} px)ï¼Œå¸å¼•åŠ›ä¸è¶³ã€‚", "short_desc_impact": "æ— æ³•å……åˆ†å±•ç¤ºé¡µé¢å–ç‚¹ï¼Œåœ¨æœç´¢ç»“æœä¸­ç¼ºä¹ç«äº‰åŠ›ã€‚", "short_desc_sugg": "æ‰©å……æè¿°è‡³ 400-920 pxï¼Œæä¾›æ›´å¤šæœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚",
        "missing_h1": "ç¼ºå¤± H1 æ ‡ç­¾", "missing_h1_desc": "é¡µé¢ç¼ºä¹ <h1> ä¸»æ ‡é¢˜ã€‚", "missing_h1_impact": "æœç´¢å¼•æ“éš¾ä»¥ç†è§£å†…å®¹çš„å±‚çº§ç»“æ„å’Œæ ¸å¿ƒä¸»é¢˜ï¼Œé™ä½äº†å…³é”®è¯çš„ç›¸å…³æ€§æƒé‡ã€‚", "missing_h1_sugg": "ç¡®ä¿æ¯ä¸ªé¡µé¢æœ‰ä¸”ä»…æœ‰ä¸€ä¸ª H1 æ ‡ç­¾ï¼Œæ¦‚æ‹¬å½“å‰é¡µé¢çš„ä¸»é¢˜ã€‚",
        "missing_viewport": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®", "missing_viewport_desc": "æœªé…ç½® <meta name='viewport'> æ ‡ç­¾ã€‚", "missing_viewport_impact": "åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ˜¾ç¤ºå¼‚å¸¸ï¼ˆå­—ä½“æå°ï¼‰ã€‚Google ç§»åŠ¨ä¼˜å…ˆç´¢å¼•ä¼šä¸¥é‡æƒ©ç½šæ­¤ç±»é¡µé¢ã€‚", "missing_viewport_sugg": "åœ¨ <head> ä¸­æ·»åŠ æ ‡å‡†çš„ viewport meta æ ‡ç­¾ã€‚",
        "missing_canonical": "ç¼ºå¤± Canonical æ ‡ç­¾", "missing_canonical_desc": "æœªæŒ‡å®šè§„èŒƒé“¾æ¥ã€‚", "missing_canonical_impact": "æ— æ³•åº”å¯¹ URL å‚æ•°ï¼ˆå¦‚ ?id=1ï¼‰å¯¼è‡´çš„é‡å¤å†…å®¹é—®é¢˜ï¼Œå®¹æ˜“é€ æˆæƒé‡ç¨€é‡Šã€‚", "missing_canonical_sugg": "åœ¨æ‰€æœ‰é¡µé¢æ·»åŠ è‡ªå¼•ç”¨ï¼ˆSelf-referencingï¼‰æˆ–æŒ‡å‘åŸä»¶çš„ Canonical æ ‡ç­¾ã€‚",
        "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®", "missing_jsonld_desc": "æœªæ£€æµ‹åˆ° Schema.org æ ‡è®°ã€‚", "missing_jsonld_impact": "é”™å¤±å¯Œåª’ä½“æœç´¢ç»“æœï¼ˆRich Resultsï¼‰ï¼Œåœ¨ SERP ä¸­ä¸å¦‚ç«äº‰å¯¹æ‰‹æ˜¾çœ¼ã€‚", "missing_jsonld_sugg": "å»ºè®®é…ç½®ç»“æ„åŒ–æ•°æ®ã€‚åŸºäºé¡µé¢å†…å®¹ï¼Œæ¨èæ·»åŠ ï¼š{}ã€‚",
        "missing_hreflang": "ç¼ºå¤± Hreflang", "missing_hreflang_desc": "æœªå‘ç°è¯­è¨€åŒºåŸŸæ ‡è®°ï¼ˆHTMLæˆ–Sitemapä¸­å‡æ— ï¼‰ã€‚", "missing_hreflang_impact": "å¤šè¯­è¨€ç«™ç‚¹æ— æ³•æ­£ç¡®å®šä½ç›®æ ‡å—ä¼—ï¼Œå¯¼è‡´æµé‡ä¸ç²¾å‡†ã€‚", "missing_hreflang_sugg": "åœ¨ HTML å¤´éƒ¨æˆ– Sitemap ä¸­é…ç½® hreflang æ ‡ç­¾ã€‚",
        "soft_404": "ç–‘ä¼¼è½¯ 404 (Soft 404)", "soft_404_desc": "é¡µé¢è¿”å› 200 çŠ¶æ€ç ä½†å†…å®¹æ˜¾ç¤ºâ€œæœªæ‰¾åˆ°â€ã€‚", "soft_404_impact": "ä¸¥é‡æµªè´¹çˆ¬è™«é¢„ç®—ï¼Œå¯¼è‡´æ— æ•ˆé¡µé¢æŒ¤å æœ‰æ•ˆé¡µé¢çš„ç´¢å¼•åé¢ã€‚", "soft_404_sugg": "é…ç½®æœåŠ¡å™¨å¯¹ä¸å­˜åœ¨çš„é¡µé¢è¿”å› 404 HTTP çŠ¶æ€ç ã€‚",
        "missing_alt": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§", "missing_alt_desc": "å›¾ç‰‡æ ‡ç­¾ç¼ºå°‘ alt å±æ€§ã€‚", "missing_alt_impact": "æœç´¢å¼•æ“æ— æ³•ç†è§£å›¾ç‰‡å†…å®¹ï¼Œé”™å¤±å›¾ç‰‡æœç´¢æµé‡ã€‚", "missing_alt_sugg": "ä¸ºæ‰€æœ‰æœ‰æ„ä¹‰çš„å›¾ç‰‡æ·»åŠ æè¿°æ€§çš„ alt å±æ€§ã€‚",
        "js_links": "å‘ç° JS ä¼ªé“¾æ¥", "js_links_desc": "ä½¿ç”¨äº† href='javascript:...' å½¢å¼çš„é“¾æ¥ã€‚", "js_links_impact": "çˆ¬è™«æ— æ³•è·Ÿè¸ªæ­¤ç±»é“¾æ¥ï¼Œå¯¼è‡´å†…éƒ¨é“¾æ¥æ–­è£‚ï¼Œæ·±å±‚é¡µé¢å˜æˆâ€œå­¤å²›â€ã€‚", "js_links_sugg": "ä½¿ç”¨æ ‡å‡†çš„ <a href> æ ‡ç­¾ï¼Œä»…åœ¨ onclick äº‹ä»¶ä¸­å¤„ç† JS é€»è¾‘ã€‚",
        "url_underscore": "URL åŒ…å«ä¸‹åˆ’çº¿", "url_underscore_desc": "URL è·¯å¾„ä¸­ä½¿ç”¨ä¸‹åˆ’çº¿ (_) åˆ†éš”å•è¯ã€‚", "url_underscore_impact": "Google å»ºè®®ä½¿ç”¨è¿å­—ç¬¦ã€‚ä¸‹åˆ’çº¿å¯èƒ½å¯¼è‡´å…³é”®è¯æ— æ³•è¢«æ­£ç¡®åˆ‡åˆ†ï¼ˆè¢«è§†ä¸ºä¸€ä¸ªé•¿å•è¯ï¼‰ã€‚", "url_underscore_sugg": "åœ¨ URL ç»“æ„ä¸­ä½¿ç”¨è¿å­—ç¬¦ (-) ä»£æ›¿ä¸‹åˆ’çº¿ã€‚",
        "url_uppercase": "URL åŒ…å«å¤§å†™å­—æ¯", "url_uppercase_desc": "URL è·¯å¾„ä¸­æ··ç”¨äº†å¤§å†™å­—æ¯ã€‚", "url_uppercase_impact": "æœåŠ¡å™¨é€šå¸¸åŒºåˆ†å¤§å°å†™ï¼Œææ˜“é€ æˆä¸€é¡µå¤šå€ï¼ˆDuplicate Contentï¼‰å’Œ 404 é”™è¯¯ã€‚", "url_uppercase_sugg": "å¼ºåˆ¶æ‰€æœ‰ URL ä½¿ç”¨å°å†™å­—æ¯ã€‚",
        "missing_keywords": "ç¼ºå¤± Meta Keywords (ç™¾åº¦ä¸“ç”¨)",
        "missing_keywords_desc": "é¡µé¢æœªåŒ…å« <meta name='keywords'> æ ‡ç­¾ã€‚",
        "missing_keywords_impact": "è™½ç„¶ Google å·²å¿½ç•¥æ­¤æ ‡ç­¾ï¼Œä½†ç™¾åº¦ä»å°†å…¶ä½œä¸ºç›¸å…³æ€§ä¿¡å·ä¹‹ä¸€ã€‚",
        "missing_keywords_sugg": "æ·»åŠ åŒ…å« 3-5 ä¸ªæ ¸å¿ƒå…³é”®è¯çš„ meta keywords æ ‡ç­¾ã€‚",
        "missing_baidu_stats": "ç¼ºå¤±ç™¾åº¦ç»Ÿè®¡ä»£ç ",
        "missing_baidu_stats_desc": "é¡µé¢ä»£ç ä¸­æœªå‘ç° 'hm.baidu.com' ç›¸å…³è„šæœ¬ã€‚",
        "missing_baidu_stats_impact": "æ— æ³•å‡†ç¡®è¿½è¸ªæ¥è‡ªç™¾åº¦çš„è‡ªç„¶æœç´¢æµé‡å’Œç”¨æˆ·è¡Œä¸ºã€‚",
        "missing_baidu_stats_sugg": "å®‰è£…ç™¾åº¦ç»Ÿè®¡ (Baidu Tongji) ä»£ç ã€‚",
        "missing_baidu_verify": "ç¼ºå¤±ç™¾åº¦ç«™é•¿éªŒè¯",
        "missing_baidu_verify_desc": "æœªæ£€æµ‹åˆ° 'baidu-site-verification' æ ‡ç­¾ã€‚",
        "missing_baidu_verify_impact": "å¯èƒ½å½±å“æ–°ç«™æ”¶å½•é€Ÿåº¦å’Œç«™é•¿å¹³å°æ•°æ®è·å–ã€‚",
        "missing_baidu_verify_sugg": "åœ¨ HTML å¤´éƒ¨æ·»åŠ ç™¾åº¦éªŒè¯æ ‡ç­¾ã€‚",
        "baidu_robots_missing": "Robots.txt ç¼ºå°‘ç™¾åº¦è§„åˆ™",
        "baidu_robots_missing_desc": "æœªå‘ç°é’ˆå¯¹ 'Baiduspider' çš„ä¸“é—¨ User-agent æŒ‡ä»¤ã€‚",
        "baidu_robots_missing_impact": "ç™¾åº¦çˆ¬è™«å¯èƒ½æŠ“å–æ•ˆç‡ä½ä¸‹æˆ–æŠ“å–æ— ç”¨é¡µé¢ï¼Œæµªè´¹æœåŠ¡å™¨èµ„æºã€‚",
        "baidu_robots_missing_sugg": "æ·»åŠ  'User-agent: Baiduspider' å¹¶é…ç½®åˆç†çš„ Disallow è§„åˆ™ã€‚",
        "missing_applicable_device": "ç¼ºå¤±ç§»åŠ¨é€‚é…æ ‡ç­¾ (ç™¾åº¦)",
        "missing_applicable_device_desc": "æœªæ£€æµ‹åˆ° 'applicable-device' meta æ ‡ç­¾ã€‚",
        "missing_applicable_device_impact": "ç™¾åº¦æ— æ³•å‡†ç¡®è¯†åˆ«é¡µé¢é€‚é…çš„è®¾å¤‡ç±»å‹ï¼ˆPC/ç§»åŠ¨ï¼‰ï¼Œå½±å“ç§»åŠ¨ç«¯æ’åã€‚",
        "missing_applicable_device_sugg": "æ·»åŠ  <meta name='applicable-device' content='pc,mobile'>ã€‚",
        "missing_no_transform": "ç¼ºå¤±è½¬ç æ§åˆ¶æ ‡ç­¾ (ç™¾åº¦)",
        "missing_no_transform_desc": "æœªæ£€æµ‹åˆ° 'Cache-Control: no-transform'ã€‚",
        "missing_no_transform_impact": "ç™¾åº¦å¯èƒ½ä¼šå¯¹é¡µé¢è¿›è¡Œå¼ºåˆ¶è½¬ç ï¼Œç ´åé¡µé¢åŸæœ‰çš„å¸ƒå±€å’Œå¹¿å‘Šã€‚",
        "missing_no_transform_sugg": "æ·»åŠ  <meta http-equiv='Cache-Control' content='no-transform'>.",
        "missing_icp": "ç¼ºå¤± ICP å¤‡æ¡ˆå·",
        "missing_icp_desc": "é¡µé¢æºä»£ç ä¸­æœªæ£€æµ‹åˆ° ICP å¤‡æ¡ˆå· (å¦‚ 'äº¬ICPå¤‡')ã€‚",
        "missing_icp_impact": "æ ¹æ®ä¸­å›½æ³•å¾‹ï¼Œå¢ƒå†…æ‰˜ç®¡ç½‘ç«™å¿…é¡»åœ¨é¡µé¢åº•éƒ¨å…¬ç¤º ICP å¤‡æ¡ˆå·ï¼Œå¦åˆ™å¯èƒ½è¢«å…³åœæˆ–é™æƒã€‚",
        "missing_icp_sugg": "åœ¨é¡µè„šæ·»åŠ å·¥ä¿¡éƒ¨ ICP å¤‡æ¡ˆå·å¹¶é“¾æ¥è‡³ beian.miit.gov.cnã€‚",
        "content_not_chinese": "é¡µé¢ä¸­æ–‡å†…å®¹å æ¯”è¿‡ä½",
        "content_not_chinese_desc": "æ£€æµ‹åˆ°é¡µé¢ä¸­æ–‡å­—ç¬¦å æ¯”ä½äº 5%ã€‚",
        "content_not_chinese_impact": "ç™¾åº¦ä¼˜å…ˆæ”¶å½•ä¸­æ–‡å†…å®¹ã€‚éä¸­æ–‡é¡µé¢åœ¨ç™¾åº¦ä¸­æ–‡æœç´¢ä¸­çš„æ’åèƒ½åŠ›æå¼±ã€‚",
        "content_not_chinese_sugg": "ç¡®ä¿ç›®æ ‡é¡µé¢çš„æ ¸å¿ƒå†…å®¹ä¸ºç®€ä½“ä¸­æ–‡ã€‚",
        "server_not_in_china": "æœåŠ¡å™¨ä¸åœ¨ä¸­å›½å¤§é™† (ç™¾åº¦å»ºè®®)",
        "server_not_in_china_desc": "æ£€æµ‹åˆ°æœåŠ¡å™¨ IP ä½äºï¼š{}ã€‚ç™¾åº¦ä¼˜å…ˆæ”¶å½•å’Œæ’åä¸­å›½å¤§é™†æœåŠ¡å™¨çš„ç½‘ç«™ã€‚",
        "server_not_in_china_impact": "è·¨å¢ƒåŠ è½½é€Ÿåº¦æ…¢ï¼Œææ˜“å¯¼è‡´ç™¾åº¦çˆ¬è™«æŠ“å–è¶…æ—¶æˆ–æ”¾å¼ƒï¼Œä¸¥é‡å½±å“æ’åã€‚",
        "server_not_in_china_sugg": "å»ºè®®å°†æœåŠ¡å™¨è¿ç§»è‡³ä¸­å›½å¤§é™†ï¼Œå¹¶å®Œæˆ ICP å¤‡æ¡ˆã€‚"
    },
    "en": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "Deep Audit Edition v14.12",
        "nav_label": "Navigation",
        "nav_options": ["Input URL", "Dashboard", "Data Matrix", "PPT Generator"],
        "lang_label": "Language / è¯­è¨€",
        "clear_data": "Clear Data & Reset",
        "cache_info": "Cached {} pages",
        "sitemap_status_title": "Sitemap Status:",
        "sitemap_found_href": "âœ… Hreflang Found", 
        "sitemap_no_href": "âš ï¸ No Hreflang",       
        "sitemap_missing": "âŒ Sitemap Missing",
        
        # PSI Related
        "psi_settings": "Google PSI API Settings (Optional)",
        "psi_api_key_label": "Enter Google PageSpeed API Key",
        "psi_api_help": "Enter API Key to fetch Real User Metrics (LCP, CLS, INP) for the home page. Leave empty for code-only check.",
        "psi_list_url_label": "Product List URL (Optional)", 
        "psi_detail_url_label": "Product Detail URL (Optional)", 
        "psi_get_key": "No API Key? [Get one for free here](https://developers.google.com/speed/docs/insights/v5/get-started)",
        "psi_fetching": "Fetching real CWV data from Google API ({}) ...",
        "psi_success": "Real user data fetched successfully!",
        "psi_error": "API Failed or No CrUX Data",
        
        "input_header": "Start Deep Audit",
        "input_info": "Note: v14.12 features fixed indentation and Baidu SEO checks.",
        "input_label": "Target URL",
        "input_placeholder": "https://example.com",
        "max_pages_label": "Max Pages to Crawl",
        "adv_settings": "Advanced Settings", 
        "check_robots_label": "Check & Respect Robots.txt", 
        "crawl_sitemap_label": "Parse Sitemap from Robots.txt", 
        "baidu_mode_label": "Enable Baidu SEO Audit Mode", 
        "allow_subdomains_label": "Allow Subdomains (e.g. blog.site.com)", 
        "allow_outside_folder_label": "Allow Outside Start Folder (e.g. /fr/ from /en/)", 
        "manual_sitemaps": "Manual Sitemap URLs (One per line, Optional)", 
        "manual_pages_label": "Manual Pages to Audit (One per line)",
        "sitemap_content_label": "Paste Sitemap XML Content (Direct Parse)",
        "start_btn": "Start Deep Crawl",
        "error_url": "Invalid URL format",
        "spinner_crawl": "Running Deep Audit (Max {} pages)...", 
        "error_no_data": "No pages crawled. Reason: {}", 
        "success_audit": "Audit Complete! Analyzed {} pages.",
        
        "dashboard_header": "Executive Summary",
        "warn_no_data": "No data available.",
        "kpi_health": "Health Score",
        "kpi_pages": "Analyzed Pages",
        "kpi_issues": "Total Issues",
        "kpi_critical": "Critical Issues",
        "chart_issues": "Issue Distribution",
        "chart_no_issues": "No significant issues found.",
        "chart_status": "HTTP Status Codes",
        "cwv_title": "Core Web Vitals - Real User Data (Home Only)",
        "cwv_source": "Source: Google Chrome User Experience Report (CrUX)",
        
        "matrix_header": "Crawled Data Matrix",
        "download_csv": "Download CSV Report",
        
        "ppt_header": "Pitch Deck Preview",
        "ppt_success_no_issues": "No critical issues found.",
        "ppt_download_header": "ğŸ“¥ Export Report",
        "ppt_info": "Note: PPT optimized for 16:9 with logical issue ordering.",
        "ppt_btn": "Generate & Download .pptx",
        "ppt_preview_header": "Web Preview",
        "ppt_slide_title": "Issue Type:",
        "ppt_category": "Category:",
        "ppt_severity": "Severity:",
        "ppt_impact": "Impact:",
        "ppt_impact_desc": "Affects **{}** pages in crawled sample.",
        "ppt_desc": "Description:",
        "ppt_business_impact": "ğŸ“‰ Business & SEO Impact:",
        "ppt_sugg": "ğŸ’¡ Suggestion:",
        "ppt_examples": "ğŸ” Examples:",
        "ppt_prev": "â¬…ï¸ Previous",
        "ppt_next": "Next â¡ï¸",
        
        # Categories
        "cat_access": "Access & Indexing",
        "cat_indexability": "Indexability",
        "cat_technical": "Technical SEO",
        "cat_content": "On-Page Content",
        "cat_image_ux": "UX & Assets",
        "cat_cwv_performance": "Core Web Vitals (Performance)",
        
        "ppt_cover_title": "SEO Technical Audit",
        "ppt_cover_sub": "Generated by AuditAI Pro v14.0",
        "ppt_slide_desc_title": "Description & Impact",
        "ppt_slide_count_title": "Affected Pages (in sample): {}",
        "ppt_slide_ex_title": "Affected Page Examples",
        "ppt_slide_sugg_title": "ğŸ’¡ Recommendation:",
        "serp_sim_title": "Google SERP Simulation:",
        "rich_sim_title": "Rich Results Simulation:",
        "code_sim_title": "Code Snippet:",
        "visual_sim_title": "Visual Experience Simulation:",
        "cwv_sim_title": "CWV Metric Visualization:",

        # Issues
        "lcp_issue": "LCP (Largest Contentful Paint) Fails", 
        "lcp_issue_desc": "LCP is {:.2f}s (Target <2.5s). Main content takes too long to appear.", 
        "lcp_issue_impact": "LCP is a core ranking factor. Slow loading speeds significantly increase bounce rates and lower search rankings.", 
        "lcp_issue_sugg": "Optimize image sizes (use WebP), implement a CDN, defer non-critical JavaScript, and preload the LCP element.",
        
        "cls_issue": "CLS (Cumulative Layout Shift) Fails", 
        "cls_issue_desc": "CLS score is {:.3f} (Target <0.1). Elements on the page shift unexpectedly during loading.", 
        "cls_issue_impact": "A Core Web Vital ranking factor. Visual instability frustrates users and can lead to accidental clicks, damaging brand reputation.", 
        "cls_issue_sugg": "Set explicit width and height attributes for all images and videos, and avoid inserting dynamic content above the fold.",
        
        "inp_issue": "INP (Interaction to Next Paint) Fails", 
        "inp_issue_desc": "INP is {}ms (Target <200ms). The page is unresponsive to user clicks or interactions.", 
        "inp_issue_impact": "A new Core Web Vital. High latency makes the site feel 'broken' or sluggish, severely impacting user conversion rates.", 
        "inp_issue_sugg": "Reduce main-thread blocking, break up Long Tasks, and optimize complex JavaScript event handlers.",
        "fcp_issue": "FCP (First Contentful Paint) Slow", "fcp_issue_desc": "FCP is {:.2f}s (Target <1.8s). Time to see first content is too long.", "fcp_issue_impact": "Slow FCP increases bounce rate as users feel the site is unresponsive.", "fcp_issue_sugg": "Improve TTFB and eliminate render-blocking resources.",
        "fcp_baidu_issue": "FCP not meeting Baidu standards", "fcp_baidu_issue_desc": "FCP is {:.2f}s (Baidu Target < 2.0s).", "fcp_baidu_issue_impact": "Baidu has strict requirements for mobile landing page speed. >2s leads to ranking penalty.", "fcp_baidu_issue_sugg": "Use China CDN, optimize critical path, ensure FCP < 2s.",
        
        "no_robots": "Missing Robots.txt", 
        "no_robots_desc": "The robots.txt file was not found in the root directory, or the server returned an error.", 
        "no_robots_impact": "Search engines may index useless or admin pages, wasting your crawl budget and server resources.", 
        "no_robots_sugg": "Create a standard robots.txt file in the root directory and ensure it is publicly accessible.",
        
        "robots_bad_rule": "Robots.txt Blocking", 
        "robots_bad_rule_desc": "A global blocking rule (Disallow: /) was detected.", 
        "robots_bad_rule_impact": "This prevents search engines from crawling your entire site, resulting in total de-indexing and zero organic traffic.", 
        "robots_bad_rule_sugg": "Remove the 'Disallow: /' rule immediately to allow crawling.",
        
        "robots_quality_issue": "Robots.txt Configuration", 
        "robots_quality_issue_desc": "Potential issue in Robots.txt: {}.", 
        "robots_quality_issue_impact": "May block important resources like CSS/JS, affecting how Google renders the page.", 
        "robots_quality_issue_sugg": "Review Robots.txt to ensure CSS and JS files are crawlable.",
        
        "robots_no_sitemap": "Sitemap Missing in Robots", 
        "robots_no_sitemap_desc": "The location of the Sitemap XML is not specified in the robots.txt file.", 
        "robots_no_sitemap_impact": "This slows down the discovery of new pages and content updates, especially for larger websites.", 
        "robots_no_sitemap_sugg": "Add a 'Sitemap: [URL]' directive to the bottom of your robots.txt file.",
        
        "no_sitemap": "Sitemap Failed", 
        "no_sitemap_desc": "Unable to access the Sitemap file (403 Forbidden or 404 Not Found).", 
        "no_sitemap_impact": "Search engines will struggle to find deep links or orphan pages, leading to poor indexing coverage.", 
        "no_sitemap_sugg": "Verify the Sitemap URL is correct and that server permissions allow external access.",
        
        "sitemap_invalid": "Invalid Sitemap", 
        "sitemap_invalid_desc": "XML parsing failed. The file format does not adhere to the standard protocol.", 
        "sitemap_invalid_impact": "Search engines cannot read the links inside, rendering the Sitemap completely useless.", 
        "sitemap_invalid_sugg": "Validate your XML syntax to ensure there are no unclosed tags or invalid characters.",
        
        "no_favicon": "Missing Favicon", 
        "no_favicon_desc": "No Favicon icon was detected on the homepage.", 
        "no_favicon_impact": "Reduces brand visibility in browser tabs and lowers the Click-Through Rate (CTR) in search results.", 
        "no_favicon_sugg": "Create a .ico or .png icon and link it in the <head> section.",
        
        "duplicate": "Duplicate Content", 
        "duplicate_desc": "Identical content detected across multiple URLs without proper canonicalization.", 
        "duplicate_impact": "Causes keyword cannibalization and dilutes link equity, preventing both pages from ranking well.", 
        "duplicate_sugg": "Select a preferred URL and use rel='canonical' tags on duplicate versions to point to it.",
        
        "http_3xx": "Redirect Chain", 
        "http_3xx_desc": "Internal link triggers a redirect (Chain: {}).", 
        "http_3xx_impact": "Wastes crawl budget, adds latency to page load, and dilutes the link equity passed to the destination.", 
        "http_3xx_sugg": "Update the internal link to point directly to the final destination URL.",
        
        "http_4xx": "Broken Link", 
        "http_4xx_desc": "Internal link returns a 4xx error (e.g., 404 Not Found).", 
        "http_4xx_impact": "Creates a bad user experience, breaks the flow of link equity, and may cause indexed pages to be dropped.", 
        "http_4xx_sugg": "Fix the broken link or remove it.",
        
        "http_5xx": "Server Error", 
        "http_5xx_desc": "Server returned a 5xx error (e.g., 500 Internal Server Error).", 
        "http_5xx_impact": "Signals server instability. Googlebot will reduce the crawl rate of your site to avoid overloading it.", 
        "http_5xx_sugg": "Check server error logs and ensure database stability.",
        
        "hreflang_invalid": "Invalid Hreflang", 
        "hreflang_invalid_desc": "The language code format does not comply with ISO 639-1 standards.", 
        "hreflang_invalid_impact": "Google cannot identify the target language, causing international targeting to fail.", 
        "hreflang_invalid_sugg": "Use standard ISO codes (e.g., 'en-US' instead of 'en_US').",
        
        "hreflang_no_default": "No x-default", 
        "hreflang_no_default_desc": "Missing 'x-default' fallback attribute.", 
        "hreflang_no_default_impact": "Users from unspecified regions may be served the wrong language version.", 
        "hreflang_no_default_sugg": "Add an hreflang='x-default' tag to specify the default version.",
        
        "alt_bad_quality": "Bad Alt Text", 
        "alt_bad_quality_desc": "Alt text uses filenames or generic words like 'image'.", 
        "alt_bad_quality_impact": "Search engines cannot understand the image context, hurting Image SEO and accessibility.", 
        "alt_bad_quality_sugg": "Use descriptive text that accurately describes the image content.",
        
        "anchor_bad_quality": "Bad Anchor", 
        "anchor_bad_quality_desc": "Generic anchor text found (e.g., 'Click here').", 
        "anchor_bad_quality_impact": "Fails to pass keyword relevance to the target page, reducing its ranking potential.", 
        "anchor_bad_quality_sugg": "Use descriptive keywords in the anchor text.",
        
        "cls_risk": "CLS Risk (Static)", 
        "cls_risk_desc": "Images missing width or height attributes detected.", 
        "cls_risk_impact": "Images will push content down as they load, causing layout shifts and hurting Core Web Vitals.", 
        "cls_risk_sugg": "Explicitly set width and height attributes on all image tags.",
        
        "missing_title": "Missing Title", 
        "missing_title_desc": "No <title> tag found in the page code.", 
        "missing_title_impact": "Title is the most important on-page SEO factor. Missing it causes severe ranking loss.", 
        "missing_title_sugg": "Add a unique, keyword-rich title to every page.",
        
        "short_title": "Title Short", 
        "short_title_desc": "Title is too short (~{}px).", 
        "short_title_impact": "Missed opportunity to target relevant keywords and attract clicks.", 
        "short_title_sugg": "Expand the title to ~285-575px, including your brand name.",
        
        "long_title": "Title Long", 
        "long_title_desc": "Title exceeds optimal width (~{}px).", 
        "long_title_impact": "The title will be truncated in search results, reducing readability and CTR.", 
        "long_title_sugg": "Shorten the title to under 600px, keeping important keywords at the front.",
        
        "missing_desc": "Missing Description", 
        "missing_desc_desc": "No meta description tag found.", 
        "missing_desc_impact": "Google will generate a snippet from page text, which is often irrelevant and lowers CTR.", 
        "missing_desc_sugg": "Add a compelling meta description that summarizes the page content.",
        
        "short_desc": "Description Short", 
        "short_desc_desc": "Description content is too thin (~{}px).", 
        "short_desc_impact": "Fails to provide enough context to entice users to click.", 
        "short_desc_sugg": "Expand the description to 400-920px with a call to action.",
        
        "missing_h1": "Missing H1", 
        "missing_h1_desc": "No <h1> heading tag found.", 
        "missing_h1_impact": "Search engines struggle to identify the main topic of the page.", 
        "missing_h1_sugg": "Ensure every page has exactly one H1 tag describing the main topic.",
        
        "missing_viewport": "No Viewport", 
        "missing_viewport_desc": "Mobile viewport meta tag is missing.", 
        "missing_viewport_impact": "The page is not mobile-friendly. Google Mobile-First Indexing will severely penalize it.", 
        "missing_viewport_sugg": "Add the standard viewport meta tag to the <head>.",
        
        "missing_canonical": "No Canonical", 
        "missing_canonical_desc": "Missing canonical tag.", 
        "missing_canonical_impact": "High risk of duplicate content issues, especially with URL parameters.", 
        "missing_canonical_sugg": "Add a self-referencing canonical tag to all pages.",
        
        "missing_jsonld": "No Schema", 
        "missing_jsonld_desc": "No JSON-LD structured data found.", 
        "missing_jsonld_impact": "Missed opportunity for Rich Snippets (e.g., Stars, Price) which boost CTR.", 
        "missing_jsonld_sugg": "Add JSON-LD schema based on page type: {}.",
        
        "missing_hreflang": "No Hreflang", 
        "missing_hreflang_desc": "No language targeting tags found.", 
        "missing_hreflang_impact": "Poor international targeting.", 
        "missing_hreflang_sugg": "Add hreflang tags.",
        
        "soft_404": "Soft 404", 
        "soft_404_desc": "Page returns a 200 OK status but displays an error message.", 
        "soft_404_impact": "Wastes crawl budget on invalid pages and confuses search engines.", 
        "soft_404_sugg": "Configure the server to return a 404 Not Found status code.",
        
        "missing_alt": "Missing Alt", 
        "missing_alt_desc": "Images lack alternative text attributes.", 
        "missing_alt_impact": "Bad for accessibility and prevents images from ranking in Image Search.", 
        "missing_alt_sugg": "Add descriptive alt text to all relevant images.",
        
        "js_links": "JS Links", 
        "js_links_desc": "Uncrawlable JavaScript links found.", 
        "js_links_impact": "Search engines cannot follow these links, leaving pages orphaned.", 
        "js_links_sugg": "Replace with standard <a href> tags.",
        
        "url_underscore": "URL Underscores", 
        "url_underscore_desc": "URL uses underscores to separate words.", 
        "url_underscore_impact": "Google treats underscores as joiners, not separators, hurting keyword parsing.", 
        "url_underscore_sugg": "Use hyphens (-) instead of underscores.",
        
        "url_uppercase": "URL Uppercase", 
        "url_uppercase_desc": "URL contains uppercase letters.", 
        "url_uppercase_impact": "Can lead to duplicate content issues on case-sensitive servers.", 
        "url_uppercase_sugg": "Force all URLs to be lowercase.",
        
        # Baidu
        "missing_keywords": "Missing Meta Keywords (Baidu)",
        "missing_keywords_desc": "No <meta name='keywords'> tag found.",
        "missing_keywords_impact": "Baidu still uses keywords as a ranking signal, unlike Google.",
        "missing_keywords_sugg": "Add meta keywords tag with 3-5 relevant keywords.",
        "missing_baidu_stats": "Missing Baidu Analytics",
        "missing_baidu_stats_desc": "Baidu Tongji script (hm.baidu.com) not found.",
        "missing_baidu_stats_impact": "Unable to track Baidu traffic effectively.",
        "missing_baidu_stats_sugg": "Install Baidu Tongji script.",
        "missing_baidu_verify": "Missing Baidu Verification",
        "missing_baidu_verify_desc": "No 'baidu-site-verification' tag found.",
        "missing_baidu_verify_impact": "May delay site indexing on Baidu.",
        "missing_baidu_verify_sugg": "Add verification tag.",
        "baidu_robots_missing": "Missing Baidu Rules",
        "baidu_robots_missing_desc": "No specific rules for 'Baiduspider' in Robots.txt.",
        "baidu_robots_missing_impact": "Inefficient crawling by Baidu.",
        "baidu_robots_missing_sugg": "Add User-agent: Baiduspider directives.",
        "missing_applicable_device": "Missing Applicable Device (Baidu)",
        "missing_applicable_device_desc": "Meta tag 'applicable-device' not found.",
        "missing_applicable_device_impact": "Baidu can't identify if page is PC/Mobile adapted.",
        "missing_applicable_device_sugg": "Add <meta name='applicable-device' content='pc,mobile'>.",
        "missing_no_transform": "Missing No-transform (Baidu)",
        "missing_no_transform_desc": "Cache-Control: no-transform not found.",
        "missing_no_transform_impact": "Baidu might transcode your page (Siteapp), breaking layout.",
        "missing_no_transform_sugg": "Add <meta http-equiv='Cache-Control' content='no-transform'>.",
        "missing_icp": "Missing ICP Number",
        "missing_icp_desc": "No ICP filing number found in page content.",
        "missing_icp_impact": "Required by Chinese law for mainland hosting; affects trust and Baidu ranking.",
        "missing_icp_sugg": "Add ICP number in footer linking to beian.miit.gov.cn.",
        "content_not_chinese": "Low Chinese Content Ratio",
        "content_not_chinese_desc": "Chinese character ratio is below 5%.",
        "content_not_chinese_impact": "Baidu prioritizes Chinese content. Low ratio affects ranking in CN search.",
        "content_not_chinese_sugg": "Ensure main content is in Simplified Chinese.",
        "server_not_in_china": "Server Not In China (Baidu)",
        "server_not_in_china_desc": "Server IP detected in: {}. Baidu prefers mainland China hosting.",
        "server_not_in_china_impact": "Slow cross-border loading may cause Baidu spider timeouts.",
        "server_not_in_china_sugg": "Migrate hosting to Mainland China and get ICP filing."
    }
}

# --- Level 6: æ ¸å¿ƒé€»è¾‘ (Data Layer) ---
def get_translated_text(issue_id, lang, args=None):
    if args is None: args = []
    t = TRANSLATIONS[lang]
    
    def safe_format(text, arguments):
        try: return text.format(*arguments)
        except IndexError: return text

    return {
        "title": t.get(issue_id, issue_id),
        "desc": safe_format(t.get(issue_id + "_desc", ""), args),
        "impact": t.get(issue_id + "_impact", ""),
        "suggestion": safe_format(t.get(issue_id + "_sugg", ""), args)
    }

def fetch_psi_data(url, api_key):
    if not api_key: return None
    endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&strategy=mobile"
    try:
        response = requests.get(endpoint, timeout=30)
        if response.status_code == 200:
            data = response.json()
            crux = data.get('loadingExperience', {}).get('metrics', {})
            if not crux: return {"error": "No CrUX data available"}
            return {
                "LCP": crux.get('LARGEST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
                "CLS": crux.get('CUMULATIVE_LAYOUT_SHIFT_SCORE', {}).get('percentile', 0) / 100,
                "INP": crux.get('INTERACTION_TO_NEXT_PAINT', {}).get('percentile', 0),
                "FCP": crux.get('FIRST_CONTENTFUL_PAINT_MS', {}).get('percentile', 0) / 1000,
            }
        else: return {"error": f"API Error: {response.status_code}"}
    except Exception as e: return {"error": str(e)}

def check_cwv_issues(cwv_data, url, label=""):
    issues = []
    if not cwv_data or "error" in cwv_data: return issues
    category_key = "cwv_performance"
    
    lcp = cwv_data.get("LCP", 0)
    if lcp > 2.5:
        issues.append({
            "id": "lcp_issue", "category": category_key, "severity": "Critical" if lcp > 4.0 else "High",
            "url": url, "args": [lcp], "examples": [f"{url} ({lcp:.2f}s) {label}"] 
        })
    
    inp = cwv_data.get("INP", 0)
    if inp > 200:
        issues.append({
            "id": "inp_issue", "category": category_key, "severity": "Critical" if inp > 500 else "High",
            "url": url, "args": [inp], "examples": [f"{url} ({inp}ms) {label}"]
        })

    cls = cwv_data.get("CLS", 0)
    if cls > 0.1:
        issues.append({
            "id": "cls_issue", "category": category_key, "severity": "Critical" if cls > 0.25 else "High",
            "url": url, "args": [cls], "examples": [f"{url} ({cls:.3f}) {label}"]
        })
    
    fcp = cwv_data.get("FCP", 0)
    if fcp > 1.8:
        issues.append({
            "id": "fcp_issue", "category": category_key, "severity": "Critical" if fcp > 3.0 else "Medium",
            "url": url, "args": [fcp], "examples": [f"{url} ({fcp:.2f}s) {label}"]
        })

    return issues

def check_site_level_assets(start_url, lang="zh", check_robots=True, crawl_sitemap_flag=True, manual_sitemaps=None, baidu_mode=False):
    issues = []
    sitemap_has_hreflang = False
    
    initial_netloc = urlparse(start_url).netloc
    base_url = f"{urlparse(start_url).scheme}://{initial_netloc}"
    headers = get_browser_headers()
    
    robots_url = urljoin(base_url, "/robots.txt")
    if check_robots:
        try:
            r = requests.get(robots_url, headers=headers, timeout=10, allow_redirects=True, stream=True, verify=False)
            if r.status_code != 200:
                issues.append({"id": "no_robots", "category": "access", "severity": "Medium", "url": robots_url, "examples": [robots_url]})
            else:
                content = r.text.lower()
                if len(content.strip()) < 5:
                     issues.append({"id": "robots_quality_issue", "category": "access", "severity": "Medium", "url": robots_url, "args": ["File is empty or too short"], "examples": [robots_url]})
                if "user-agent" not in content:
                     issues.append({"id": "robots_quality_issue", "category": "access", "severity": "Medium", "url": robots_url, "args": ["Missing User-agent directive"], "examples": [robots_url]})
                if "disallow: /*.css" in content or "disallow: /*.js" in content:
                     issues.append({"id": "robots_quality_issue", "category": "access", "severity": "High", "url": robots_url, "args": ["Blocking CSS/JS resources"], "examples": [robots_url]})
                
                if "disallow: /" in content and "allow:" not in content:
                    issues.append({"id": "robots_bad_rule", "category": "access", "severity": "Critical", "url": robots_url, "examples": [robots_url]})
                
                if baidu_mode:
                    if "baiduspider" not in content:
                        issues.append({"id": "baidu_robots_missing", "category": "access", "severity": "Low", "url": robots_url, "examples": [robots_url]})
                
                if "sitemap:" not in content:
                    issues.append({"id": "robots_no_sitemap", "category": "access", "severity": "Low", "url": robots_url, "examples": [robots_url]})
                
                if crawl_sitemap_flag:
                    sitemaps_in_robots = re.findall(r'sitemap:\s*(https?://\S+)', content, re.IGNORECASE)
                    if sitemaps_in_robots:
                        if manual_sitemaps is None: manual_sitemaps = []
                        manual_sitemaps.extend(sitemaps_in_robots)
            r.close()
        except: 
            issues.append({"id": "no_robots", "category": "access", "severity": "Medium", "url": robots_url, "examples": [robots_url]})

    sitemap_urls = manual_sitemaps if manual_sitemaps else [urljoin(base_url, "/sitemap.xml")]
    any_valid = False
    for sm_url in sitemap_urls:
        if not sm_url.strip(): continue
        try:
            r = requests.get(sm_url, headers=headers, timeout=15, verify=False)
            if r.status_code == 200:
                try:
                    ET.fromstring(r.content)
                    any_valid = True
                    if 'hreflang' in r.text or 'xhtml' in r.text: sitemap_has_hreflang = True
                except:
                    if not sm_url.endswith('.gz'):
                        issues.append({"id": "sitemap_invalid", "category": "access", "severity": "Medium", "url": sm_url, "examples": [sm_url]})
            else:
                if manual_sitemaps: issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "url": sm_url, "examples": [sm_url]})
        except:
            if manual_sitemaps: issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "url": sm_url, "examples": [sm_url]})

    if not any_valid and not manual_sitemaps:
         issues.append({"id": "no_sitemap", "category": "access", "severity": "Low", "url": sitemap_urls[0], "examples": [sitemap_urls[0]]})

    try:
        r = requests.get(urljoin(base_url, "/favicon.ico"), headers=headers, timeout=5, verify=False)
        if r.status_code != 200 or int(r.headers.get('content-length', 0)) == 0:
            issues.append({"id": "no_favicon", "category": "image_ux", "severity": "Low", "url": base_url, "examples": [base_url]})
    except: pass
    
    if baidu_mode:
        cc, country_name = check_server_location(start_url)
        if cc and cc != 'CN':
             issues.append({"id": "server_not_in_china", "category": "technical", "severity": "High", "url": start_url, "args": [country_name], "examples": [start_url]})

    return issues, sitemap_has_hreflang

def analyze_page(url, content, status, sitemap_has_hreflang, baidu_mode=False):
    soup = BeautifulSoup(content, 'html.parser')
    issues = []
    
    title = soup.title.string.strip() if soup.title else None
    desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = desc['content'].strip() if desc else None
    h1 = soup.find('h1')
    h1_content = h1.get_text().strip() if h1 else None
    
    can_tag = soup.find('link', attrs={'rel': 'canonical'})
    can_url = can_tag['href'] if can_tag else None

    if status == 200:
        is_self_canonical = True
        if can_url:
            def norm_u(u): return u.split('#')[0].rstrip('/')
            try:
                abs_can = urljoin(url, can_url)
                if norm_u(abs_can) != norm_u(url):
                    is_self_canonical = False
            except: pass

        if not can_url:
            issues.append({"id": "missing_canonical", "category": "indexability", "severity": "Medium", "url": url})
            is_self_canonical = True

        hreflangs = soup.find_all('link', hreflang=True)
        if hreflangs:
            has_x_default = False
            invalid = []
            pat = re.compile(r'^[a-z]{2}(-[a-zA-Z]{2})?$|x-default', re.IGNORECASE)
            for link in hreflangs:
                code = link.get('hreflang', '').strip()
                if code.lower() == 'x-default': has_x_default = True
                if not pat.match(code): invalid.append(code)
            if invalid:
                issues.append({"id": "hreflang_invalid", "category": "indexability", "severity": "High", "url": url, "args": [", ".join(invalid[:3])]})
            if not has_x_default:
                issues.append({"id": "hreflang_no_default", "category": "indexability", "severity": "Low", "url": url})
        elif not sitemap_has_hreflang:
             if is_self_canonical:
                issues.append({"id": "missing_hreflang", "category": "indexability", "severity": "Low", "url": url})

        if is_self_canonical:
            if not soup.find('meta', attrs={'name': 'viewport'}):
                issues.append({"id": "missing_viewport", "category": "technical", "severity": "Critical", "url": url})
            
            if not soup.find('script', type='application/ld+json'):
                 path = urlparse(url).path.lower()
                 rec = "BreadcrumbList"
                 if path in ["/", ""]: rec = "Organization/WebSite"
                 elif any(x in path for x in ["product", "shop"]): rec = "Product"
                 elif any(x in path for x in ["blog", "news"]): rec = "Article"
                 issues.append({"id": "missing_jsonld", "category": "technical", "severity": "Medium", "url": url, "args": [rec]})

            if '_' in url: issues.append({"id": "url_underscore", "category": "technical", "severity": "Low", "url": url})
            if any(c.isupper() for c in urlparse(url).path): issues.append({"id": "url_uppercase", "category": "technical", "severity": "Medium", "url": url})
            
            if soup.find('a', href=lambda x: x and x.lower().startswith('javascript:')):
                issues.append({"id": "js_links", "category": "access", "severity": "High", "url": url}) 

            imgs = soup.find_all('img')
            missing_alt = 0
            bad_alt = 0
            cls_risk = 0
            for img in imgs:
                alt = img.get('alt', '').strip()
                if not alt: missing_alt += 1
                elif len(alt) < 3 or any(x in alt.lower() for x in ["image", "photo", "img"]): bad_alt += 1
                if not img.get('width') or not img.get('height'): cls_risk += 1
            
            if missing_alt > 0: issues.append({"id": "missing_alt", "category": "image_ux", "severity": "Medium", "url": url})
            if bad_alt > 0: issues.append({"id": "alt_bad_quality", "category": "image_ux", "severity": "Low", "url": url})
            if cls_risk > 0: issues.append({"id": "cls_risk", "category": "cwv_performance", "severity": "Medium", "url": url})

            links = soup.find_all('a', href=True)
            bad_anchors = ["click here", "read more", "more"]
            if any(a.get_text().strip().lower() in bad_anchors for a in links):
                issues.append({"id": "anchor_bad_quality", "category": "access", "severity": "Low", "url": url})
            
            if not title: 
                issues.append({"id": "missing_title", "category": "content", "severity": "High", "url": url})
            else:
                px_w = estimate_pixel_width(title)
                if px_w < 200:
                    issues.append({"id": "short_title", "category": "content", "severity": "Medium", "url": url, "evidence": title, "args": [int(px_w)]})
                elif px_w > 600:
                    issues.append({"id": "long_title", "category": "content", "severity": "Low", "url": url, "evidence": title, "args": [int(px_w)]})

            if not desc_content: 
                issues.append({"id": "missing_desc", "category": "content", "severity": "High", "url": url})
            else:
                px_w_d = estimate_pixel_width(desc_content)
                if px_w_d < 400:
                    issues.append({"id": "short_desc", "category": "content", "severity": "Low", "url": url, "evidence": desc_content, "args": [int(px_w_d)]})

            if not h1_content: issues.append({"id": "missing_h1", "category": "content", "severity": "High", "url": url})

            if (title and "not found" in title.lower()) or (soup.find('h1') and "not found" in soup.find('h1').get_text().lower()):
                issues.append({"id": "soft_404", "category": "access", "severity": "Critical", "url": url})
        
        if baidu_mode:
            keywords = soup.find('meta', attrs={'name': 'keywords'})
            if not keywords or not keywords.get('content', '').strip():
                 issues.append({"id": "missing_keywords", "category": "content", "severity": "Medium", "url": url})
            
            if "hm.baidu.com" not in str(soup):
                 issues.append({"id": "missing_baidu_stats", "category": "technical", "severity": "Low", "url": url})
            
            if not soup.find('meta', attrs={'name': 'applicable-device'}):
                 issues.append({"id": "missing_applicable_device", "category": "technical", "severity": "Medium", "url": url})
            
            has_no_transform = False
            for meta in soup.find_all('meta'):
                if meta.get('http-equiv', '').lower() == 'cache-control' and 'no-transform' in meta.get('content', '').lower():
                    has_no_transform = True
                    break
            if not has_no_transform:
                 issues.append({"id": "missing_no_transform", "category": "technical", "severity": "Medium", "url": url})
            
            page_text = soup.get_text()
            if "ICPå¤‡" not in page_text and "ICPè¯" not in page_text:
                 issues.append({"id": "missing_icp", "category": "technical", "severity": "High", "url": url})
            
            chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', page_text))
            total_chars = len(page_text.strip())
            if total_chars > 200 and (chinese_chars / total_chars) < 0.05:
                 issues.append({"id": "content_not_chinese", "category": "content", "severity": "Medium", "url": url})


    return {
        "URL": url, 
        "Status": status, 
        "Title": title, 
        "Description": desc_content,
        "H1": h1_content,
        "Canonical": can_url,
        "Content_Hash": hashlib.md5(soup.get_text().encode('utf-8')).hexdigest()
    }, issues

def crawl_website(start_url, max_pages, lang, manual_robots, manual_sitemaps, psi_key, list_url=None, detail_url=None, check_robots=True, crawl_sitemap=True, allow_sub=False, allow_outside=False, manual_pages=None, baidu_mode=False):
    visited = set()
    seen_hashes = {} 
    seen_urls = set()
    
    queue = [start_url]
    seen_urls.add(start_url)
    if list_url and is_valid_url(list_url):
         queue.append(list_url)
         seen_urls.add(list_url)
    if detail_url and is_valid_url(detail_url):
         queue.append(detail_url)
         seen_urls.add(detail_url)

    if manual_pages:
        for p in manual_pages:
            if is_valid_url(p) and p not in seen_urls:
                queue.append(p)
                seen_urls.add(p)

    results_data = []
    all_issues = []
    first_error = None
    target_domain = None
    
    start_netloc = urlparse(start_url).netloc.replace('www.', '')
    start_path = urlparse(start_url).path
    if not start_path.endswith('/'): start_path += '/'
    
    def clean_url(u): return u.split('?')[0].split('#')[0]

    progress_bar = st.progress(0, text="Initializing...")
    sitemap_has_hreflang = False
    
    try:
        site_issues, sitemap_has_hreflang = check_site_level_assets(
            start_url, lang, check_robots, crawl_sitemap, manual_sitemaps, baidu_mode
        )
        all_issues.extend(site_issues)
        st.session_state['sitemap_hreflang_found'] = sitemap_has_hreflang
    except Exception as e:
        pass

    if psi_key:
        with st.spinner(TRANSLATIONS[lang]["psi_fetching"].format("Pages")):
            targets = [("Home", start_url)]
            if list_url and is_valid_url(list_url): targets.append(("List", list_url))
            if detail_url and is_valid_url(detail_url): targets.append(("Detail", detail_url))
            
            for label, t_url in targets:
                cwv_data = fetch_psi_data(t_url, psi_key)
                if cwv_data and "error" not in cwv_data:
                    if label == "Home": st.session_state['cwv_data'] = cwv_data
                    all_issues.extend(check_cwv_issues(cwv_data, t_url, label=f"({label})"))

    count = 0
    headers = get_browser_headers()
    
    while queue and count < max_pages:
        url = queue.pop(0)
        visited.add(url)
        
        if any(x in url.lower() for x in ['/login', '/signin', '/admin', '/cart', '/account']):
            continue

        count += 1
        progress_bar.progress(int(count/max_pages*100), text=f"Crawling ({count}/{max_pages}): {url}")
        time.sleep(0.1)
        
        try:
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
            current_url = response.url 
            
            if count == 1 and url == start_url:
                 start_netloc = urlparse(current_url).netloc.replace('www.', '')

            final_status = response.status_code

            if response.history:
                chain_list = [r.url for r in response.history] + [current_url]
                origin_netloc = urlparse(chain_list[0]).netloc.replace('www.', '')
                chain_display_parts = []
                for u in chain_list:
                    u_obj = urlparse(u)
                    u_netloc = u_obj.netloc.replace('www.', '')
                    if u_netloc != origin_netloc:
                        chain_display_parts.append(u) 
                    else:
                        p = u_obj.path
                        if not p: p = "/"
                        chain_display_parts.append(p)

                chain_str = " -> ".join(chain_display_parts)
                all_issues.append({"id": "http_3xx", "category": "access", "severity": "Medium", "url": url, "args": [chain_str]})

            if final_status >= 400:
                is_5xx = final_status >= 500
                all_issues.append({"id": "http_5xx" if is_5xx else "http_4xx", "category": "access", "severity": "Critical" if is_5xx else "High", "url": url, "args": [str(final_status)]})

            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                if 'type="password"' in response.text.lower():
                     continue

                page_data, page_issues = analyze_page(current_url, response.content, final_status, sitemap_has_hreflang, baidu_mode)
                
                if final_status == 200:
                    current_hash = page_data['Content_Hash']
                    current_canonical = page_data['Canonical']
                    current_clean = clean_url(current_url)
                    
                    if current_hash in seen_hashes:
                        original_url = seen_hashes[current_hash]
                        if current_url != original_url and not (current_canonical and current_canonical != current_url):
                            all_issues.append({
                                "id": "duplicate", "category": "indexability", 
                                "severity": "High", "url": current_url, 
                                "meta": original_url 
                            })
                    else:
                        seen_hashes[current_hash] = current_url

                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                soup = BeautifulSoup(response.content, 'html.parser')
                for a in soup.find_all('a', href=True):
                    raw_link = urljoin(current_url, a['href'])
                    link = raw_link.split('#')[0] 
                    
                    link_parsed = urlparse(link)
                    link_netloc = link_parsed.netloc.replace('www.', '')
                    link_path = link_parsed.path

                    is_internal = False
                    if not link_netloc: is_internal = True
                    elif allow_sub:
                        is_internal = link_netloc.endswith(start_netloc)
                    else:
                        is_internal = link_netloc == start_netloc

                    path_ok = True
                    if not allow_outside:
                        if not link_path.startswith(start_path): path_ok = False
                    
                    if is_internal and path_ok and link not in seen_urls:
                        if not any(link.lower().endswith(ext) for ext in ['.jpg', '.png', '.pdf', '.zip', '.css', '.js', '.json', '.xml']):
                            seen_urls.add(link)
                            queue.append(link)
            else:
                if count == 1: first_error = f"Content type: {content_type}"
        except Exception as e:
            if count == 1: first_error = str(e)
            pass
    
    progress_bar.empty()
    if not results_data and first_error: return None, None, first_error
    return results_data, all_issues, None

# --- Level 7: å…¨å±€ PPT ç»˜å›¾å‡½æ•° ---
def set_font(font_obj, size, bold=False, color=None, lang="zh"):
    font_obj.size = Pt(size)
    font_obj.name = 'Microsoft YaHei' if lang == "zh" else 'Arial'
    font_obj.bold = bold
    if color: font_obj.color.rgb = color

def draw_cwv_gauge(slide, metric, value, thresholds):
    good, poor = thresholds
    total_w = 4
    x_start = 7.2
    y_pos = 4.8
    h = 0.4

    r1 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(x_start), Inches(y_pos), Inches(total_w/3), Inches(h))
    r1.fill.solid()
    r1.fill.fore_color.rgb = RGBColor(12, 206, 107)
    r1.line.fill.background()

    r2 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(x_start + total_w/3), Inches(y_pos), Inches(total_w/3), Inches(h))
    r2.fill.solid()
    r2.fill.fore_color.rgb = RGBColor(255, 164, 0)
    r2.line.fill.background()
    
    r3 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(x_start + 2*total_w/3), Inches(y_pos), Inches(total_w/3), Inches(h))
    r3.fill.solid()
    r3.fill.fore_color.rgb = RGBColor(255, 78, 66)
    r3.line.fill.background()
    
    def add_label(x_offset, text):
        tb = slide.shapes.add_textbox(Inches(x_start + x_offset - 0.2), Inches(y_pos + 0.40), Inches(0.5), Inches(0.3))
        tb.margin_top = 0
        p = tb.text_frame.add_paragraph()
        p.text = text
        p.font.size = Pt(9)
        p.font.color.rgb = RGBColor(80, 80, 80)

    add_label(0, "0")
    add_label(total_w/3, str(good))
    add_label(2*total_w/3, str(poor))

    pos = 0
    if value <= good:
        pos = (value / good) * (total_w/3)
    elif value <= poor:
        pos = (total_w/3) + ((value - good) / (poor - good)) * (total_w/3)
    else:
        cap = poor * 1.5
        normalized = min(value, cap)
        pos = (2*total_w/3) + ((normalized - poor) / (cap - poor)) * (total_w/3)

    marker_x = x_start + pos
    
    tri = slide.shapes.add_shape(MSO_SHAPE.ISOSCELES_TRIANGLE, Inches(marker_x - 0.1), Inches(y_pos - 0.2), Inches(0.2), Inches(0.2))
    tri.rotation = 180
    tri.fill.solid()
    tri.fill.fore_color.rgb = RGBColor(50, 50, 50)
    tri.line.fill.background()
    
    tb = slide.shapes.add_textbox(Inches(marker_x - 0.5), Inches(y_pos - 0.8), Inches(1), Inches(0.3))
    p = tb.text_frame.add_paragraph()
    p.text = f"{value}"
    p.font.size = Pt(11)
    p.font.bold = True
    p.alignment = PP_ALIGN.CENTER

def draw_serp_preview(slide, issue_id, issue_title, evidence, url, txt, lang="zh"):
    box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(1.8))
    box.fill.solid()
    box.fill.fore_color.rgb = RGBColor(255, 255, 255)
    box.line.color.rgb = RGBColor(220, 220, 220)
    
    label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
    p = label.text_frame.add_paragraph()
    
    if "favicon" in issue_id:
        p.text = txt["visual_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        circle = slide.shapes.add_shape(MSO_SHAPE.OVAL, Inches(7.2), Inches(4.3), Inches(0.25), Inches(0.25))
        circle.fill.solid()
        circle.fill.fore_color.rgb = RGBColor(200, 200, 200) 
        l1 = slide.shapes.add_shape(MSO_SHAPE.ARC, Inches(7.2), Inches(4.3), Inches(0.25), Inches(0.25))
        l1.line.color.rgb = RGBColor(150, 150, 150)
        l2 = slide.shapes.add_connector(MSO_CONNECTOR.STRAIGHT, Inches(7.325), Inches(4.3), Inches(7.325), Inches(4.55))
        l2.line.color.rgb = RGBColor(150, 150, 150)

        tb = slide.shapes.add_textbox(Inches(7.5), Inches(4.25), Inches(4), Inches(0.4))
        p2 = tb.text_frame.add_paragraph()
        p2.text = urlparse(url).netloc
        set_font(p2.font, 12, False, RGBColor(32, 33, 36), lang)
        
        tb_t = slide.shapes.add_textbox(Inches(7.2), Inches(4.6), Inches(5), Inches(0.4))
        p_t = tb_t.text_frame.add_paragraph()
        p_t.text = "Page Title Example"
        set_font(p_t.font, 16, False, RGBColor(26, 13, 171), lang)
        
        tb_d = slide.shapes.add_textbox(Inches(7.2), Inches(5.0), Inches(5), Inches(0.4))
        p_d = tb_d.text_frame.add_paragraph()
        p_d.text = "This simulates a missing favicon result on Google mobile SERP."
        set_font(p_d.font, 12, False, RGBColor(80, 80, 80), lang)

    elif "alt" in issue_id:
        p.text = "Screen Reader View:"
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        
        img_box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(7.2), Inches(4.2), Inches(1.5), Inches(1.0))
        img_box.line.color.rgb = RGBColor(100, 100, 100)
        img_box.fill.background() 
        
        bubble_text = '<img src="..." />'
        color = RGBColor(200, 0, 0)
        if "quality" in issue_id:
            bubble_text = '<img src="..." alt="image" />'
            color = RGBColor(255, 165, 0)
        
        callout = slide.shapes.add_shape(MSO_SHAPE.LINE_CALLOUT_2, Inches(8.8), Inches(4.2), Inches(3.5), Inches(0.8))
        callout.fill.solid()
        callout.fill.fore_color.rgb = RGBColor(240, 240, 240)
        callout.text_frame.text = bubble_text
        callout.text_frame.paragraphs[0].font.color.rgb = color
            
    elif "lcp" in issue_id:
        p.text = txt["cwv_sim_title"] + " LCP"
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        val = float(re.findall(r"(\d+\.\d+)", evidence)[0]) if re.findall(r"(\d+\.\d+)", evidence) else 3.0
        draw_cwv_gauge(slide, "LCP", val, (2.5, 4.0))

    elif "inp" in issue_id:
        p.text = txt["cwv_sim_title"] + " INP"
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        val = float(re.findall(r"(\d+)", evidence)[0]) if re.findall(r"(\d+)", evidence) else 300
        draw_cwv_gauge(slide, "INP", val, (200, 500))

    elif "cls" in issue_id:
        p.text = txt["cwv_sim_title"] + " CLS"
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        val = float(re.findall(r"(\d+\.\d+)", evidence)[0]) if re.findall(r"(\d+\.\d+)", evidence) else 0.2
        draw_cwv_gauge(slide, "CLS", val, (0.1, 0.25))

    elif "fcp" in issue_id:
        p.text = txt["cwv_sim_title"] + " FCP"
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        val = float(re.findall(r"(\d+\.\d+)", evidence)[0]) if re.findall(r"(\d+\.\d+)", evidence) else 2.0
        thresholds = (1.0, 2.0) if "baidu" in issue_id else (1.8, 3.0)
        draw_cwv_gauge(slide, "FCP", val, thresholds)
             
    elif "3xx" in issue_id:
        p.text = "Redirect Flow:"
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        
        parts = evidence.split(' -> ')
        if len(parts) > 1:
            x = 7.2
            display_parts = parts[:3]
            if len(parts) > 3: display_parts[-1] = "Final"

            for i, part in enumerate(display_parts):
                box_w = 2.0 if len(part) > 20 else 1.5
                b = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(x), Inches(4.5), Inches(box_w), Inches(0.6))
                b.fill.solid()
                b.fill.fore_color.rgb = RGBColor(240, 240, 240) if i < len(display_parts)-1 else RGBColor(220, 255, 220)
                b.text_frame.text = part
                b.text_frame.paragraphs[0].font.size = Pt(9)
                b.text_frame.paragraphs[0].font.color.rgb = RGBColor(0,0,0)
                
                if i < len(display_parts) - 1:
                    ar = slide.shapes.add_shape(MSO_SHAPE.RIGHT_ARROW, Inches(x+box_w), Inches(4.7), Inches(0.3), Inches(0.2))
                    ar.fill.solid()
                    ar.fill.fore_color.rgb = RGBColor(100, 100, 100)
                    x += (box_w + 0.3)
            
    else:
        p.text = txt["serp_sim_title"]
        set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)
        
        tf = box.text_frame
        tf.margin_left = Inches(0.2)
        tf.margin_top = Inches(0.2)
        p_serp = tf.add_paragraph()
        p_serp.text = f"{urlparse(url).netloc} â€º ..."
        set_font(p_serp.font, 12, False, RGBColor(32, 33, 36), lang)
        
        if "short_desc" in issue_id or "missing_desc" in issue_id:
            p_serp = tf.add_paragraph()
            p_serp.space_before = Pt(5)
            p_serp.text = evidence[:60] + "..." if evidence else "Title of the page"
            set_font(p_serp.font, 18, False, RGBColor(26, 13, 171), lang)
            
            p_serp = tf.add_paragraph()
            p_serp.space_before = Pt(3)
            if "missing" in issue_id:
                p_serp.text = "No description available in code..."
            else:
                p_serp.text = evidence 
            set_font(p_serp.font, 14, False, RGBColor(77, 81, 86), lang)
        
        elif "long_title" in issue_id:
            p_serp = tf.add_paragraph()
            p_serp.space_before = Pt(5)
            p_serp.text = evidence[:55] + " ..."
            set_font(p_serp.font, 18, False, RGBColor(26, 13, 171), lang)

            p_serp = tf.add_paragraph()
            p_serp.space_before = Pt(3)
            p_serp.text = "The meta description of the page would appear here..."
            set_font(p_serp.font, 14, False, RGBColor(77, 81, 86), lang)

        else:
            p_serp = tf.add_paragraph()
            p_serp.space_before = Pt(5)
            p_serp.text = evidence if evidence else "Untitled Page"
            set_font(p_serp.font, 18, False, RGBColor(26, 13, 171), lang)

def draw_code_preview(slide, txt, lang="zh"):
    box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(1.5))
    box.fill.solid()
    box.fill.fore_color.rgb = RGBColor(245, 245, 245)
    box.line.color.rgb = RGBColor(200, 200, 200)
    tf = box.text_frame
    tf.margin_left = Inches(0.1)
    p = tf.add_paragraph()
    p.text = '<a href="javascript:void(0)">\n  Click Here\n</a>'
    set_font(p.font, 14, True, RGBColor(200, 0, 0), lang)
    
    label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
    p = label.text_frame.add_paragraph()
    p.text = txt["code_sim_title"]
    set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)

def draw_hreflang_preview(slide, url, missing_type, txt, lang="zh"):
    box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(2.0))
    box.fill.solid()
    box.fill.fore_color.rgb = RGBColor(245, 245, 245)
    box.line.color.rgb = RGBColor(200, 200, 200)
    
    tf = box.text_frame
    tf.margin_left = Inches(0.2)
    tf.margin_top = Inches(0.2)
    
    p = tf.add_paragraph()
    p.text = "<!-- Correct Implementation -->"
    set_font(p.font, 10, False, RGBColor(128, 128, 128), lang)
    
    p = tf.add_paragraph()
    clean_url = url.split('?')[0][:40] + "..."
    if "default" in missing_type:
        p.text = f'<link rel="alternate" hreflang="x-default" href="{clean_url}" />'
        set_font(p.font, 11, True, RGBColor(200, 0, 0), lang)
    else:
        p.text = f'<link rel="alternate" hreflang="en" href="{clean_url}" />\n<link rel="alternate" hreflang="fr" href="..." />'
        set_font(p.font, 11, False, RGBColor(0, 0, 128), lang)

    label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
    p = label.text_frame.add_paragraph()
    p.text = txt["code_sim_title"]
    set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)

def draw_rich_snippet_preview(slide, url, txt, lang="zh"):
    box = slide.shapes.add_shape(MSO_SHAPE.ROUNDED_RECTANGLE, Inches(7), Inches(4), Inches(5.8), Inches(2.0))
    box.fill.solid()
    box.fill.fore_color.rgb = RGBColor(255, 255, 255)
    box.line.color.rgb = RGBColor(220, 220, 220)
    tf = box.text_frame
    tf.margin_left = Inches(0.2)
    tf.margin_top = Inches(0.2)
    p = tf.add_paragraph()
    p.text = f"{urlparse(url).netloc} â€º product"
    set_font(p.font, 12, False, RGBColor(32, 33, 36), lang)
    
    p = tf.add_paragraph()
    p.space_before = Pt(5)
    p.text = "Product Name Example - Best Choice"
    set_font(p.font, 18, False, RGBColor(26, 13, 171), lang)
    
    p = tf.add_paragraph()
    p.space_before = Pt(3)
    p.text = "â˜…â˜…â˜…â˜…â˜… Rating: 4.8 Â· $199.00 Â· In stock"
    set_font(p.font, 12, False, RGBColor(231, 113, 27), lang)
    
    p = tf.add_paragraph()
    p.space_before = Pt(3)
    p.text = "This is a rich result enabled by Schema..."
    set_font(p.font, 14, False, RGBColor(77, 81, 86), lang)
    
    label = slide.shapes.add_textbox(Inches(7), Inches(3.6), Inches(4), Inches(0.3))
    p = label.text_frame.add_paragraph()
    p.text = txt["rich_sim_title"]
    set_font(p.font, 12, True, RGBColor(100, 100, 100), lang)

def create_styled_pptx(slides_data, lang):
    prs = Presentation()
    prs.slide_width = Inches(13.333)
    prs.slide_height = Inches(7.5)
    txt = TRANSLATIONS[lang] 
    
    # Cover
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, Inches(13.333), Inches(7.5))
    bg.fill.solid()
    bg.fill.fore_color.rgb = RGBColor(18, 52, 86)
    title = slide.shapes.add_textbox(Inches(1), Inches(2.5), Inches(11), Inches(2))
    p = title.text_frame.add_paragraph()
    p.text = txt["ppt_cover_title"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 54, True, RGBColor(255, 255, 255), lang)
    
    sub = slide.shapes.add_textbox(Inches(1), Inches(4), Inches(11), Inches(1))
    p = sub.text_frame.add_paragraph()
    p.text = txt["ppt_cover_sub"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 24, False, RGBColor(200, 200, 200), lang)

    # Slides
    for s in slides_data:
        t_data = get_translated_text(s['id'], lang, s.get('args'))
        slide = prs.slides.add_slide(prs.slide_layouts[6])
        
        # Header
        h_shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, Inches(13.333), Inches(1.2))
        h_shape.fill.solid()
        h_shape.fill.fore_color.rgb = RGBColor(240, 242, 246)
        h_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(10), Inches(0.8))
        p = h_box.text_frame.add_paragraph()
        p.text = t_data['title']
        set_font(p.font, 32, True, RGBColor(50, 50, 50), lang)
        
        sev_color = RGBColor(220, 53, 69) if s['severity'] == "Critical" else RGBColor(253, 126, 20)
        sev_box = slide.shapes.add_textbox(Inches(11), Inches(0.35), Inches(2), Inches(0.5))
        p = sev_box.text_frame.add_paragraph()
        p.text = s['severity']
        p.alignment = PP_ALIGN.CENTER
        set_font(p.font, 18, True, sev_color, lang)
        
        # Category
        cat_key = f"cat_{s['category']}"
        cat_label = txt.get(cat_key, s['category'].upper())
        cat_box = slide.shapes.add_textbox(Inches(0.5), Inches(1.3), Inches(4), Inches(0.4))
        p = cat_box.text_frame.add_paragraph()
        p.text = cat_label
        set_font(p.font, 14, True, RGBColor(0, 102, 204), lang)

        # Desc
        d_title = slide.shapes.add_textbox(Inches(0.5), Inches(1.8), Inches(6), Inches(0.5))
        p = d_title.text_frame.add_paragraph()
        p.text = txt["ppt_desc"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30), lang)
        
        d_box = slide.shapes.add_textbox(Inches(0.5), Inches(2.3), Inches(6), Inches(1.2))
        tf = d_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = t_data['desc']
        set_font(p.font, 14, False, RGBColor(80, 80, 80), lang)
        
        # Impact
        i_title = slide.shapes.add_textbox(Inches(0.5), Inches(3.6), Inches(6), Inches(0.5))
        p = i_title.text_frame.add_paragraph()
        p.text = txt["ppt_business_impact"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30), lang)
        
        i_box = slide.shapes.add_textbox(Inches(0.5), Inches(4.1), Inches(6), Inches(1.2))
        tf = i_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = t_data['impact']
        set_font(p.font, 14, False, RGBColor(220, 53, 69), lang)

        # Suggestion
        s_bg = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, Inches(0.5), Inches(5.8), Inches(12.333), Inches(1.5))
        s_bg.fill.solid()
        s_bg.fill.fore_color.rgb = RGBColor(230, 244, 234)
        s_bg.line.color.rgb = RGBColor(40, 167, 69)
        s_box = slide.shapes.add_textbox(Inches(0.7), Inches(5.9), Inches(11.9), Inches(1.3))
        tf = s_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        p = tf.add_paragraph()
        p.text = txt["ppt_slide_sugg_title"]
        set_font(p.font, 16, True, RGBColor(21, 87, 36), lang)
        p = tf.add_paragraph()
        p.text = t_data['suggestion']
        set_font(p.font, 14, False, RGBColor(21, 87, 36), lang)

        # Right Column
        ex_title = slide.shapes.add_textbox(Inches(7), Inches(1.5), Inches(5.8), Inches(0.5))
        p = ex_title.text_frame.add_paragraph()
        p.text = txt["ppt_slide_ex_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30), lang)
        
        ex_box = slide.shapes.add_textbox(Inches(7), Inches(2.0), Inches(5.8), Inches(1.5)) 
        tf = ex_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE
        
        for idx, url in enumerate(s['examples'][:4]): 
            p = tf.add_paragraph()
            if s['id'] == 'duplicate' and "Duplicate Group:" in url:
                 parts = url.split("\n")
                 p.text = f"Group {idx+1}:\n   â€¢ {parts[1].replace('- ', '').strip()}\n   â€¢ {parts[2].replace('- ', '').strip()}"
                 set_font(p.font, 10, False, RGBColor(80, 80, 80), lang)
            else:
                 p.text = f"â€¢ {url}"
                 set_font(p.font, 11, False, RGBColor(0, 102, 204), lang)
            p.space_after = Pt(6)

        # Visualization Logic
        is_serp = any(k in s['id'] for k in ["title", "desc", "favicon", "alt", "lcp", "inp", "cls", "3xx", "fcp"])
        is_rich = "jsonld" in s['id']
        is_code = "js_links" in s['id'] or "anchor" in s['id']
        is_hreflang = "hreflang" in s['id']
        is_cwv = any(k in s['id'] for k in ["lcp", "inp", "cls", "fcp", "risk"])
        is_img = "alt" in s['id'] or "favicon" in s['id']
        is_3xx = "3xx" in s['id'] 
        
        ev = s.get('example_evidence', '')
        ex_url = s['examples'][0] if s['examples'] else "example.com"
        if "Duplicate" in ex_url: ex_url = ex_url.split("\n")[1].replace("- ", "").strip()
        if "3xx" in s['id'] and s.get('args'): ev = s['args'][0]

        if is_code:
            draw_code_preview(slide, txt, lang)
        elif is_hreflang:
            type_str = s['id']
            if "invalid" in type_str and s.get('args'):
                type_str = f"invalid: {s['args'][0]}"
            draw_hreflang_preview(slide, ex_url, type_str, txt, lang)
        elif is_rich:
            draw_rich_snippet_preview(slide, ex_url, txt, lang)
        elif is_serp:
            draw_serp_preview(slide, s['id'], t_data['title'], ev, ex_url, txt, lang)

    out = BytesIO()
    prs.save(out)
    out.seek(0)
    return out

# --- 7. UI Logic (No indentation) ---
if 'audit_data' not in st.session_state: st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state: st.session_state['audit_issues'] = []
if 'language' not in st.session_state: st.session_state['language'] = "zh"
if 'cwv_data' not in st.session_state: st.session_state['cwv_data'] = None
if 'sitemap_hreflang_found' not in st.session_state: st.session_state['sitemap_hreflang_found'] = False

lang = st.session_state['language']
ui = TRANSLATIONS[lang]

with st.sidebar:
    st.title(ui["sidebar_title"])
    st.caption(ui["sidebar_caption"])
    st.divider()
    
    sl = st.radio(ui["lang_label"], ["ä¸­æ–‡", "English"], index=0 if lang=="zh" else 1)
    if (sl == "ä¸­æ–‡" and lang == "en") or (sl == "English" and lang == "zh"):
        st.session_state['language'] = "zh" if sl == "ä¸­æ–‡" else "en"
        st.rerun()

    st.divider()
    opts = ui["nav_options"]
    keys = ["input", "dashboard", "matrix", "ppt"]
    sel = st.radio(ui["nav_label"], opts)
    menu_key = keys[opts.index(sel)]
    
    st.divider()
    if st.session_state['audit_data']:
        st.success(ui["cache_info"].format(len(st.session_state['audit_data'])))
        st.markdown(f"**{ui['sitemap_status_title']}**")
        if st.session_state['sitemap_hreflang_found']: st.caption(ui["sitemap_found_href"])
        else: st.caption(ui["sitemap_no_href"])
        
        if st.button(ui["clear_data"]):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.session_state['cwv_data'] = None
            st.rerun()

if menu_key == "input":
    st.header(ui["input_header"])
    st.info(ui["input_info"])
    c1, c2 = st.columns([3, 1])
    with c1: target_url = st.text_input(ui["input_label"], placeholder=ui["input_placeholder"])
    with c2: max_pages = st.number_input(ui.get("max_pages_label", "Max Pages"), min_value=1, max_value=1000, value=100)
    
    with st.expander(ui.get("adv_settings", "Advanced")):
        allow_sub = st.checkbox(ui["allow_subdomains_label"], value=False)
        allow_out = st.checkbox(ui["allow_outside_folder_label"], value=False)
        check_robots_flag = st.checkbox(ui["check_robots_label"], value=True)
        crawl_sitemap_flag = st.checkbox(ui["crawl_sitemap_label"], value=True)
        baidu_mode_flag = st.checkbox(ui["baidu_mode_label"], value=False)
        manual_sitemaps_text = st.text_area(ui.get("manual_sitemaps", "Manual Sitemaps"), placeholder="https://example.com/sitemap.xml")
        manual_sitemaps = [s.strip() for s in manual_sitemaps_text.split('\n') if s.strip()]
        manual_pages_text = st.text_area(ui.get("manual_pages_label", "Manual Pages"), placeholder="https://example.com/page1")
        manual_pages = [s.strip() for s in manual_pages_text.split('\n') if s.strip()]
        sitemap_content_text = st.text_area(ui.get("sitemap_content_label", "Paste Sitemap Content"), height=150)

    
    with st.expander(ui.get("psi_settings", "Google PSI")):
        psi_key = st.text_input(ui.get("psi_api_key_label", "API Key"), type="password", help=ui.get("psi_api_help", ""))
        psi_list_url = st.text_input(ui.get("psi_list_url_label", "List URL"))
        psi_detail_url = st.text_input(ui.get("psi_detail_url_label", "Detail URL"))
        st.caption(ui["psi_get_key"])

    if st.button(ui["start_btn"], type="primary"):
        if not target_url or not is_valid_url(target_url): 
            st.error(ui["error_url"])
        else:
            with st.spinner(ui["spinner_crawl"].format(max_pages)):
                # Handle pasted sitemap content
                if sitemap_content_text:
                    extracted_urls = re.findall(r'<loc>\s*(https?://[^<]+)\s*</loc>', sitemap_content_text)
                    if manual_pages is None: manual_pages = []
                    manual_pages.extend(extracted_urls)

                data, issues, error_msg = crawl_website(
                    target_url, max_pages, lang, None, manual_sitemaps, psi_key, 
                    psi_list_url, psi_detail_url, check_robots_flag, crawl_sitemap_flag,
                    allow_sub, allow_out, manual_pages, baidu_mode_flag
                )
                if not data:
                    st.error(ui["error_no_data"].format(error_msg or "Unknown Error"))
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(ui["success_audit"].format(len(data)))
                    st.balloons()

elif menu_key == "dashboard":
    st.header(ui["dashboard_header"])
    if not st.session_state['audit_data']: st.warning(ui["warn_no_data"])
    else:
        if st.session_state.get('cwv_data'):
            c = st.session_state['cwv_data']
            st.subheader(ui["cwv_title"])
            st.caption(ui["cwv_source"])
            c1, c2, c3, c4 = st.columns(4)
            def metric_color(val, good, poor):
                if val <= good: return "normal"
                if val >= poor: return "inverse"
                return "off"
            c1.metric("LCP (Loading)", f"{c['LCP']:.2f}s", delta_color=metric_color(c['LCP'], 2.5, 4.0))
            c2.metric("CLS (Visual)", f"{c['CLS']:.3f}", delta_color=metric_color(c['CLS'], 0.1, 0.25))
            c3.metric("INP (Interact)", f"{c['INP']}ms", delta_color=metric_color(c['INP'], 200, 500))
            c4.metric("FCP", f"{c['FCP']:.2f}s")
            st.divider()

        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        total = len(issues)
        score = max(0, 100 - int(total * 0.5))
        critical = len([i for i in issues if i['severity'] == 'Critical'])
        
        k1, k2, k3, k4 = st.columns(4)
        k1.metric(ui["kpi_health"], f"{score}/100")
        k2.metric(ui["kpi_pages"], str(len(df)))
        k3.metric(ui["kpi_issues"], str(total), delta_color="inverse")
        k4.metric(ui["kpi_critical"], str(critical), delta_color="inverse")
        
        st.divider()
        c1, c2 = st.columns(2)
        with c1:
            st.subheader(ui["chart_issues"])
            if issues:
                issue_counts = pd.DataFrame(issues)['id'].value_counts().reset_index()
                issue_counts.columns = ['id', 'count']
                issue_counts['name'] = issue_counts['id'].apply(lambda x: get_translated_text(x, lang)['title'])
                st.bar_chart(issue_counts.set_index('name'))
            else: st.info(ui["chart_no_issues"])
        with c2:
            st.subheader(ui["chart_status"])
            if not df.empty: st.bar_chart(df['Status'].value_counts())

elif menu_key == "matrix":
    st.header(ui["matrix_header"])
    if not st.session_state['audit_data']: st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(df, use_container_width=True)
        st.download_button(ui["download_csv"], df.to_csv().encode('utf-8'), "audit.csv")

elif menu_key == "ppt":
    st.header(ui["ppt_header"])
    if not st.session_state['audit_issues']: st.warning(ui["warn_no_data"])
    else:
        raw = st.session_state['audit_issues']
        grouped = {}
        for i in raw:
            iid = i['id']
            if iid not in grouped:
                grouped[iid] = {
                    "id": iid, "category": i['category'], "severity": i['severity'],
                    "count": 0, "examples": [], "args": i.get('args', []),
                    "example_evidence": i.get("evidence", "")
                }
            grouped[iid]['count'] += 1
            if len(grouped[iid]['examples']) < 5:
                if iid == "duplicate" and "meta" in i:
                     # Clean grouping for duplicate
                     grouped[iid]['examples'].append(f"Duplicate Group:\n- {i['url']}\n- {i['meta']}")
                else:
                     grouped[iid]['examples'].append(i['url'])
        
        slides = sorted(list(grouped.values()), key=lambda x: (
            CATEGORY_ORDER.index(x['category']),
            get_issue_priority(x['id']),
            SEVERITY_ORDER.get(x['severity'], 3)
        ))
        
        st.write(f"### {ui['ppt_download_header']}")
        st.info(ui["ppt_info"])
        if st.button(ui["ppt_btn"]):
            with st.spinner("Generating..."):
                f = create_styled_pptx(slides, lang)
                st.download_button(ui["ppt_btn"], f, f"seo_audit_{lang}.pptx")
        
        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(slides): st.session_state.slide_index = 0
        
        s = slides[st.session_state.slide_index]
        t_data = get_translated_text(s['id'], lang, s['args'])
        
        with st.container(border=True):
            st.caption(f"ğŸ“‚ {ui.get('cat_'+s['category'], s['category'])}")
            st.markdown(f"### {ui['ppt_slide_title']} {t_data['title']}")
            
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if s['severity'] == "Critical" else "orange"
                st.markdown(f"**{ui['ppt_severity']}** :{color}[{s['severity']}]")
                st.markdown(f"**{ui['ppt_impact']}** {ui['ppt_impact_desc'].format(s['count'])}")
                
                st.markdown(f"**{ui['ppt_desc']}**")
                st.write(t_data['desc'])
                
                st.markdown(f"**{ui['ppt_business_impact']}**") 
                st.error(t_data['impact']) 
                
                st.info(f"{ui['ppt_sugg']} {t_data['suggestion']}")
            with c2:
                # Visualization Logic
                is_serp = any(k in s['id'] for k in ["title", "desc", "favicon", "alt", "lcp", "inp", "cls", "3xx", "fcp"])
                is_rich = "jsonld" in s['id']
                is_code = "js_links" in s['id'] or "anchor" in s['id']
                is_hreflang = "hreflang" in s['id']
                is_cwv = any(k in s['id'] for k in ["lcp", "inp", "cls", "fcp", "risk"])
                is_img = "alt" in s['id'] or "favicon" in s['id']
                is_3xx = "3xx" in s['id'] 
                
                ev = s.get('example_evidence', '')
                ex_url = s['examples'][0] if s['examples'] else "example.com"
                if "Duplicate" in ex_url: ex_url = ex_url.split("\n")[1].replace("- ", "").strip()
                if "3xx" in s['id'] and s.get('args'): ev = s['args'][0]

                if is_code:
                    draw_code_preview(slide, txt, lang)
                elif is_hreflang:
                    type_str = s['id']
                    if "invalid" in type_str and s.get('args'):
                        type_str = f"invalid: {s['args'][0]}"
                    draw_hreflang_preview(slide, ex_url, type_str, txt, lang)
                elif is_rich:
                    draw_rich_snippet_preview(slide, ex_url, txt, lang)
                elif is_serp:
                    draw_serp_preview(slide, s['id'], t_data['title'], ev, ex_url, txt, lang)

    out = BytesIO()
    prs.save(out)
    out.seek(0)
    return out
