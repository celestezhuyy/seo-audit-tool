import streamlit as st
import time
import pandas as pd
import requests
import hashlib
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. å›½é™…åŒ–å­—å…¸ (i18n) ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "å®æ—¶çˆ¬è™«ç‰ˆ v2.4",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        
        "input_header": "å¼€å§‹æ–°çš„å®¡è®¡",
        "input_info": "è¯´æ˜: åŒ…å«é‡å¤å†…å®¹æ£€æµ‹ã€è½¯404å¢å¼ºæ£€æµ‹åŠç¾åŒ–ç‰ˆ PPT å¯¼å‡ºåŠŸèƒ½ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°", # æ–°å¢
        "start_btn": "å¼€å§‹çœŸå®çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨å¯åŠ¨çˆ¬è™« (Max {} pages)...", # ä¿®æ”¹ä¸ºåŠ¨æ€å ä½ç¬¦
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚",
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP çŠ¶æ€ç åˆ†å¸ƒ",
        
        "matrix_header": "å…¨ç«™æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼Œé‡‡ç”¨ç°ä»£æ’ç‰ˆå’Œé…è‰²ã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "å…¨ç«™å…± **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "æè¿°:",
        "ppt_sugg": "ğŸ’¡ å»ºè®®:",
        "ppt_examples": "ğŸ” ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        
        # PPT Static Text
        "ppt_cover_title": "SEO æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro",
        "ppt_slide_desc_title": "é—®é¢˜æè¿° & å½±å“",
        "ppt_slide_count_title": "å—å½±å“é¡µé¢æ•°é‡: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹",
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
    },
    "en": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "Live Crawler Edition v2.4",
        "nav_label": "Navigation",
        "nav_options": ["Input URL", "Dashboard", "Data Matrix", "PPT Generator"],
        "lang_label": "Language / è¯­è¨€",
        "clear_data": "Clear Data & Reset",
        "cache_info": "Cached {} pages",
        
        "input_header": "Start New Audit",
        "input_info": "Note: Includes Duplicate Content detection, Soft 404s, and Styled PPT Export.",
        "input_label": "Target URL",
        "input_placeholder": "https://example.com",
        "max_pages_label": "Max Pages to Crawl", # New
        "start_btn": "Start Live Crawl",
        "error_url": "Invalid URL format",
        "spinner_crawl": "Starting Crawler (Max {} pages)...", # Dynamic
        "error_no_data": "No pages crawled.",
        "success_audit": "Audit Complete! Analyzed {} pages.",
        
        "dashboard_header": "Executive Summary",
        "warn_no_data": "No data available.",
        "kpi_health": "Health Score",
        "kpi_pages": "Analyzed Pages",
        "kpi_issues": "Total Issues",
        "kpi_critical": "Critical Issues",
        "chart_issues": "Issue Distribution",
        "chart_no_issues": "No significant issues found.",
        "chart_status": "HTTP Status Codes",
        
        "matrix_header": "Data Matrix (Big Sheet)",
        "download_csv": "Download CSV Report",
        
        "ppt_header": "Pitch Deck Preview",
        "ppt_success_no_issues": "No critical issues found.",
        "ppt_download_header": "ğŸ“¥ Export Report",
        "ppt_info": "Note: PPT is optimized for 16:9 widescreen with modern layout.",
        "ppt_btn": "Generate & Download .pptx",
        "ppt_preview_header": "Web Preview",
        "ppt_slide_title": "Issue Type:",
        "ppt_severity": "Severity:",
        "ppt_impact": "Impact:",
        "ppt_impact_desc": "Affects **{}** pages site-wide.",
        "ppt_desc": "Description:",
        "ppt_sugg": "ğŸ’¡ Suggestion:",
        "ppt_examples": "ğŸ” Examples:",
        "ppt_prev": "â¬…ï¸ Previous",
        "ppt_next": "Next â¡ï¸",
        
        # PPT Static Text
        "ppt_cover_title": "SEO Technical Audit",
        "ppt_cover_sub": "Generated by AuditAI Pro",
        "ppt_slide_desc_title": "Description & Impact",
        "ppt_slide_count_title": "Affected Pages: {}",
        "ppt_slide_ex_title": "Example URLs",
        "ppt_slide_sugg_title": "ğŸ’¡ Recommendation:",
    }
}

# --- 3. çˆ¬è™«æ ¸å¿ƒå¼•æ“ (æ”¯æŒå¤šè¯­è¨€) ---

def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def check_site_level_assets(start_url, lang="zh"):
    """æ£€æŸ¥ç«™ç‚¹çº§åˆ«çš„ SEO èµ„äº§"""
    issues = []
    parsed_url = urlparse(start_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    
    # æ–‡æœ¬å­—å…¸
    txt = {
        "zh": {
            "no_robots": "ç¼ºå¤± Robots.txt",
            "no_robots_desc": "æ— æ³•åœ¨æ ¹ç›®å½•æ‰¾åˆ° robots.txt æ–‡ä»¶ã€‚",
            "no_robots_sugg": "åœ¨ç½‘ç«™æ ¹ç›®å½•åˆ›å»º robots.txt æ–‡ä»¶ã€‚",
            "no_sitemap": "æ ¹ç›®å½•æœªå‘ç° Sitemap.xml",
            "no_sitemap_desc": "æ ¹ç›®å½•æ—  sitemap.xmlã€‚",
            "no_sitemap_sugg": "ç¡®ä¿ Sitemap å¯è®¿é—®å¹¶åœ¨ robots.txt ä¸­å¼•ç”¨ã€‚"
        },
        "en": {
            "no_robots": "Missing Robots.txt",
            "no_robots_desc": "Robots.txt file not found in root directory.",
            "no_robots_sugg": "Create robots.txt in root directory.",
            "no_sitemap": "Sitemap.xml Not Found",
            "no_sitemap_desc": "Sitemap.xml not found in root directory.",
            "no_sitemap_sugg": "Ensure Sitemap is accessible and linked in robots.txt."
        }
    }
    t = txt[lang]

    # 1. Robots.txt
    robots_url = urljoin(base_url, "/robots.txt")
    try:
        r = requests.head(robots_url, timeout=5)
        if r.status_code != 200:
            issues.append({
                "severity": "Medium",
                "title": t["no_robots"],
                "desc": t["no_robots_desc"],
                "suggestion": t["no_robots_sugg"],
                "url": robots_url,
                "meta": f"Status: {r.status_code}"
            })
    except:
        pass 

    # 2. Sitemap.xml
    sitemap_url = urljoin(base_url, "/sitemap.xml")
    try:
        r = requests.head(sitemap_url, timeout=5)
        if r.status_code != 200:
            issues.append({
                "severity": "Low",
                "title": t["no_sitemap"],
                "desc": t["no_sitemap_desc"],
                "suggestion": t["no_sitemap_sugg"],
                "url": sitemap_url,
                "meta": f"Status: {r.status_code}"
            })
    except:
        pass

    return issues

def analyze_page(url, html_content, status_code, lang="zh"):
    """åˆ†æå•ä¸ªé¡µé¢ï¼Œæ”¯æŒå¤šè¯­è¨€è¿”å›"""
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    
    # è¯­è¨€åŒ…
    txt = {
        "zh": {
            "missing_title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title)",
            "missing_title_desc": "é¡µé¢æ²¡æœ‰ <title> æ ‡ç­¾ã€‚",
            "missing_title_sugg": "æ·»åŠ æè¿°æ€§æ ‡é¢˜ã€‚",
            "short_title": "æ ‡é¢˜è¿‡çŸ­",
            "short_title_desc": "æ ‡é¢˜è¿‡çŸ­ï¼Œéš¾ä»¥è¦†ç›–å…³é”®è¯ã€‚",
            "short_title_sugg": "æ‰©å……æ ‡é¢˜é•¿åº¦ã€‚",
            "long_title": "æ ‡é¢˜è¿‡é•¿",
            "long_title_desc": "æ ‡é¢˜è¿‡é•¿ï¼Œå¯èƒ½è¢«æˆªæ–­ã€‚",
            "long_title_sugg": "ç²¾ç®€æ ‡é¢˜é•¿åº¦ã€‚",
            "missing_desc": "ç¼ºå¤±å…ƒæè¿°",
            "missing_desc_desc": "ç¼ºå¤± Meta Descriptionï¼Œå½±å“ç‚¹å‡»ç‡ã€‚",
            "missing_desc_sugg": "æ·»åŠ  Meta Descriptionã€‚",
            "short_desc": "å…ƒæè¿°è¿‡çŸ­",
            "short_desc_desc": "å…ƒæè¿°å†…å®¹è¿‡å°‘ã€‚",
            "short_desc_sugg": "æ‰©å……å…ƒæè¿°è‡³ 120-160 å­—ç¬¦ã€‚",
            "missing_h1": "ç¼ºå¤± H1 æ ‡ç­¾",
            "missing_h1_desc": "é¡µé¢ç¼ºä¹ H1 ä¸»æ ‡é¢˜ã€‚",
            "missing_h1_sugg": "æ·»åŠ å”¯ä¸€çš„ H1 æ ‡ç­¾ã€‚",
            "missing_viewport": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®",
            "missing_viewport_desc": "æœªé…ç½® Viewportï¼Œç§»åŠ¨ç«¯ä½“éªŒå·®ã€‚",
            "missing_viewport_sugg": "æ·»åŠ  viewport meta æ ‡ç­¾ã€‚",
            "missing_canonical": "ç¼ºå¤± Canonical æ ‡ç­¾",
            "missing_canonical_desc": "æœªæŒ‡å®šè§„èŒƒé“¾æ¥ï¼Œå¯èƒ½å¯¼è‡´é‡å¤å†…å®¹ã€‚",
            "missing_canonical_sugg": "æ·»åŠ  canonical æ ‡ç­¾æŒ‡å‘æ ‡å‡† URLã€‚",
            "missing_favicon": "ç¼ºå¤± Favicon",
            "missing_favicon_desc": "æœªè®¾ç½®ç½‘ç«™å›¾æ ‡ã€‚",
            "missing_favicon_sugg": "æ·»åŠ  link rel='icon'ã€‚",
            "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®",
            "missing_jsonld_desc": "æœªæ£€æµ‹åˆ° JSON-LDã€‚",
            "missing_jsonld_sugg": "æ·»åŠ  JSON-LD ç»“æ„åŒ–æ•°æ®ã€‚",
            "missing_hreflang": "ç¼ºå¤± Hreflang",
            "missing_hreflang_desc": "æœªå‘ç°è¯­è¨€åŒºåŸŸæ ‡è®°ã€‚",
            "missing_hreflang_sugg": "å¦‚ä¸ºå¤šè¯­è¨€ç«™ç‚¹è¯·æ·»åŠ  hreflangã€‚",
            "url_underscore": "URL åŒ…å«ä¸‹åˆ’çº¿",
            "url_underscore_desc": "å»ºè®®ä½¿ç”¨è¿å­—ç¬¦ (-)ã€‚",
            "url_underscore_sugg": "æ›¿æ¢ä¸‹åˆ’çº¿ä¸ºè¿å­—ç¬¦ã€‚",
            "url_uppercase": "URL åŒ…å«å¤§å†™å­—æ¯",
            "url_uppercase_desc": "å»ºè®®ç»Ÿä¸€ä½¿ç”¨å°å†™ã€‚",
            "url_uppercase_sugg": "è½¬æ¢ä¸ºå°å†™ URLã€‚",
            "js_links": "å‘ç° JS ä¼ªé“¾æ¥",
            "js_links_desc": "ä½¿ç”¨äº† href='javascript:'ï¼Œçˆ¬è™«æ— æ³•æŠ“å–ã€‚",
            "js_links_sugg": "ä½¿ç”¨æ ‡å‡† <a href>ã€‚",
            "soft_404": "ç–‘ä¼¼è½¯ 404 (Soft 404)",
            "soft_404_desc": "è¿”å› 200 ä½†å†…å®¹æ˜¾ç¤ºæœªæ‰¾åˆ°ã€‚",
            "soft_404_sugg": "é…ç½®æœåŠ¡å™¨è¿”å› 404 çŠ¶æ€ç ã€‚",
            "missing_alt": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§",
            "missing_alt_desc": "å›¾ç‰‡ç¼ºå°‘æ›¿ä»£æ–‡æœ¬ã€‚",
            "missing_alt_sugg": "æ·»åŠ  alt å±æ€§ã€‚",
            "duplicate": "å‘ç°é‡å¤å†…å®¹",
            "duplicate_desc": "å†…å®¹ä¸å¦ä¸€é¡µé¢é«˜åº¦é‡å¤ã€‚",
            "duplicate_sugg": "ä½¿ç”¨ Canonical æˆ–åˆå¹¶é¡µé¢ã€‚"
        },
        "en": {
            "missing_title": "Missing Title Tag",
            "missing_title_desc": "Page has no <title> tag.",
            "missing_title_sugg": "Add a descriptive title.",
            "short_title": "Title Too Short",
            "short_title_desc": "Title is too short for keywords.",
            "short_title_sugg": "Increase title length.",
            "long_title": "Title Too Long",
            "long_title_desc": "Title may be truncated in SERPs.",
            "long_title_sugg": "Shorten the title.",
            "missing_desc": "Missing Meta Description",
            "missing_desc_desc": "Missing description affects CTR.",
            "missing_desc_sugg": "Add a meta description.",
            "short_desc": "Meta Description Too Short",
            "short_desc_desc": "Description content is too thin.",
            "short_desc_sugg": "Expand to 120-160 chars.",
            "missing_h1": "Missing H1 Tag",
            "missing_h1_desc": "Page lacks a main H1 heading.",
            "missing_h1_sugg": "Add a unique H1 tag.",
            "missing_viewport": "Missing Mobile Viewport",
            "missing_viewport_desc": "No viewport tag found.",
            "missing_viewport_sugg": "Add viewport meta tag.",
            "missing_canonical": "Missing Canonical Tag",
            "missing_canonical_desc": "Canonical URL not specified.",
            "missing_canonical_sugg": "Add canonical link tag.",
            "missing_favicon": "Missing Favicon",
            "missing_favicon_desc": "No site icon detected.",
            "missing_favicon_sugg": "Add link rel='icon'.",
            "missing_jsonld": "Missing Structured Data",
            "missing_jsonld_desc": "No JSON-LD detected.",
            "missing_jsonld_sugg": "Add JSON-LD schema.",
            "missing_hreflang": "Missing Hreflang",
            "missing_hreflang_desc": "No language/region targeting found.",
            "missing_hreflang_sugg": "Add hreflang if multilingual.",
            "url_underscore": "URL Contains Underscores",
            "url_underscore_desc": "Hyphens (-) are preferred.",
            "url_underscore_sugg": "Replace underscores with hyphens.",
            "url_uppercase": "URL Contains Uppercase",
            "url_uppercase_desc": "URLs are case-sensitive.",
            "url_uppercase_sugg": "Use lowercase URLs.",
            "js_links": "JavaScript Pseudo-links",
            "js_links_desc": "href='javascript:' found.",
            "js_links_sugg": "Use standard <a href>.",
            "soft_404": "Suspected Soft 404",
            "soft_404_desc": "Returns 200 but content says 'Not Found'.",
            "soft_404_sugg": "Return 404 HTTP status code.",
            "missing_alt": "Images Missing Alt Text",
            "missing_alt_desc": "Images lack alternative text.",
            "missing_alt_sugg": "Add alt attributes.",
            "duplicate": "Duplicate Content Detected",
            "duplicate_desc": "Content matches another page.",
            "duplicate_sugg": "Use canonicals or merge pages."
        }
    }
    t = txt[lang]
    
    # æå–çº¯æ–‡æœ¬
    for script in soup(["script", "style"]):
        script.extract()
    text_content = soup.get_text().strip()
    text_content_lower = text_content.lower()
    content_hash = get_content_hash(text_content)

    # --- æ£€æŸ¥é€»è¾‘ (ä½¿ç”¨ t[key] è·å–æ–‡æœ¬) ---
    
    # 1. Title
    title_tag = soup.title
    title = title_tag.string.strip() if title_tag and title_tag.string else None
    
    if not title:
        issues.append({"severity": "High", "title": t["missing_title"], "desc": t["missing_title_desc"], "suggestion": t["missing_title_sugg"], "url": url})
    elif len(title) < 10:
         issues.append({"severity": "Medium", "title": t["short_title"], "desc": t["short_title_desc"], "suggestion": t["short_title_sugg"], "url": url})
    elif len(title) > 60:
         issues.append({"severity": "Low", "title": t["long_title"], "desc": t["long_title_desc"], "suggestion": t["long_title_sugg"], "url": url})

    # 2. Meta Description
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = meta_desc['content'].strip() if meta_desc and meta_desc.get('content') else None
    
    if not desc_content:
        issues.append({"severity": "High", "title": t["missing_desc"], "desc": t["missing_desc_desc"], "suggestion": t["missing_desc_sugg"], "url": url})
    elif len(desc_content) < 50:
        issues.append({"severity": "Low", "title": t["short_desc"], "desc": t["short_desc_desc"], "suggestion": t["short_desc_sugg"], "url": url})

    # 3. H1
    h1 = soup.find('h1')
    h1_text = h1.get_text().strip() if h1 else "No H1"
    if not h1:
        issues.append({"severity": "High", "title": t["missing_h1"], "desc": t["missing_h1_desc"], "suggestion": t["missing_h1_sugg"], "url": url})

    # 4. Viewport
    viewport = soup.find('meta', attrs={'name': 'viewport'})
    if not viewport:
        issues.append({"severity": "Critical", "title": t["missing_viewport"], "desc": t["missing_viewport_desc"], "suggestion": t["missing_viewport_sugg"], "url": url})

    # 5. Canonical
    canonical = soup.find('link', attrs={'rel': 'canonical'})
    if not canonical:
        issues.append({"severity": "Medium", "title": t["missing_canonical"], "desc": t["missing_canonical_desc"], "suggestion": t["missing_canonical_sugg"], "url": url})

    # 6. Favicon
    favicon = soup.find('link', rel=lambda x: x and 'icon' in x.lower())
    if not favicon:
         issues.append({"severity": "Low", "title": t["missing_favicon"], "desc": t["missing_favicon_desc"], "suggestion": t["missing_favicon_sugg"], "url": url})

    # 7. JSON-LD
    schema = soup.find('script', type='application/ld+json')
    if not schema:
         issues.append({"severity": "Medium", "title": t["missing_jsonld"], "desc": t["missing_jsonld_desc"], "suggestion": t["missing_jsonld_sugg"], "url": url})

    # 8. Hreflang
    hreflang = soup.find('link', hreflang=True)
    if not hreflang:
        issues.append({"severity": "Low", "title": t["missing_hreflang"], "desc": t["missing_hreflang_desc"], "suggestion": t["missing_hreflang_sugg"], "url": url})

    # 9. URL Structure
    parsed_url = urlparse(url)
    path = parsed_url.path
    if '_' in path:
         issues.append({"severity": "Low", "title": t["url_underscore"], "desc": t["url_underscore_desc"], "suggestion": t["url_underscore_sugg"], "url": url})
    if any(c.isupper() for c in path):
         issues.append({"severity": "Medium", "title": t["url_uppercase"], "desc": t["url_uppercase_desc"], "suggestion": t["url_uppercase_sugg"], "url": url})

    # 10. JS Links
    js_links = soup.find_all('a', href=lambda x: x and x.lower().startswith('javascript:'))
    if js_links:
        issues.append({"severity": "High", "title": t["js_links"], "desc": t["js_links_desc"], "suggestion": t["js_links_sugg"], "url": url, "meta": f"Count: {len(js_links)}"})

    # 11. Soft 404
    is_soft_404 = False
    error_keywords = ["page not found", "404 error", "é¡µé¢æœªæ‰¾åˆ°", "404 é”™è¯¯"]
    if title and any(k in title.lower() for k in error_keywords): is_soft_404 = True
    elif h1_text != "No H1" and any(k in h1_text.lower() for k in error_keywords): is_soft_404 = True
    elif any(k in text_content_lower[:500] for k in error_keywords): is_soft_404 = True

    if status_code == 200 and is_soft_404:
        issues.append({"severity": "Critical", "title": t["soft_404"], "desc": t["soft_404_desc"], "suggestion": t["soft_404_sugg"], "url": url})
        
    # 12. Alt
    images = soup.find_all('img')
    missing_alt = 0
    for img in images:
        if not img.get('alt'): missing_alt += 1
    if missing_alt > 0:
        issues.append({"severity": "Medium", "title": t["missing_alt"], "desc": t["missing_alt_desc"], "suggestion": t["missing_alt_sugg"], "url": url, "meta": f"{missing_alt} images"})

    # Internal Links
    internal_links = set()
    base_domain = urlparse(url).netloc
    for a in soup.find_all('a', href=True):
        link = urljoin(url, a['href'])
        parsed_link = urlparse(link)
        if parsed_link.netloc == base_domain:
            if not any(link.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.pdf', '.css', '.js', '.zip']):
                internal_links.add(link)

    return {
        "URL": url,
        "Status": status_code,
        "Title": title or "No Title",
        "H1": h1_text,
        "Links_Count": len(internal_links),
        "Issues_Count": len(issues),
        "Content_Hash": content_hash
    }, issues, internal_links

def crawl_website(start_url, max_pages=100, lang="zh"):
    """æ‰§è¡Œçˆ¬å–ï¼Œä¼ é€’è¯­è¨€å‚æ•°"""
    visited = set()
    seen_hashes = {}
    queue = [start_url]
    results_data = []
    all_issues = []
    
    # è·å–å¯¹åº”è¯­è¨€çš„ UI æ–‡æœ¬
    t_dup_title = "Duplicate Content Detected" if lang == "en" else "å‘ç°é‡å¤å†…å®¹"
    t_dup_desc = "Content matches another page." if lang == "en" else "å†…å®¹ä¸å¦ä¸€é¡µé¢é«˜åº¦é‡å¤ã€‚"
    t_dup_sugg = "Use canonicals or merge pages." if lang == "en" else "ä½¿ç”¨ Canonical æˆ–åˆå¹¶é¡µé¢ã€‚"
    
    progress_bar = st.progress(0, text="Initializing...")
    
    try:
        site_issues = check_site_level_assets(start_url, lang=lang)
        all_issues.extend(site_issues)
    except Exception as e:
        st.toast(f"Site-check failed: {str(e)}")

    pages_crawled = 0
    
    while queue and pages_crawled < max_pages:
        url = queue.pop(0)
        if url in visited: continue
        visited.add(url)
        pages_crawled += 1
        
        progress = int((pages_crawled / max_pages) * 100)
        progress_bar.progress(progress, text=f"Crawling ({pages_crawled}/{max_pages}): {url}")
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; SEOAuditBot/2.0)'}
            response = requests.get(url, headers=headers, timeout=10)
            
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                # ä¼ é€’ lang å‚æ•°
                page_data, page_issues, new_links = analyze_page(url, response.content, response.status_code, lang=lang)
                
                # Duplicate Check
                current_hash = page_data['Content_Hash']
                if current_hash in seen_hashes:
                    original_url = seen_hashes[current_hash]
                    page_issues.append({
                        "severity": "High", "title": t_dup_title, "desc": f"{t_dup_desc} ({original_url})", 
                        "suggestion": t_dup_sugg, "url": url, "meta": f"Duplicate of: {original_url}"
                    })
                else:
                    seen_hashes[current_hash] = url

                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                for link in new_links:
                    if link not in visited and link not in queue: queue.append(link)
        except Exception as e:
            pass
    
    progress_bar.progress(100, text="Done!")
    time.sleep(0.5)
    progress_bar.empty()
    return results_data, all_issues

def create_styled_pptx(slides_data, lang="zh"):
    """ç”Ÿæˆ PPTï¼Œæ”¯æŒå¤šè¯­è¨€é™æ€æ–‡æœ¬"""
    prs = Presentation()
    prs.slide_width = Inches(13.333)
    prs.slide_height = Inches(7.5)
    
    txt = TRANSLATIONS[lang] # ä½¿ç”¨å…¨å±€ç¿»è¯‘å­—å…¸
    
    # ä¿®å¤ï¼šç›´æ¥æ“ä½œ font_objï¼Œä¸å†è®¿é—® .font å±æ€§
    def set_font(font_obj, size, bold=False, color=None):
        font_obj.size = Pt(size)
        font_obj.name = 'Microsoft YaHei' if lang == "zh" else 'Arial'
        font_obj.bold = bold
        if color: font_obj.color.rgb = color

    # --- Cover ---
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg_shape = slide.shapes.add_shape(1, Inches(0), Inches(0), Inches(13.333), Inches(7.5))
    bg_shape.fill.solid()
    bg_shape.fill.fore_color.rgb = RGBColor(18, 52, 86)
    bg_shape.line.fill.background()

    title_box = slide.shapes.add_textbox(Inches(1), Inches(2.5), Inches(11), Inches(2))
    p = title_box.text_frame.add_paragraph()
    p.text = txt["ppt_cover_title"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 54, True, RGBColor(255, 255, 255))
    
    sub_box = slide.shapes.add_textbox(Inches(1), Inches(4), Inches(11), Inches(1))
    p_sub = sub_box.text_frame.add_paragraph()
    p_sub.text = txt["ppt_cover_sub"]
    p_sub.alignment = PP_ALIGN.CENTER
    set_font(p_sub.font, 24, False, RGBColor(200, 200, 200))

    # --- Slides ---
    for issue in slides_data:
        slide = prs.slides.add_slide(prs.slide_layouts[6])
        
        # Header
        header_shape = slide.shapes.add_shape(1, Inches(0), Inches(0), Inches(13.333), Inches(1.2))
        header_shape.fill.solid()
        header_shape.fill.fore_color.rgb = RGBColor(240, 242, 246)
        header_shape.line.fill.background()
        
        header_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(10), Inches(0.8))
        p = header_box.text_frame.add_paragraph()
        p.text = issue['title']
        set_font(p.font, 32, True, RGBColor(50, 50, 50))
        
        # Severity
        sev_color = RGBColor(220, 53, 69) if issue['severity'] == "Critical" else RGBColor(253, 126, 20)
        sev_box = slide.shapes.add_textbox(Inches(11), Inches(0.35), Inches(2), Inches(0.5))
        p = sev_box.text_frame.add_paragraph()
        p.text = f"{issue['severity']}"
        p.alignment = PP_ALIGN.CENTER
        set_font(p.font, 18, True, sev_color)
        
        # Left: Desc
        desc_title_box = slide.shapes.add_textbox(Inches(0.5), Inches(1.5), Inches(6), Inches(0.5))
        p = desc_title_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_desc_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        desc_box = slide.shapes.add_textbox(Inches(0.5), Inches(2.0), Inches(6), Inches(2.5))
        tf = desc_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE # Add this
        p = tf.add_paragraph()
        p.text = issue['desc']
        set_font(p.font, 14, False, RGBColor(80, 80, 80))
        
        count_box = slide.shapes.add_textbox(Inches(0.5), Inches(3.5), Inches(6), Inches(0.5))
        p = count_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_count_title"].format(issue['count'])
        set_font(p.font, 14, True, RGBColor(100, 100, 100))

        # Right: Examples
        ex_title_box = slide.shapes.add_textbox(Inches(7), Inches(1.5), Inches(5.8), Inches(0.5))
        p = ex_title_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_ex_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        ex_box = slide.shapes.add_textbox(Inches(7), Inches(2.0), Inches(5.8), Inches(2.5))
        tf = ex_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE # Add this
        for ex_url in issue['examples'][:5]:
            p = tf.add_paragraph()
            p.text = f"â€¢ {ex_url}"
            set_font(p.font, 12, False, RGBColor(0, 102, 204))
            p.space_after = Pt(6)

        # Bottom: Suggestion
        sugg_bg = slide.shapes.add_shape(1, Inches(0.5), Inches(5.5), Inches(12.333), Inches(1.5))
        sugg_bg.fill.solid()
        sugg_bg.fill.fore_color.rgb = RGBColor(230, 244, 234)
        sugg_bg.line.color.rgb = RGBColor(40, 167, 69)
        
        sugg_box = slide.shapes.add_textbox(Inches(0.7), Inches(5.6), Inches(11.9), Inches(1.3))
        tf = sugg_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE # Add this
        
        p = tf.add_paragraph()
        p.text = txt["ppt_slide_sugg_title"]
        set_font(p.font, 16, True, RGBColor(21, 87, 36))
        
        p = tf.add_paragraph()
        p.text = issue['suggestion']
        set_font(p.font, 14, False, RGBColor(21, 87, 36))
        p.space_before = Pt(5)

    binary_output = BytesIO()
    prs.save(binary_output)
    binary_output.seek(0)
    return binary_output

# --- 4. Init Session ---
if 'audit_data' not in st.session_state: st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state: st.session_state['audit_issues'] = []
if 'language' not in st.session_state: st.session_state['language'] = "zh" # é»˜è®¤è¯­è¨€

# --- 5. Sidebar ---
# è·å–å½“å‰è¯­è¨€å¯¹åº”çš„ UI æ–‡æœ¬
lang = st.session_state['language']
ui = TRANSLATIONS[lang]

with st.sidebar:
    st.title(ui["sidebar_title"])
    st.caption(ui["sidebar_caption"])
    
    # è¯­è¨€åˆ‡æ¢
    st.divider()
    selected_lang = st.radio(ui["lang_label"], ["ä¸­æ–‡", "English"], index=0 if lang=="zh" else 1)
    new_lang = "zh" if selected_lang == "ä¸­æ–‡" else "en"
    
    if new_lang != lang:
        st.session_state['language'] = new_lang
        st.rerun()
    
    st.divider()
    
    menu_options = ui["nav_options"]
    # æ˜ å°„é€‰æ‹©å›å†…éƒ¨ key
    menu_map = {ui["nav_options"][i]: ["input", "dashboard", "matrix", "ppt"][i] for i in range(4)}
    
    selected_menu = st.radio(ui["nav_label"], menu_options)
    menu_key = menu_map[selected_menu]
    
    st.divider()
    if st.session_state['audit_data'] is not None:
        st.success(ui["cache_info"].format(len(st.session_state['audit_data'])))
        if st.button(ui["clear_data"]):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.rerun()

# --- 6. Main Logic ---

if menu_key == "input":
    st.header(ui["input_header"])
    st.info(ui["input_info"])
    
    col1, col2 = st.columns([3, 1])
    with col1:
        url_input = st.text_input(ui["input_label"], placeholder=ui["input_placeholder"])
    with col2:
        max_pages = st.number_input(ui.get("max_pages_label", "Max Pages / æœ€å¤§é¡µé¢æ•°"), min_value=1, max_value=1000, value=100)
    
    start_btn = st.button(ui["start_btn"], type="primary", use_container_width=True)
    
    if start_btn and url_input:
        if not is_valid_url(url_input):
            st.error(ui["error_url"])
        else:
            with st.spinner(ui["spinner_crawl"].format(max_pages)):
                # ä¼ é€’å½“å‰è¯­è¨€ lang å’Œ max_pages
                data, issues = crawl_website(url_input, max_pages=max_pages, lang=lang)
                if not data:
                    st.error(ui["error_no_data"])
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(ui["success_audit"].format(len(data)))
                    st.balloons()

elif menu_key == "dashboard":
    st.header(ui["dashboard_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        
        total = len(issues)
        score = max(0, 100 - int(total * 0.5))
        critical = len([i for i in issues if i['severity'] == 'Critical'])
        
        k1, k2, k3, k4 = st.columns(4)
        k1.metric(ui["kpi_health"], f"{score}/100")
        k2.metric(ui["kpi_pages"], str(len(df)))
        k3.metric(ui["kpi_issues"], str(total), delta_color="inverse")
        k4.metric(ui["kpi_critical"], str(critical), delta_color="inverse")
        
        st.divider()
        c1, c2 = st.columns(2)
        with c1:
            st.subheader(ui["chart_issues"])
            if issues:
                st.bar_chart(pd.DataFrame(issues)['title'].value_counts())
            else:
                st.info(ui["chart_no_issues"])
        with c2:
            st.subheader(ui["chart_status"])
            if not df.empty:
                st.bar_chart(df['Status'].value_counts())

elif menu_key == "matrix":
    st.header(ui["matrix_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(
            df,
            column_config={
                "URL": st.column_config.LinkColumn("URL"),
                "Status": st.column_config.NumberColumn("Status", format="%d"),
            },
            use_container_width=True,
            hide_index=True
        )
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(ui["download_csv"], csv, "audit_report.csv", "text/csv")

elif menu_key == "ppt":
    st.header(ui["ppt_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    elif not st.session_state['audit_issues']:
        st.success(ui["ppt_success_no_issues"])
    else:
        # Group issues
        raw_issues = st.session_state['audit_issues']
        grouped = {}
        for i in raw_issues:
            t = i['title']
            if t not in grouped:
                grouped[t] = {
                    "title": t, "severity": i['severity'], "desc": i['desc'], 
                    "suggestion": i['suggestion'], "count": 0, "examples": []
                }
            grouped[t]['count'] += 1
            if len(grouped[t]['examples']) < 5: grouped[t]['examples'].append(i['url'])
        
        sov_order = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}
        slides = sorted(list(grouped.values()), key=lambda x: (sov_order.get(x['severity'], 3), -x['count']))

        # Download
        st.write(f"### {ui['ppt_download_header']}")
        st.info(ui["ppt_info"])
        if st.button(ui["ppt_btn"]):
            with st.spinner("Generating..."):
                # ä¼ é€’å½“å‰è¯­è¨€ lang
                pptx = create_styled_pptx(slides, lang=lang)
                fname = f"seo_audit_report_{lang}.pptx"
                st.download_button(ui["ppt_btn"], pptx, fname, "application/vnd.openxmlformats-officedocument.presentationml.presentation")

        st.divider()
        st.subheader(ui["ppt_preview_header"])
        
        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(slides): st.session_state.slide_index = 0
        
        s = slides[st.session_state.slide_index]
        
        with st.container(border=True):
            st.markdown(f"### {ui['ppt_slide_title']} {s['title']}")
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if s['severity'] == "Critical" else "orange" if s['severity'] == "High" else "blue"
                st.markdown(f"**{ui['ppt_severity']}** :{color}[{s['severity']}]")
                st.markdown(f"**{ui['ppt_impact']}** {ui['ppt_impact_desc'].format(s['count'])}")
                st.markdown(f"**{ui['ppt_desc']}** {s['desc']}")
                st.info(f"{ui['ppt_sugg']} {s['suggestion']}")
            with c2:
                st.markdown(f"**{ui['ppt_examples']}**")
                for ex in s['examples']: st.markdown(f"- `{ex}`")

        cp, ct, cn = st.columns([1, 2, 1])
        with cp:
            if st.button(ui["ppt_prev"]):
                st.session_state.slide_index = max(0, st.session_state.slide_index - 1)
                st.rerun()
        with ct:
            st.markdown(f"<div style='text-align: center'>Slide {st.session_state.slide_index + 1} / {len(slides)}</div>", unsafe_allow_html=True)
        with cn:
            if st.button(ui["ppt_next"]):
                st.session_state.slide_index = min(len(slides) - 1, st.session_state.slide_index + 1)
                st.rerun()
