import streamlit as st
import time
import pandas as pd
import requests
import hashlib
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO

# å°è¯•å¯¼å…¥ pptx
try:
    from pptx import Presentation
    from pptx.util import Inches, Pt, Cm
    from pptx.dml.color import RGBColor
    from pptx.enum.text import PP_ALIGN, MSO_AUTO_SIZE
except ImportError:
    st.error("Missing dependencies! Please add 'python-pptx' to requirements.txt.")
    st.stop()

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="NextGen SEO Auditor",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. å›½é™…åŒ–å­—å…¸ (i18n) ---
TRANSLATIONS = {
    "zh": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "å®æ—¶çˆ¬è™«ç‰ˆ v2.6",
        "nav_label": "åŠŸèƒ½å¯¼èˆª",
        "nav_options": ["è¾“å…¥ç½‘å€", "ä»ªè¡¨ç›˜", "æ•°æ®çŸ©é˜µ", "PPT ç”Ÿæˆå™¨"],
        "lang_label": "è¯­è¨€ / Language",
        "clear_data": "æ¸…é™¤æ•°æ®å¹¶é‡ç½®",
        "cache_info": "å·²ç¼“å­˜ {} ä¸ªé¡µé¢",
        "sitemap_status_title": "Sitemap æ‰«æçŠ¶æ€:",
        "sitemap_found_href": "âœ… å‘ç° Hreflang é…ç½®",
        "sitemap_no_href": "âš ï¸ æœªå‘ç° Hreflang",
        "sitemap_missing": "âŒ æœªæ‰¾åˆ° Sitemap",
        
        "input_header": "å¼€å§‹æ–°çš„å®¡è®¡",
        "input_info": "è¯´æ˜: å¢å¼ºç‰ˆ Sitemap/Robots æ£€æµ‹ï¼ˆæ”¯æŒé‡å®šå‘ï¼‰ï¼Œæ”¯æŒ Sitemap Hreflang éªŒè¯ã€‚",
        "input_label": "è¾“å…¥ç›®æ ‡ç½‘å€",
        "input_placeholder": "https://example.com",
        "max_pages_label": "æœ€å¤§çˆ¬å–é¡µé¢æ•°", 
        "start_btn": "å¼€å§‹çœŸå®çˆ¬å–",
        "error_url": "ç½‘å€æ ¼å¼é”™è¯¯",
        "spinner_crawl": "æ­£åœ¨å¯åŠ¨çˆ¬è™« (Max {} pages)...", 
        "error_no_data": "æœªèƒ½çˆ¬å–åˆ°ä»»ä½•é¡µé¢ã€‚",
        "success_audit": "å®¡è®¡å®Œæˆï¼å…±åˆ†æ {} ä¸ªé¡µé¢ã€‚",
        
        "dashboard_header": "æ‰§è¡Œæ‘˜è¦ (Executive Summary)",
        "warn_no_data": "æš‚æ— æ•°æ®ã€‚",
        "kpi_health": "ç½‘ç«™å¥åº·åº¦",
        "kpi_pages": "å·²åˆ†æé¡µé¢",
        "kpi_issues": "å‘ç°é—®é¢˜æ€»æ•°",
        "kpi_critical": "ä¸¥é‡é—®é¢˜",
        "chart_issues": "é—®é¢˜ç±»å‹åˆ†å¸ƒ",
        "chart_no_issues": "æœªå‘ç°æ˜æ˜¾é—®é¢˜ã€‚",
        "chart_status": "HTTP çŠ¶æ€ç åˆ†å¸ƒ",
        
        "matrix_header": "çˆ¬å–æ•°æ®æ˜ç»† (Big Sheet)",
        "download_csv": "ä¸‹è½½ CSV æŠ¥å‘Š",
        
        "ppt_header": "æ¼”ç¤ºæ–‡ç¨¿é¢„è§ˆ (Pitch Deck Mode)",
        "ppt_success_no_issues": "æ— ä¸¥é‡é—®é¢˜ã€‚",
        "ppt_download_header": "ğŸ“¥ å¯¼å‡ºæŠ¥å‘Š",
        "ppt_info": "è¯´æ˜ï¼šç”Ÿæˆçš„ PPT å·²ä¼˜åŒ–ä¸º 16:9 å®½å±ï¼Œé‡‡ç”¨ç°ä»£æ’ç‰ˆå’Œé…è‰²ã€‚",
        "ppt_btn": "ç”Ÿæˆå¹¶ä¸‹è½½ç¾åŒ–ç‰ˆ .pptx",
        "ppt_preview_header": "ç½‘é¡µç‰ˆé¢„è§ˆ",
        "ppt_slide_title": "é—®é¢˜ç±»å‹:",
        "ppt_severity": "ä¸¥é‡ç¨‹åº¦:",
        "ppt_impact": "å½±å“èŒƒå›´:",
        "ppt_impact_desc": "åœ¨å·²çˆ¬å–æ ·æœ¬ä¸­å‘ç° **{}** ä¸ªé¡µé¢ã€‚",
        "ppt_desc": "æè¿°:",
        "ppt_sugg": "ğŸ’¡ å»ºè®®:",
        "ppt_examples": "ğŸ” ç¤ºä¾‹:",
        "ppt_prev": "â¬…ï¸ ä¸Šä¸€é¡µ",
        "ppt_next": "ä¸‹ä¸€é¡µ â¡ï¸",
        
        # PPT Static Text
        "ppt_cover_title": "SEO æŠ€æœ¯å®¡è®¡æŠ¥å‘Š",
        "ppt_cover_sub": "Generated by AuditAI Pro",
        "ppt_slide_desc_title": "é—®é¢˜æè¿° & å½±å“",
        "ppt_slide_count_title": "æ ·æœ¬ä¸­å—å½±å“é¡µé¢æ•°: {} ä¸ª",
        "ppt_slide_ex_title": "å—å½±å“é¡µé¢ç¤ºä¾‹",
        "ppt_slide_sugg_title": "ğŸ’¡ ä¿®å¤å»ºè®®:",
    },
    "en": {
        "sidebar_title": "ğŸ” AuditAI Pro",
        "sidebar_caption": "Live Crawler Edition v2.6",
        "nav_label": "Navigation",
        "nav_options": ["Input URL", "Dashboard", "Data Matrix", "PPT Generator"],
        "lang_label": "Language / è¯­è¨€",
        "clear_data": "Clear Data & Reset",
        "cache_info": "Cached {} pages",
        "sitemap_status_title": "Sitemap Scan:",
        "sitemap_found_href": "âœ… Hreflang Found",
        "sitemap_no_href": "âš ï¸ No Hreflang",
        "sitemap_missing": "âŒ Sitemap Missing",
        
        "input_header": "Start New Audit",
        "input_info": "Note: Robust Sitemap/Robots detection (Redirects supported) & Sitemap Hreflang check.",
        "input_label": "Target URL",
        "input_placeholder": "https://example.com",
        "max_pages_label": "Max Pages to Crawl", 
        "start_btn": "Start Live Crawl",
        "error_url": "Invalid URL format",
        "spinner_crawl": "Starting Crawler (Max {} pages)...", 
        "error_no_data": "No pages crawled.",
        "success_audit": "Audit Complete! Analyzed {} pages.",
        
        "dashboard_header": "Executive Summary",
        "warn_no_data": "No data available.",
        "kpi_health": "Health Score",
        "kpi_pages": "Analyzed Pages",
        "kpi_issues": "Total Issues",
        "kpi_critical": "Critical Issues",
        "chart_issues": "Issue Distribution",
        "chart_no_issues": "No significant issues found.",
        "chart_status": "HTTP Status Codes",
        
        "matrix_header": "Crawled Data Matrix",
        "download_csv": "Download CSV Report",
        
        "ppt_header": "Pitch Deck Preview",
        "ppt_success_no_issues": "No critical issues found.",
        "ppt_download_header": "ğŸ“¥ Export Report",
        "ppt_info": "Note: PPT is optimized for 16:9 widescreen with modern layout.",
        "ppt_btn": "Generate & Download .pptx",
        "ppt_preview_header": "Web Preview",
        "ppt_slide_title": "Issue Type:",
        "ppt_severity": "Severity:",
        "ppt_impact": "Impact:",
        "ppt_impact_desc": "Affects **{}** pages in crawled sample.",
        "ppt_desc": "Description:",
        "ppt_sugg": "ğŸ’¡ Suggestion:",
        "ppt_examples": "ğŸ” Examples:",
        "ppt_prev": "â¬…ï¸ Previous",
        "ppt_next": "Next â¡ï¸",
        
        # PPT Static Text
        "ppt_cover_title": "SEO Technical Audit",
        "ppt_cover_sub": "Generated by AuditAI Pro",
        "ppt_slide_desc_title": "Description & Impact",
        "ppt_slide_count_title": "Affected Pages (in sample): {}",
        "ppt_slide_ex_title": "Example URLs",
        "ppt_slide_sugg_title": "ğŸ’¡ Recommendation:",
    }
}

# --- 3. çˆ¬è™«æ ¸å¿ƒå¼•æ“ (æ”¯æŒå¤šè¯­è¨€) ---

def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def get_browser_headers():
    """è¿”å›æ¨¡æ‹Ÿæµè§ˆå™¨çš„ Headersï¼Œé˜²æ­¢è¢« WAF æ‹¦æˆª"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }

def check_site_level_assets(start_url, lang="zh"):
    """
    æ£€æŸ¥ç«™ç‚¹çº§åˆ«çš„ SEO èµ„äº§ã€‚
    æ”¹è¿›ç‚¹ï¼šä½¿ç”¨ GET + allow_redirects å¤„ç† 301/302ï¼Œå¹¶æ£€æŸ¥ Sitemap å†…å®¹ä¸­çš„ Hreflangã€‚
    """
    issues = []
    sitemap_has_hreflang = False
    
    parsed_url = urlparse(start_url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    headers = get_browser_headers()
    
    # æ–‡æœ¬å­—å…¸
    txt = {
        "zh": {
            "no_robots": "ç¼ºå¤± Robots.txt",
            "no_robots_desc": "æ— æ³•åœ¨æ ¹ç›®å½•æ‰¾åˆ° robots.txt æ–‡ä»¶ï¼Œæˆ–æœåŠ¡å™¨æ‹’ç»è®¿é—® (403/404)ã€‚",
            "no_robots_sugg": "åœ¨ç½‘ç«™æ ¹ç›®å½•åˆ›å»º robots.txt æ–‡ä»¶å¹¶ç¡®ä¿å¯è®¿é—®ã€‚",
            "no_sitemap": "æ ¹ç›®å½•æœªå‘ç° Sitemap.xml",
            "no_sitemap_desc": "æ ¹ç›®å½•æ—  sitemap.xmlï¼Œä¼šé™ä½æœç´¢å¼•æ“å‘ç°æ–°é¡µé¢å’Œæ›´æ–°é¡µé¢çš„æ•ˆç‡ã€‚",
            "no_sitemap_sugg": "ç¡®ä¿ Sitemap å¯è®¿é—®å¹¶åœ¨ robots.txt ä¸­å¼•ç”¨ã€‚",
            "no_favicon": "ç«™ç‚¹ç¼ºå¤± Favicon",
            "no_favicon_desc": "æœªåœ¨é¦–é¡µæ£€æµ‹åˆ° Faviconï¼Œé™ä½å“ç‰Œè¾¨è¯†åº¦ï¼Œç›´æ¥å½±å“æœç´¢ç»“æœé¡µ (SERP) çš„ç”¨æˆ·ç‚¹å‡»ç‡ã€‚",
            "no_favicon_sugg": "é…ç½®å…¨å±€ Favicon ä»¥æå‡ SERP å“ç‰Œè¾¨è¯†åº¦ã€‚"
        },
        "en": {
            "no_robots": "Missing Robots.txt",
            "no_robots_desc": "Robots.txt file not found in root directory, or access denied (403/404).",
            "no_robots_sugg": "Create robots.txt in root directory and ensure it is accessible.",
            "no_sitemap": "Sitemap.xml Not Found",
            "no_sitemap_desc": "Sitemap.xml not found in root directory.",
            "no_sitemap_sugg": "Ensure Sitemap is accessible and linked in robots.txt.",
            "no_favicon": "Site Missing Favicon",
            "no_favicon_desc": "No Favicon found. This reduces brand visibility and negatively impacts Click-Through Rate (CTR) in SERPs.",
            "no_favicon_sugg": "Configure a global Favicon for brand visibility."
        }
    }
    t = txt[lang]

    # 1. Robots.txt (ä½¿ç”¨ GET æ›¿ä»£ HEADï¼Œæ›´ç¨³å®š)
    robots_url = urljoin(base_url, "/robots.txt")
    try:
        r = requests.get(robots_url, headers=headers, timeout=10, allow_redirects=True, stream=True)
        # åªè¦æœ€ç»ˆçŠ¶æ€ç æ˜¯ 200ï¼Œå°±ç®—å­˜åœ¨ï¼ˆå³ä½¿ç»è¿‡äº†é‡å®šå‘ï¼‰
        if r.status_code != 200:
            issues.append({"severity": "Medium", "title": t["no_robots"], "desc": t["no_robots_desc"], "suggestion": t["no_robots_sugg"], "url": robots_url})
        r.close()
    except: 
        # ç½‘ç»œé”™è¯¯ä¹Ÿç®—ç¼ºå¤±
        issues.append({"severity": "Medium", "title": t["no_robots"], "desc": t["no_robots_desc"], "suggestion": t["no_robots_sugg"], "url": robots_url})

    # 2. Sitemap.xml Check & Parse
    sitemap_url = urljoin(base_url, "/sitemap.xml")
    try:
        # åŒæ ·ä½¿ç”¨ GET + allow_redirects
        r = requests.get(sitemap_url, headers=headers, timeout=15, allow_redirects=True)
        if r.status_code == 200:
            # æ‰«æ Sitemap å†…å®¹æ£€æŸ¥æ˜¯å¦é…ç½®äº† hreflang
            # æ£€æŸ¥å…³é”®è¯ï¼š'hreflang' æˆ– 'xhtml:link'
            content_sample = r.text[:500000].lower() # åªè¯»å‰ 500KB é˜²æ­¢å†…å­˜æº¢å‡º
            if 'hreflang' in content_sample or 'xhtml:link' in content_sample:
                sitemap_has_hreflang = True
        else:
            issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": t["no_sitemap_desc"], "suggestion": t["no_sitemap_sugg"], "url": sitemap_url})
    except:
        issues.append({"severity": "Low", "title": t["no_sitemap"], "desc": t["no_sitemap_desc"], "suggestion": t["no_sitemap_sugg"], "url": sitemap_url})

    # 3. Favicon (Site Level Check)
    has_favicon = False
    try:
        # Check explicit link in homepage
        resp = requests.get(base_url, headers=headers, timeout=10, allow_redirects=True)
        soup = BeautifulSoup(resp.content, 'html.parser')
        if soup.find('link', rel=lambda x: x and 'icon' in x.lower()):
            has_favicon = True
        else:
            # Check implicit /favicon.ico
            r_ico = requests.get(urljoin(base_url, "/favicon.ico"), headers=headers, timeout=5, allow_redirects=True, stream=True)
            if r_ico.status_code == 200 and int(r_ico.headers.get('content-length', 0)) > 0:
                has_favicon = True
            r_ico.close()
        
        if not has_favicon:
            issues.append({"severity": "Low", "title": t["no_favicon"], "desc": t["no_favicon_desc"], "suggestion": t["no_favicon_sugg"], "url": base_url})
    except: pass

    return issues, sitemap_has_hreflang

def analyze_page(url, html_content, status_code, lang="zh", sitemap_has_hreflang=False):
    """åˆ†æå•ä¸ªé¡µé¢ï¼Œæ”¯æŒå¤šè¯­è¨€è¿”å›"""
    soup = BeautifulSoup(html_content, 'html.parser')
    issues = []
    
    # è¯­è¨€åŒ… - åŒ…å«å½±å“æè¿°
    txt = {
        "zh": {
            "missing_title": "ç¼ºå¤±é¡µé¢æ ‡é¢˜ (Title)",
            "missing_title_desc": "é¡µé¢æ²¡æœ‰ <title> æ ‡ç­¾ã€‚æœç´¢å¼•æ“æ— æ³•æŠ“å–é¡µé¢ä¸»é¢˜ï¼Œä¸¥é‡å½±å“å…³é”®è¯æ’åã€‚",
            "missing_title_sugg": "æ·»åŠ æè¿°æ€§æ ‡é¢˜ã€‚",
            "short_title": "æ ‡é¢˜è¿‡çŸ­",
            "short_title_desc": "æ ‡é¢˜è¿‡çŸ­ï¼Œéš¾ä»¥è¦†ç›–æ ¸å¿ƒå…³é”®è¯ï¼Œé™ä½äº†åœ¨æœç´¢ç»“æœä¸­çš„å±•ç¤ºæœºä¼šã€‚",
            "short_title_sugg": "æ‰©å……æ ‡é¢˜é•¿åº¦ã€‚",
            "long_title": "æ ‡é¢˜è¿‡é•¿",
            "long_title_desc": "æ ‡é¢˜è¿‡é•¿ï¼Œå¯èƒ½åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­ï¼Œå½±å“ç”¨æˆ·ä½“éªŒå’Œç‚¹å‡»ç‡ã€‚",
            "long_title_sugg": "ç²¾ç®€æ ‡é¢˜é•¿åº¦ã€‚",
            "missing_desc": "ç¼ºå¤±å…ƒæè¿°",
            "missing_desc_desc": "ç¼ºå¤± Meta Descriptionã€‚è™½ç„¶ä¸ç›´æ¥å½±å“æ’åï¼Œä½†ä¼šå¤§å¹…é™ä½æœç´¢ç»“æœçš„ç‚¹å‡»ç‡ (CTR)ã€‚",
            "missing_desc_sugg": "æ·»åŠ  Meta Descriptionã€‚",
            "short_desc": "å…ƒæè¿°è¿‡çŸ­",
            "short_desc_desc": "å…ƒæè¿°å†…å®¹è¿‡å°‘ï¼Œæ— æ³•åœ¨æœç´¢ç»“æœä¸­æœ‰æ•ˆå¸å¼•ç”¨æˆ·ç‚¹å‡»ã€‚",
            "short_desc_sugg": "æ‰©å……å…ƒæè¿°è‡³ 120-160 å­—ç¬¦ã€‚",
            "missing_h1": "ç¼ºå¤± H1 æ ‡ç­¾",
            "missing_h1_desc": "é¡µé¢ç¼ºä¹ H1 ä¸»æ ‡é¢˜ã€‚æœç´¢å¼•æ“éš¾ä»¥ç†è§£å†…å®¹çš„å±‚çº§ç»“æ„ï¼Œé™ä½äº†æ ¸å¿ƒå…³é”®è¯çš„ç›¸å…³æ€§æƒé‡ã€‚",
            "missing_h1_sugg": "æ·»åŠ å”¯ä¸€çš„ H1 æ ‡ç­¾ã€‚",
            "missing_viewport": "ç¼ºå¤±ç§»åŠ¨ç«¯è§†å£é…ç½®",
            "missing_viewport_desc": "æœªé…ç½® Viewportã€‚Google é‡‡ç”¨ç§»åŠ¨ä¼˜å…ˆç´¢å¼•ï¼Œè¿™ä¼šå¯¼è‡´é¡µé¢åœ¨ç§»åŠ¨ç«¯æ’åå¤§å¹…ä¸‹é™ã€‚",
            "missing_viewport_sugg": "æ·»åŠ  viewport meta æ ‡ç­¾ã€‚",
            "missing_canonical": "ç¼ºå¤± Canonical æ ‡ç­¾",
            "missing_canonical_desc": "æœªæŒ‡å®šè§„èŒƒé“¾æ¥ã€‚å¦‚æœé¡µé¢å­˜åœ¨å¤šä¸ªè®¿é—®è·¯å¾„ï¼ˆå¦‚å¸¦å‚æ•°ï¼‰ï¼Œä¼šå¯¼è‡´æƒé‡åˆ†æ•£å’Œé‡å¤å†…å®¹é—®é¢˜ã€‚",
            "missing_canonical_sugg": "æ·»åŠ  canonical æ ‡ç­¾æŒ‡å‘æ ‡å‡† URLã€‚",
            "missing_jsonld": "ç¼ºå¤±ç»“æ„åŒ–æ•°æ®",
            "missing_jsonld_desc": "æœªæ£€æµ‹åˆ° Schema æ ‡è®°ã€‚é”™å¤±è·å¾—å¯Œåª’ä½“æœç´¢ç»“æœï¼ˆå¦‚æ˜Ÿçº§ã€ä»·æ ¼æ˜¾ç¤ºï¼‰çš„æœºä¼šï¼Œé™ä½ç‚¹å‡»ç‡ã€‚",
            "missing_jsonld_sugg": "æ ¹æ®é¡µé¢ç±»å‹æ·»åŠ å¯¹åº”çš„ JSON-LDï¼š\n- äº§å“é¡µ: Product\n- æ–‡ç« é¡µ: Article\n- é¦–é¡µ: Organization/WebSite",
            "missing_hreflang": "ç¼ºå¤± Hreflang",
            "missing_hreflang_desc": "æœªå‘ç°è¯­è¨€åŒºåŸŸæ ‡è®°ï¼ˆHTML æˆ– Sitemap ä¸­å‡æœªæ‰¾åˆ°ï¼‰ã€‚å¯èƒ½å¯¼è‡´å‘ç”¨æˆ·å±•ç¤ºé”™è¯¯çš„è¯­è¨€ç‰ˆæœ¬ã€‚",
            "missing_hreflang_sugg": "å¦‚ä¸ºå¤šè¯­è¨€ç«™ç‚¹è¯·åœ¨ HTML å¤´éƒ¨æˆ– Sitemap ä¸­æ·»åŠ  hreflangã€‚",
            "url_underscore": "URL åŒ…å«ä¸‹åˆ’çº¿",
            "url_underscore_desc": "URL ä½¿ç”¨ä¸‹åˆ’çº¿ (_)ã€‚Google å°†ä¸‹åˆ’çº¿è§†ä¸ºå­—ç¬¦è¿æ¥è€Œéåˆ†éš”ç¬¦ï¼Œå½±å“å…³é”®è¯åˆ‡åˆ†å’Œè¯†åˆ«ã€‚",
            "url_underscore_sugg": "æ›¿æ¢ä¸‹åˆ’çº¿ä¸ºè¿å­—ç¬¦ (-)ã€‚",
            "url_uppercase": "URL åŒ…å«å¤§å†™å­—æ¯",
            "url_uppercase_desc": "URL åŒ…å«å¤§å†™å­—æ¯ã€‚æœåŠ¡å™¨é€šå¸¸åŒºåˆ†å¤§å°å†™ï¼Œè¿™ææ˜“å¯¼è‡´é‡å¤å†…å®¹æ”¶å½•å’Œæƒé‡åˆ†æ•£ã€‚",
            "url_uppercase_sugg": "è½¬æ¢ä¸ºå°å†™ URLã€‚",
            "js_links": "å‘ç° JS ä¼ªé“¾æ¥",
            "js_links_desc": "ä½¿ç”¨äº† href='javascript:'ã€‚çˆ¬è™«æ— æ³•è·Ÿè¸ªæ­¤ç±»é“¾æ¥ï¼Œå¯¼è‡´å†…éƒ¨é¡µé¢æ— æ³•è¢«å‘ç°å’Œä¼ é€’æƒé‡ã€‚",
            "js_links_sugg": "ä½¿ç”¨æ ‡å‡† <a href>ã€‚",
            "soft_404": "ç–‘ä¼¼è½¯ 404 (Soft 404)",
            "soft_404_desc": "é¡µé¢è¿”å› 200 çŠ¶æ€ç ä½†å†…å®¹æ˜¾ç¤ºâ€œæœªæ‰¾åˆ°â€ã€‚è¿™ä¼šä¸¥é‡æµªè´¹çˆ¬è™«é¢„ç®—ï¼Œå¹¶å¯¼è‡´æ— æ•ˆé¡µé¢è¢«ç´¢å¼•ã€‚",
            "soft_404_sugg": "é…ç½®æœåŠ¡å™¨è¿”å› 404 çŠ¶æ€ç ã€‚",
            "missing_alt": "å›¾ç‰‡ç¼ºå¤± Alt å±æ€§",
            "missing_alt_desc": "å›¾ç‰‡ç¼ºå°‘æ›¿ä»£æ–‡æœ¬ã€‚æœç´¢å¼•æ“æ— æ³•ç†è§£å›¾ç‰‡å†…å®¹ï¼Œä¸”ä¸åˆ©äºå›¾ç‰‡æœç´¢æ’åå’Œæ— éšœç¢è®¿é—®ã€‚",
            "missing_alt_sugg": "æ·»åŠ  alt å±æ€§ã€‚",
            "duplicate": "å‘ç°é‡å¤å†…å®¹",
            "duplicate_desc": "å†…å®¹ä¸å¦ä¸€é¡µé¢é«˜åº¦é‡å¤ã€‚è¿™ä¼šå¯¼è‡´é¡µé¢ä¹‹é—´ç›¸äº’ç«äº‰æ’åï¼ˆå…³é”®è¯åŒç±»ç›¸é£Ÿï¼‰ï¼Œåˆ†æ•£é¡µé¢æƒé‡ã€‚",
            "duplicate_sugg": "ä½¿ç”¨ Canonical æˆ–åˆå¹¶é¡µé¢ã€‚"
        },
        "en": {
            "missing_title": "Missing Title Tag",
            "missing_title_desc": "Page has no <title> tag. Search engines cannot identify the page topic, severely impacting keyword rankings.",
            "missing_title_sugg": "Add a descriptive title.",
            "short_title": "Title Too Short",
            "short_title_desc": "Title is too short to cover core keywords, reducing visibility in search results.",
            "short_title_sugg": "Increase title length.",
            "long_title": "Title Too Long",
            "long_title_desc": "Title may be truncated in SERPs, negatively affecting user experience and Click-Through Rate (CTR).",
            "long_title_sugg": "Shorten the title.",
            "missing_desc": "Missing Meta Description",
            "missing_desc_desc": "Missing description. While not a direct ranking factor, it significantly lowers the Click-Through Rate (CTR) in search results.",
            "missing_desc_sugg": "Add a meta description.",
            "short_desc": "Meta Description Too Short",
            "short_desc_desc": "Description content is too thin to effectively attract user clicks in SERPs.",
            "short_desc_sugg": "Expand to 120-160 chars.",
            "missing_h1": "Missing H1 Tag",
            "missing_h1_desc": "Page lacks a main H1 heading. Search engines struggle to understand content hierarchy, reducing the relevance weight of core keywords.",
            "missing_h1_sugg": "Add a unique H1 tag.",
            "missing_viewport": "Missing Mobile Viewport",
            "missing_viewport_desc": "No viewport tag found. With Google's Mobile-First Indexing, this causes severe ranking drops on mobile devices.",
            "missing_viewport_sugg": "Add viewport meta tag.",
            "missing_canonical": "Missing Canonical Tag",
            "missing_canonical_desc": "Canonical URL not specified. If the page has multiple access paths (e.g., parameters), it causes link equity dilution and duplicate content issues.",
            "missing_canonical_sugg": "Add canonical link tag.",
            "missing_jsonld": "Missing Structured Data",
            "missing_jsonld_desc": "No Schema markup found. Missed opportunity for Rich Snippets (e.g., stars, price), reducing CTR.",
            "missing_jsonld_sugg": "Add JSON-LD based on page type:\n- Product page: Product\n- Blog: Article\n- Home: Organization",
            "missing_hreflang": "Missing Hreflang",
            "missing_hreflang_desc": "No language/region targeting found (Checked both HTML & Sitemap). This may cause the wrong language version to be shown.",
            "missing_hreflang_sugg": "Add hreflang if multilingual.",
            "url_underscore": "URL Contains Underscores",
            "url_underscore_desc": "URL uses underscores (_). Google treats underscores as character joiners rather than separators, affecting keyword parsing.",
            "url_underscore_sugg": "Replace underscores with hyphens.",
            "url_uppercase": "URL Contains Uppercase",
            "url_uppercase_desc": "URL contains uppercase letters. Servers are case-sensitive, leading to potential duplicate content and split link equity.",
            "url_uppercase_sugg": "Use lowercase URLs.",
            "js_links": "JavaScript Pseudo-links",
            "js_links_desc": "href='javascript:' found. Crawlers cannot follow these links, preventing internal pages from being discovered and ranked.",
            "js_links_sugg": "Use standard <a href>.",
            "soft_404": "Suspected Soft 404",
            "soft_404_desc": "Returns 200 but content says 'Not Found'. This wastes crawl budget and causes invalid pages to be indexed.",
            "soft_404_sugg": "Return 404 HTTP status code.",
            "missing_alt": "Images Missing Alt Text",
            "missing_alt_desc": "Images lack alternative text. Search engines cannot understand image content, hurting image search rankings and accessibility.",
            "missing_alt_sugg": "Add alt attributes.",
            "duplicate": "Duplicate Content Detected",
            "duplicate_desc": "Content highly matches another page. This causes keyword cannibalization and dilutes page authority.",
            "duplicate_sugg": "Use canonicals or merge pages."
        }
    }
    t = txt[lang]
    
    # æå–çº¯æ–‡æœ¬
    for script in soup(["script", "style"]):
        script.extract()
    text_content = soup.get_text().strip()
    text_content_lower = text_content.lower()
    content_hash = get_content_hash(text_content)

    # --- æ£€æŸ¥é€»è¾‘ (ä½¿ç”¨ t[key] è·å–æ–‡æœ¬) ---
    
    # 1. Title
    title_tag = soup.title
    title = title_tag.string.strip() if title_tag and title_tag.string else None
    
    if not title:
        issues.append({"severity": "High", "title": t["missing_title"], "desc": t["missing_title_desc"], "suggestion": t["missing_title_sugg"], "url": url})
    elif len(title) < 10:
         issues.append({"severity": "Medium", "title": t["short_title"], "desc": t["short_title_desc"], "suggestion": t["short_title_sugg"], "url": url})
    elif len(title) > 60:
         issues.append({"severity": "Low", "title": t["long_title"], "desc": t["long_title_desc"], "suggestion": t["long_title_sugg"], "url": url})

    # 2. Meta Description
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    desc_content = meta_desc['content'].strip() if meta_desc and meta_desc.get('content') else None
    
    if not desc_content:
        issues.append({"severity": "High", "title": t["missing_desc"], "desc": t["missing_desc_desc"], "suggestion": t["missing_desc_sugg"], "url": url})
    elif len(desc_content) < 50:
        issues.append({"severity": "Low", "title": t["short_desc"], "desc": t["short_desc_desc"], "suggestion": t["short_desc_sugg"], "url": url})

    # 3. H1
    h1 = soup.find('h1')
    h1_text = h1.get_text().strip() if h1 else "No H1"
    if not h1:
        issues.append({"severity": "High", "title": t["missing_h1"], "desc": t["missing_h1_desc"], "suggestion": t["missing_h1_sugg"], "url": url})

    # 4. Viewport
    viewport = soup.find('meta', attrs={'name': 'viewport'})
    if not viewport:
        issues.append({"severity": "Critical", "title": t["missing_viewport"], "desc": t["missing_viewport_desc"], "suggestion": t["missing_viewport_sugg"], "url": url})

    # 5. Canonical Check & Extraction
    canonical_tag = soup.find('link', attrs={'rel': 'canonical'})
    canonical_url = canonical_tag['href'] if canonical_tag and canonical_tag.has_attr('href') else None
    
    if not canonical_url:
        issues.append({"severity": "Medium", "title": t["missing_canonical"], "desc": t["missing_canonical_desc"], "suggestion": t["missing_canonical_sugg"], "url": url})

    # 6. Favicon (Removed Page Level Check) - Now handled at Site Level

    # 7. JSON-LD (Smart Suggestions)
    schema = soup.find('script', type='application/ld+json')
    if not schema:
         # æ™ºèƒ½æ¨æ–­å»ºè®®
         parsed_u = urlparse(url)
         path = parsed_u.path.lower()
         sugg_text = t["missing_jsonld_sugg"]
         
         if path == "/" or path == "":
             spec_rec = "Organization / WebSite"
         elif "product" in path or "shop" in path or "item" in path:
             spec_rec = "Product"
         elif "blog" in path or "news" in path or "article" in path:
             spec_rec = "Article / BlogPosting"
         else:
             spec_rec = "BreadcrumbList"
             
         custom_sugg = f"Recommended Schema: **{spec_rec}**.\n\n" + sugg_text
         issues.append({"severity": "Medium", "title": t["missing_jsonld"], "desc": t["missing_jsonld_desc"], "suggestion": custom_sugg, "url": url})

    # 8. Hreflang (Cross-check with Sitemap Status)
    hreflang = soup.find('link', hreflang=True)
    if not hreflang:
        if not sitemap_has_hreflang:
            # åªæœ‰åœ¨ Sitemap ä¹Ÿæ²¡é…ç½®çš„æƒ…å†µä¸‹æ‰æŠ¥é”™
            issues.append({"severity": "Low", "title": t["missing_hreflang"], "desc": t["missing_hreflang_desc"], "suggestion": t["missing_hreflang_sugg"], "url": url})

    # 9. URL Structure
    parsed_url = urlparse(url)
    path = parsed_url.path
    if '_' in path:
         issues.append({"severity": "Low", "title": t["url_underscore"], "desc": t["url_underscore_desc"], "suggestion": t["url_underscore_sugg"], "url": url})
    if any(c.isupper() for c in path):
         issues.append({"severity": "Medium", "title": t["url_uppercase"], "desc": t["url_uppercase_desc"], "suggestion": t["url_uppercase_sugg"], "url": url})

    # 10. JS Links
    js_links = soup.find_all('a', href=lambda x: x and x.lower().startswith('javascript:'))
    if js_links:
        issues.append({"severity": "High", "title": t["js_links"], "desc": t["js_links_desc"], "suggestion": t["js_links_sugg"], "url": url, "meta": f"Count: {len(js_links)}"})

    # 11. Soft 404
    is_soft_404 = False
    error_keywords = ["page not found", "404 error", "é¡µé¢æœªæ‰¾åˆ°", "404 é”™è¯¯"]
    if title and any(k in title.lower() for k in error_keywords): is_soft_404 = True
    elif h1_text != "No H1" and any(k in h1_text.lower() for k in error_keywords): is_soft_404 = True
    elif any(k in text_content_lower[:500] for k in error_keywords): is_soft_404 = True

    if status_code == 200 and is_soft_404:
        issues.append({"severity": "Critical", "title": t["soft_404"], "desc": t["soft_404_desc"], "suggestion": t["soft_404_sugg"], "url": url})
        
    # 12. Alt
    images = soup.find_all('img')
    missing_alt = 0
    for img in images:
        if not img.get('alt'): missing_alt += 1
    if missing_alt > 0:
        issues.append({"severity": "Medium", "title": t["missing_alt"], "desc": t["missing_alt_desc"], "suggestion": t["missing_alt_sugg"], "url": url, "meta": f"{missing_alt} images"})

    # Internal Links
    internal_links = set()
    base_domain = urlparse(url).netloc
    for a in soup.find_all('a', href=True):
        link = urljoin(url, a['href'])
        parsed_link = urlparse(link)
        if parsed_link.netloc == base_domain:
            if not any(link.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.pdf', '.css', '.js', '.zip']):
                internal_links.add(link)

    return {
        "URL": url,
        "Status": status_code,
        "Title": title or "No Title",
        "H1": h1_text,
        "Links_Count": len(internal_links),
        "Issues_Count": len(issues),
        "Content_Hash": content_hash,
        "Canonical": canonical_url # Add Canonical to return data
    }, issues, internal_links

def crawl_website(start_url, max_pages=100, lang="zh"):
    """æ‰§è¡Œçˆ¬å–ï¼Œä¼ é€’è¯­è¨€å‚æ•°"""
    visited = set()
    seen_hashes = {} # Format: {hash: url}
    queue = [start_url]
    results_data = []
    all_issues = []
    
    # è·å–å¯¹åº”è¯­è¨€çš„ UI æ–‡æœ¬ - é‡å¤å†…å®¹æ£€æŸ¥
    t_dup_title = "Duplicate Content Detected" if lang == "en" else "å‘ç°é‡å¤å†…å®¹"
    t_dup_desc = "Content matches another page without proper canonical tag." if lang == "en" else "å†…å®¹é‡å¤ä¸”æœªé…ç½® Canonical æŒ‡å‘æºé¡µé¢ã€‚"
    t_dup_sugg = "Add canonical tag pointing to: " if lang == "en" else "æ·»åŠ  Canonical æ ‡ç­¾æŒ‡å‘: "
    
    progress_bar = st.progress(0, text="Initializing...")
    sitemap_has_hreflang = False
    
    try:
        # æ‰§è¡Œç«™ç‚¹çº§æ£€æŸ¥ï¼Œå¹¶è·å– Sitemap Hreflang çŠ¶æ€
        site_issues, sitemap_has_hreflang = check_site_level_assets(start_url, lang=lang)
        all_issues.extend(site_issues)
        
        # ä¿å­˜åˆ° Session State ä¾› Sidebar æ˜¾ç¤º
        st.session_state['sitemap_hreflang_found'] = sitemap_has_hreflang
        
    except Exception as e:
        st.toast(f"Site-check failed: {str(e)}")

    pages_crawled = 0
    
    while queue and pages_crawled < max_pages:
        url = queue.pop(0)
        if url in visited: continue
        visited.add(url)
        pages_crawled += 1
        
        progress = int((pages_crawled / max_pages) * 100)
        progress_bar.progress(progress, text=f"Crawling ({pages_crawled}/{max_pages}): {url}")
        
        try:
            headers = get_browser_headers()
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            
            content_type = response.headers.get('Content-Type', '').lower()
            if 'text/html' in content_type:
                # ä¼ é€’ lang å‚æ•°å’Œ sitemap_has_hreflang çŠ¶æ€
                page_data, page_issues, new_links = analyze_page(
                    url, response.content, response.status_code, 
                    lang=lang, sitemap_has_hreflang=sitemap_has_hreflang
                )
                
                # --- Advanced Duplicate Check (Canonical Aware) ---
                current_hash = page_data['Content_Hash']
                current_canonical = page_data['Canonical']
                
                if current_hash in seen_hashes:
                    original_url = seen_hashes[current_hash]
                    
                    # å…³é”®é€»è¾‘ï¼šå¦‚æœå½“å‰é¡µé¢æœ‰ Canonical ä¸”æŒ‡å‘äº†å…¶ä»–é¡µé¢ï¼ˆé€šå¸¸æ˜¯ original_urlï¼‰ï¼Œåˆ™è§†ä¸ºæ­£å¸¸
                    is_properly_canonicalized = False
                    if current_canonical and current_canonical != url:
                        is_properly_canonicalized = True
                    
                    if not is_properly_canonicalized:
                        page_issues.append({
                            "severity": "High", 
                            "title": t_dup_title, 
                            "desc": f"{t_dup_desc} (vs {original_url})", 
                            "suggestion": f"{t_dup_sugg} {original_url}", 
                            "url": url, 
                            "meta": f"Duplicate of: {original_url}"
                        })
                else:
                    seen_hashes[current_hash] = url

                results_data.append(page_data)
                all_issues.extend(page_issues)
                
                for link in new_links:
                    if link not in visited and link not in queue: queue.append(link)
        except Exception as e:
            pass
    
    progress_bar.progress(100, text="Done!")
    time.sleep(0.5)
    progress_bar.empty()
    return results_data, all_issues

def create_styled_pptx(slides_data, lang="zh"):
    """ç”Ÿæˆ PPTï¼Œæ”¯æŒå¤šè¯­è¨€é™æ€æ–‡æœ¬"""
    prs = Presentation()
    prs.slide_width = Inches(13.333)
    prs.slide_height = Inches(7.5)
    
    txt = TRANSLATIONS[lang] # ä½¿ç”¨å…¨å±€ç¿»è¯‘å­—å…¸
    
    # ä¿®å¤ï¼šç›´æ¥æ“ä½œ font_objï¼Œä¸å†è®¿é—® .font å±æ€§
    def set_font(font_obj, size, bold=False, color=None):
        font_obj.size = Pt(size)
        font_obj.name = 'Microsoft YaHei' if lang == "zh" else 'Arial'
        font_obj.bold = bold
        if color: font_obj.color.rgb = color

    # --- Cover ---
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg_shape = slide.shapes.add_shape(1, Inches(0), Inches(0), Inches(13.333), Inches(7.5))
    bg_shape.fill.solid()
    bg_shape.fill.fore_color.rgb = RGBColor(18, 52, 86)
    bg_shape.line.fill.background()

    title_box = slide.shapes.add_textbox(Inches(1), Inches(2.5), Inches(11), Inches(2))
    p = title_box.text_frame.add_paragraph()
    p.text = txt["ppt_cover_title"]
    p.alignment = PP_ALIGN.CENTER
    set_font(p.font, 54, True, RGBColor(255, 255, 255))
    
    sub_box = slide.shapes.add_textbox(Inches(1), Inches(4), Inches(11), Inches(1))
    p_sub = sub_box.text_frame.add_paragraph()
    p_sub.text = txt["ppt_cover_sub"]
    p_sub.alignment = PP_ALIGN.CENTER
    set_font(p_sub.font, 24, False, RGBColor(200, 200, 200))

    # --- Slides ---
    for issue in slides_data:
        slide = prs.slides.add_slide(prs.slide_layouts[6])
        
        # Header
        header_shape = slide.shapes.add_shape(1, Inches(0), Inches(0), Inches(13.333), Inches(1.2))
        header_shape.fill.solid()
        header_shape.fill.fore_color.rgb = RGBColor(240, 242, 246)
        header_shape.line.fill.background()
        
        header_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(10), Inches(0.8))
        p = header_box.text_frame.add_paragraph()
        p.text = issue['title']
        set_font(p.font, 32, True, RGBColor(50, 50, 50))
        
        # Severity
        sev_color = RGBColor(220, 53, 69) if issue['severity'] == "Critical" else RGBColor(253, 126, 20)
        sev_box = slide.shapes.add_textbox(Inches(11), Inches(0.35), Inches(2), Inches(0.5))
        p = sev_box.text_frame.add_paragraph()
        p.text = f"{issue['severity']}"
        p.alignment = PP_ALIGN.CENTER
        set_font(p.font, 18, True, sev_color)
        
        # Left: Desc
        desc_title_box = slide.shapes.add_textbox(Inches(0.5), Inches(1.5), Inches(6), Inches(0.5))
        p = desc_title_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_desc_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        desc_box = slide.shapes.add_textbox(Inches(0.5), Inches(2.0), Inches(6), Inches(2.5))
        tf = desc_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE # Add this
        p = tf.add_paragraph()
        p.text = issue['desc']
        set_font(p.font, 14, False, RGBColor(80, 80, 80))
        
        count_box = slide.shapes.add_textbox(Inches(0.5), Inches(3.5), Inches(6), Inches(0.5))
        p = count_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_count_title"].format(issue['count'])
        set_font(p.font, 14, True, RGBColor(100, 100, 100))

        # Right: Examples
        ex_title_box = slide.shapes.add_textbox(Inches(7), Inches(1.5), Inches(5.8), Inches(0.5))
        p = ex_title_box.text_frame.add_paragraph()
        p.text = txt["ppt_slide_ex_title"]
        set_font(p.font, 18, True, RGBColor(30, 30, 30))
        
        ex_box = slide.shapes.add_textbox(Inches(7), Inches(2.0), Inches(5.8), Inches(2.5))
        tf = ex_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE # Add this
        for ex_url in issue['examples'][:5]:
            p = tf.add_paragraph()
            p.text = f"â€¢ {ex_url}"
            set_font(p.font, 12, False, RGBColor(0, 102, 204))
            p.space_after = Pt(6)

        # Bottom: Suggestion
        sugg_bg = slide.shapes.add_shape(1, Inches(0.5), Inches(5.5), Inches(12.333), Inches(1.5))
        sugg_bg.fill.solid()
        sugg_bg.fill.fore_color.rgb = RGBColor(230, 244, 234)
        sugg_bg.line.color.rgb = RGBColor(40, 167, 69)
        
        sugg_box = slide.shapes.add_textbox(Inches(0.7), Inches(5.6), Inches(11.9), Inches(1.3))
        tf = sugg_box.text_frame
        tf.word_wrap = True
        tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE # Add this
        
        p = tf.add_paragraph()
        p.text = txt["ppt_slide_sugg_title"]
        set_font(p.font, 16, True, RGBColor(21, 87, 36))
        
        p = tf.add_paragraph()
        p.text = issue['suggestion']
        set_font(p.font, 14, False, RGBColor(21, 87, 36))
        p.space_before = Pt(5)

    binary_output = BytesIO()
    prs.save(binary_output)
    binary_output.seek(0)
    return binary_output

# --- 4. Init Session ---
if 'audit_data' not in st.session_state: st.session_state['audit_data'] = None
if 'audit_issues' not in st.session_state: st.session_state['audit_issues'] = []
if 'language' not in st.session_state: st.session_state['language'] = "zh" # é»˜è®¤è¯­è¨€
if 'sitemap_hreflang_found' not in st.session_state: st.session_state['sitemap_hreflang_found'] = False

# --- 5. Sidebar ---
# è·å–å½“å‰è¯­è¨€å¯¹åº”çš„ UI æ–‡æœ¬
lang = st.session_state['language']
ui = TRANSLATIONS[lang]

with st.sidebar:
    st.title(ui["sidebar_title"])
    st.caption(ui["sidebar_caption"])
    
    # è¯­è¨€åˆ‡æ¢
    st.divider()
    selected_lang = st.radio(ui["lang_label"], ["ä¸­æ–‡", "English"], index=0 if lang=="zh" else 1)
    new_lang = "zh" if selected_lang == "ä¸­æ–‡" else "en"
    
    if new_lang != lang:
        st.session_state['language'] = new_lang
        st.rerun()
    
    st.divider()
    
    menu_options = ui["nav_options"]
    # æ˜ å°„é€‰æ‹©å›å†…éƒ¨ key
    menu_map = {ui["nav_options"][i]: ["input", "dashboard", "matrix", "ppt"][i] for i in range(4)}
    
    selected_menu = st.radio(ui["nav_label"], menu_options)
    menu_key = menu_map[selected_menu]
    
    st.divider()
    if st.session_state['audit_data'] is not None:
        st.success(ui["cache_info"].format(len(st.session_state['audit_data'])))
        
        # æ˜¾ç¤º Sitemap æ‰«æç»“æœ
        st.markdown(f"**{ui['sitemap_status_title']}**")
        if st.session_state['sitemap_hreflang_found']:
            st.caption(ui["sitemap_found_href"])
        else:
            st.caption(ui["sitemap_no_href"])
            
        if st.button(ui["clear_data"]):
            st.session_state['audit_data'] = None
            st.session_state['audit_issues'] = []
            st.session_state['sitemap_hreflang_found'] = False
            st.rerun()

# --- 6. Main Logic ---

if menu_key == "input":
    st.header(ui["input_header"])
    st.info(ui["input_info"])
    
    col1, col2 = st.columns([3, 1])
    with col1:
        url_input = st.text_input(ui["input_label"], placeholder=ui["input_placeholder"])
    with col2:
        max_pages = st.number_input(ui.get("max_pages_label", "Max Pages / æœ€å¤§é¡µé¢æ•°"), min_value=1, max_value=1000, value=100)
    
    start_btn = st.button(ui["start_btn"], type="primary", use_container_width=True)
    
    if start_btn and url_input:
        if not is_valid_url(url_input):
            st.error(ui["error_url"])
        else:
            with st.spinner(ui["spinner_crawl"].format(max_pages)):
                # ä¼ é€’å½“å‰è¯­è¨€ lang å’Œ max_pages
                data, issues = crawl_website(url_input, max_pages=max_pages, lang=lang)
                if not data:
                    st.error(ui["error_no_data"])
                else:
                    st.session_state['audit_data'] = data
                    st.session_state['audit_issues'] = issues
                    st.success(ui["success_audit"].format(len(data)))
                    st.balloons()

elif menu_key == "dashboard":
    st.header(ui["dashboard_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        issues = st.session_state['audit_issues']
        
        total = len(issues)
        score = max(0, 100 - int(total * 0.5))
        critical = len([i for i in issues if i['severity'] == 'Critical'])
        
        k1, k2, k3, k4 = st.columns(4)
        k1.metric(ui["kpi_health"], f"{score}/100")
        k2.metric(ui["kpi_pages"], str(len(df)))
        k3.metric(ui["kpi_issues"], str(total), delta_color="inverse")
        k4.metric(ui["kpi_critical"], str(critical), delta_color="inverse")
        
        st.divider()
        c1, c2 = st.columns(2)
        with c1:
            st.subheader(ui["chart_issues"])
            if issues:
                st.bar_chart(pd.DataFrame(issues)['title'].value_counts())
            else:
                st.info(ui["chart_no_issues"])
        with c2:
            st.subheader(ui["chart_status"])
            if not df.empty:
                st.bar_chart(df['Status'].value_counts())

elif menu_key == "matrix":
    st.header(ui["matrix_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    else:
        df = pd.DataFrame(st.session_state['audit_data'])
        st.dataframe(
            df,
            column_config={
                "URL": st.column_config.LinkColumn("URL"),
                "Status": st.column_config.NumberColumn("Status", format="%d"),
            },
            use_container_width=True,
            hide_index=True
        )
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(ui["download_csv"], csv, "audit_report.csv", "text/csv")

elif menu_key == "ppt":
    st.header(ui["ppt_header"])
    if st.session_state['audit_data'] is None:
        st.warning(ui["warn_no_data"])
    elif not st.session_state['audit_issues']:
        st.success(ui["ppt_success_no_issues"])
    else:
        # Group issues
        raw_issues = st.session_state['audit_issues']
        grouped = {}
        for i in raw_issues:
            t = i['title']
            if t not in grouped:
                grouped[t] = {
                    "title": t, "severity": i['severity'], "desc": i['desc'], 
                    "suggestion": i['suggestion'], "count": 0, "examples": []
                }
            grouped[t]['count'] += 1
            if len(grouped[t]['examples']) < 5: grouped[t]['examples'].append(i['url'])
        
        sov_order = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}
        slides = sorted(list(grouped.values()), key=lambda x: (sov_order.get(x['severity'], 3), -x['count']))

        # Download
        st.write(f"### {ui['ppt_download_header']}")
        st.info(ui["ppt_info"])
        if st.button(ui["ppt_btn"]):
            with st.spinner("Generating..."):
                # ä¼ é€’å½“å‰è¯­è¨€ lang
                pptx = create_styled_pptx(slides, lang=lang)
                fname = f"seo_audit_report_{lang}.pptx"
                st.download_button(ui["ppt_btn"], pptx, fname, "application/vnd.openxmlformats-officedocument.presentationml.presentation")

        st.divider()
        st.subheader(ui["ppt_preview_header"])
        
        if 'slide_index' not in st.session_state: st.session_state.slide_index = 0
        if st.session_state.slide_index >= len(slides): st.session_state.slide_index = 0
        
        s = slides[st.session_state.slide_index]
        
        with st.container(border=True):
            st.markdown(f"### {ui['ppt_slide_title']} {s['title']}")
            c1, c2 = st.columns([1, 1])
            with c1:
                color = "red" if s['severity'] == "Critical" else "orange" if s['severity'] == "High" else "blue"
                st.markdown(f"**{ui['ppt_severity']}** :{color}[{s['severity']}]")
                st.markdown(f"**{ui['ppt_impact']}** {ui['ppt_impact_desc'].format(s['count'])}")
                st.markdown(f"**{ui['ppt_desc']}** {s['desc']}")
                st.info(f"{ui['ppt_sugg']} {s['suggestion']}")
            with c2:
                st.markdown(f"**{ui['ppt_examples']}**")
                for ex in s['examples']: st.markdown(f"- `{ex}`")

        cp, ct, cn = st.columns([1, 2, 1])
        with cp:
            if st.button(ui["ppt_prev"]):
                st.session_state.slide_index = max(0, st.session_state.slide_index - 1)
                st.rerun()
        with ct:
            st.markdown(f"<div style='text-align: center'>Slide {st.session_state.slide_index + 1} / {len(slides)}</div>", unsafe_allow_html=True)
        with cn:
            if st.button(ui["ppt_next"]):
                st.session_state.slide_index = min(len(slides) - 1, st.session_state.slide_index + 1)
                st.rerun()
